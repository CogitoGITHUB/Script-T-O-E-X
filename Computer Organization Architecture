#+title:      Computer Organization & Architecture
#+date:       [2024-12-26 Jo 03:08]
#+filetags:   :computerarchitecture:
#+identifier: 20241226T030824


* Computer Organization & Architecture
** Basic Computer Instructions
*** A simple understanding of Computer Instructions
**Computer**
 is a device that makes our work easy. Computer is a device that helps 
us to complete our task easily and speedily. Computer doesn’t have a 
brain like human beings. We have to give them instructions to what to do
 when a particular situation arises. We have to tell them everything 
from what to expect for data(what type of data), how to process it(how 
to perform calculations) to where to store the data. We humans, 
understand language that is composed of words which further is composed 
of letters. But, the computers doesn’t understand our language nor the 
words like “hello, goodmorning, discipline, etc”. They only understand 
binary language whose vocabulary contains only two letters or states or 
symbols i.e. 0 and 1, True and False, On and off. To maintain the states
 **transistors** are used.

- Transistors are tiny device that are used to store 2 values 1 and 0 or on and off.
- If the transistor is on we say that it has a value 1, and if it is off the value is 0.

For
 example, a memory chip contains hundreds of millions or even billions 
of transistors, each of which can be switched on or off individually. As
 transistor can store 2 distinct values, we can have millions of 
different values stored on a memory chip consisting entirely of 0’s and 
1’s. **But how a transistor get its value ?** When a very 
little amount of electric current passes through transistor it maintains
 the state of 1 and when there is no electric current then the 
transistor has the state of 0. **Then how it’s all connected to computer ?**
 This 0’s and 1’s forms the building block of computer. With the 
combinations of 0 and 1 we create a whole new language For example, 0 
can be written as 0,

```
1 as 1
2 as 10
3 as 11
4 as 100
5 as 101
a as 01100001
A as 01000001
s as 01110011
U as 01010101
```

Hello as,

```
01001000 01100101 01101100 01101100 01101111
```

Hello World! as,

```
01001000 01100101 01101100 01101100 01101111
00100000 01010111 01101111 01110010 01101100 01100100 00100001
```

And so on… **So now the question arises how can a human remember this code? It seems impossible!** Well
 we humans can do everything that we desire and this code can be 
remembered very easily but we don’t have to remember. We just have to 
use our language and the software (also built by human) converts our 
normal letters into the binary language. **What is software ?**
 Software is a set of instructions that tells the computer what to do, 
when to do, and how to do. Example are, paint that we use in Microsoft, 
WhatsApp and games, all are the types of different software. Suppose we 
want to add 2 number and want to know what 2 + 2 is 4. Then we must give
 the computer instructions,

- **Step-1:** take 2 value.
- **Step-2:** store that 2 value
- **Step-3:** add 2 value by using + operator
- **Step-4:** save the answer

Separate
 instructions are provided for the + operator so the computer knows how 
to do addition when it encounters + sign. So who converts this code? 
Instead of who we can ask what converts the code? And answer to that 
question is a software called interpreter that interprets our language 
code into binary code. Interpreter converts our code into machine 
language that can be understood by computer. **Now the question is how we give our input ?**
 We give our input with the use of hardware for example like scanner, 
keyboard, mouse(not the one that eats cheese). When we give input 
through hardware, the software interprets it into machine language and 
then it is processed and our output is shown. **Process:** 
If we want to display letter ‘A’ on screen we first will open notepad. 
Then we will press Capslock key or shift key to make letter capital, 
after that we will press letter ‘a’. And our screen will show the letter
 ‘A’. **Under the hood process:** When we pressed the 
capslock or shift key the software tells that whatever following this 
should be printed on the screen and after we have pressed the letter a 
which is small letter, the software first converts it into binary like 
it had converted the shift or capslock key and then after the computer 
understands it prints A on the screen.
*** Issues in Computer Design
**Computer Design**
 is the structure in which components relate to each other. The designer
 deals with a particular level of system at a time and there are 
different types of issues at different levels. At each level, the 
designer is concerned with the structure and function. The structure is 
the skeleton of the various components related to each other for 
communication. The function is the activities involved in the system.

Following are the issues in computer design: 

1. **Assumption of infinite speed:** It can’t be assumed the infinite speed of the computer as it is not
practical to assume the infinite speed. It creates problems in
designers’ thinking as well.
2. **Assumption of infinite Memory:** Like the speed of the computer, memory also can’t be assumed infinite.
Storage is always finite and this is an issue in computer design.
3. **Speed mismatch between memory and processor:** Sometimes it is possible that the speed of memory and processor does not match.
It may be memory speed is faster or processor speed is faster. A
mismatch between memory and processor leads to create problems in
designing.
4. **Handling of bugs and errors:** Handling bugs and errors are huge responsibility of any computer designer. Bugs
and errors lead to the failure of the computer system. Sometimes these
errors may be more dangerous.
5. **Multiple processors:** Designing a computer system with multiple processors leads to the huge task of
management and programming. It is a big issue in computer design.
6. **Multiple threads:** A computer system with multiple threads is always a threat to the
designer. A computer with several threads should be able to
multi-tasking and multi-processing.
7. **Shared memory:** If there are several processes to be executed at a time then all the
processes share the same memory space. It should be managed in a
specific way so that collision does not happen.
8. **Disk access:** Disk management is the key to computer design. There are several issues with disk access. It may be possible that the system does not support
multiple disk access.
9. **Better performance:** It is always an issue. A designer always tries to simplify the system for better performance in reducing power and less cost.
*** Computer System Level Hierarchy
**Computer System Level Hierarchy**
 is the combination of different levels that connects the computer with 
the user and that makes the use of the computer. It also describes how 
the computational activities are performed on the computer and it shows 
all the elements used in different levels of system.

Computer System Level Hierarchy consists of seven levels: 

!https://media.geeksforgeeks.org/wp-content/uploads/20200420004104/Untitled-Diagram95.png

- **Level-0:** It is related to digital logic. Digital logic is the basis for digital
computing and provides a fundamental understanding of how circuits and
hardware communicate within a computer. It consists of various circuits
and gates etc.
- **Level-1:** This level is
related to control. Control is the level where microcode is used in the
system. Control units are included in this level of the computer
system.
- **Level-2:** This level consists of
machines. Different types of hardware are used in the computer system to perform different types of activities. It contains instruction set
architecture.
- **Level-3:** System software is a part of this level. System software is of various types. System
software mainly helps in operating the process and it establishes the
connection between hardware and user interface. It may consist operating system, library code, etc.
- **Level-4:** Assembly language is the next level of the computer system. The machine
understands only the assembly language and hence in order, all the
high-level languages are changed in the assembly language. Assembly code is written for it.
- **Level-5:** This level of the system contains high-level language. High-level language consists
of C++, Java, FORTRAN, and many other languages. This is the language in which the user gives the command.
- **Level-6:** This the last level of the computer system hierarchy. This consists of users and executable programs.
*** Differences between Computer Architecture and Computer Organization
**Computer Architecture**
 is a functional description of requirements and design implementation 
for the various parts of a computer. It deals with the functional 
behavior of computer systems. It comes before the computer organization 
while designing a computer.

> Architecture describes what the computer does.
> 

!https://media.geeksforgeeks.org/wp-content/uploads/20190512115102/Untitled-Diagram28.png

**Computer Organization**
 comes after the decision of Computer Architecture first. Computer 
Organization is how operational attributes are linked together and 
contribute to realizing the architectural specification. Computer 
Organization deals with a structural relationship.

> The organization describes how it does it.
> 

### Difference between Computer Architecture and Computer Organization:

[Untitled Database](https://www.notion.so/4feb9a39312248f4a79ba8737435e987?pvs=21)
*** Computer Organization | Basic Computer Instructions
The 
basic computer has 16-bit instruction register (IR) which can denote 
either memory reference or register reference or input-output 
instruction.

1. **Memory Reference –** These instructions refer to memory address as an operand. The other
operand is always accumulator. Specifies 12-bit address, 3-bit opcode
(other than 111) and 1-bit addressing mode for direct and indirect
addressing.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/mref-1.jpg
    
    **Example –**IR
     register contains = 0001XXXXXXXXXXXX, i.e. ADD after fetching and 
    decoding of instruction we find out that it is a memory reference 
    instruction for ADD operation.
    
    ```
    Hence, DR ← M[AR]
    AC ← AC + DR, SC ← 0
    ```
    
2. **Register Reference –** These instructions perform operations on registers rather than memory
addresses. The IR(14 – 12) is 111 (differentiates it from memory
reference) and IR(15) is 0 (differentiates it from input/output
instructions). The rest 12 bits specify register operation.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/rref-1.jpg
    
    **Example –**IR
     register contains = 0111001000000000, i.e. CMA after fetch and decode 
    cycle we find out that it is a register reference instruction for 
    complement accumulator.
    
    ```
    Hence, AC ← ~AC
    ```
    
3. **Input/Output –** These instructions are for communication between computer and outside
environment. The IR(14 – 12) is 111 (differentiates it from memory
reference) and IR(15) is 1 (differentiates it from register reference
instructions). The rest 12 bits specify I/O operation.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/ioref-1.jpg
    
    **Example –**IR
     register contains = 1111100000000000, i.e. INP after fetch and decode 
    cycle we find out that it is an input/output instruction for inputing 
    character. Hence, INPUT character from peripheral device.
    

The set of instructions incorporated in16 bit IR register are:

1. Arithmetic, logical and shift instructions (and, add, complement, circulate left, right, etc)
2. To move information to and from memory (store the accumulator, load the accumulator)
3. Program control instructions with status conditions (branch, skip)
4. Input output instructions (input character, output character)

[Untitled Database](https://www.notion.so/10a50056c521436299f463140670c094?pvs=21)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

*** Timing diagram of MOV Instruction in Microprocessor
**Problem –** Draw the timing diagram of the given instruction in 8085,

```
MOV B, C
```

Given
 instruction copy the contents of the source register into the 
destination register and the contents of the source register are not 
altered.

**Example:**

```
MOV B, C
Opcode: MOV
Operand: B and C
```

Bis is the destination register and C is the source register whose contents need to be transferred to the destination register. **Algorithm –**
 The instruction MOV B, C is of 1 byte; therefore the complete 
instruction will be stored in a single memory address. For example:

```
2000: MOV B, C
```

Only
 opcode fetching is required for this instruction and thus we need 4 T 
states for the timing diagram. For the opcode fetch the IO/M (low 
active) = 0, S1 = 1 and S0 = 1. The timing diagram of MOV instruction is
 shown below:

**In Opcode fetch ( t1-t4 T states):**

!https://media.geeksforgeeks.org/wp-content/uploads/TM1.png

- **00 –** lower bit of address where the opcode is stored, i.e., 00.
- **20 –** higher bit of address where the opcode is stored, i.e., 20.
- **ALE –** provides signal for multiplexed address and data bus. Only in t1 is it
used as an address bus to fetch a lower bit of address otherwise it will be used as the data bus.
- **RD (low active) –**
signal is 1 in t1 & t4 as no data is read by the microprocessor.
Signal is 0 in t2 & t3 because here the data is read by a
microprocessor.
- **WR (low active) –** signal is 1 throughout, no data is written by a microprocessor.
- **IO/M (low active) –** signal is 1 throughout because the operation is performing on memory.
- **S0 and S1 –** both are 1 in case of opcode fetching.
*** Difference between assembly language and high level language
**1. Assembly level language :**It
 is a low-level language that allows users to write a program using 
alphanumeric mnemonic codes, instead of numeric code for a set of 
instructions examples of large assembly language programs from this time
 are IBM PC DOS.

**2. High-level language :**It
 is a machine-independent language. It enables a user to write a program
 in a language that resembles English words and familiar mathematical 
symbols, COBOL was the first high-level language. Examples of high-level
 language are python,c#, etc.

**Difference between assembly language and high-level language :**

[Untitled Database](https://www.notion.so/5b6a4b95289b4f3b9153d2944b2b2166?pvs=21)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Addressing Modes
**Addressing Modes**–
 The term addressing modes refers to the way in which the operand of an 
instruction is specified. The addressing mode specifies a rule for 
interpreting or modifying the address field of the instruction before 
the operand is actually executed.

**Addressing modes for 8086 instructions are divided into two categories:**

1) Addressing modes for data

2) Addressing modes for branch

The
 8086 memory addressing modes provide flexible access to memory, 
allowing you to easily access variables, arrays, records, pointers, and 
other complex data types.  The key to good assembly language programming
 is the proper use of memory addressing modes.

An assembly language program instruction consists of two parts

The memory address of an operand consists of two components:****

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Addressing_Modes_1.jpg

**IMPORTANT TERMS**

- **Starting address** of memory segment.
- **Effective address or Offset**: An offset is determined by adding any combination of three address elements: **displacement, base and index.**
    - **Displacement:** It is an 8 bit or 16 bit immediate value given in the instruction.
    - **Base**: Contents of base register, BX or BP.
    - **Index**: Content of index register SI or DI.

According to different ways of specifying an operand by 8086 microprocessor, different addressing modes are used by 8086.

**Addressing modes** used by 8086 microprocessor are discussed below:

- **Implied mode:**: In implied addressing the operand is specified in the instruction
itself. In this mode the data is 8 bits or 16 bits long and data is the
part of instruction.Zero address instruction are designed with implied
addressing mode.
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/Addressing_Modes_2.jpg
    
    ```
    Example:  CLC (used to reset Carry flag to 0)
    ```
    
- **Immediate addressing mode (symbol #):**In this mode data is present in address field of instruction .Designed like one address instruction format.**Note:**Limitation in the immediate mode is that the range of constants are restricted by size of address field.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/20190305161651/co-300x159.png
    
    ```
    Example:  MOV AL, 35H (move the data 35H into AL register)
    ```
    
- **Register mode:** In register addressing the operand is placed in one of 8 bit or 16 bit
general purpose registers. The data is in the register that is specified by the instruction.*Here one register reference is required to access the data.*
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/Addressing_Modes_3.jpg
    
    ```
    Example: MOV AX,CX (move the contents of CX register to AX register)
    ```
    
- **Register Indirect mode**: In this addressing the operand’s offset is placed in any one of the
registers BX,BP,SI,DI as specified in the instruction. The effective
address of the data is in the base register or an index register that is specified by the instruction.*Here two register reference is required to access the data.*The 8086 CPUs let you access memory indirectly through a register using the register indirect addressing modes.
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/Addressing_Modes_4.jpg
    
    ```
    MOV AX, [BX](move the contents of memory location s
    addressed by the register BX to the register AX)
    ```
    
- **Auto Indexed (increment mode)**: Effective address of the operand is the contents of a register
specified in the instruction. After accessing the operand, the contents
of this register are automatically incremented to point to the next
consecutive memory location.**(R1)+**.*Here one register reference,one memory reference and one ALU operation is required to access the data.*Example:
    
    ```
    Add R1, (R2)+  // OR
    R1 = R1 +M[R2]
    R2 = R2 + d
    ```
    
    *Useful for stepping through arrays in a loop. R2 – start of array d* – size of an element
    
- **Auto indexed ( decrement mode)**: Effective address of the operand is the contents of a register
specified in the instruction. Before accessing the operand, the contents of this register are automatically decremented to point to the previous consecutive memory location. *–***(R1)***Here one register reference,one memory reference and one ALU operation is required to access the data.*

**Example:**

```
Add R1,-(R2)   //OR
R2 = R2-d
R1 = R1 + M[R2]
```

*Auto decrement mode is same as  auto 
increment mode. Both can also be used to implement a stack as push and 
pop . Auto increment and Auto decrement modes are useful for 
implementing “Last-In-First-Out” data structures.*

- **Direct addressing/ Absolute addressing Mode (symbol [ ]):** The operand’s offset is given in the instruction as an 8 bit or 16 bit
displacement element. In this addressing mode the 16 bit effective
address of the data is the part of the instruction.*Here only one memory reference operation is required to access the data.*
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/Addressing_Modes_5.jpg
    
    ```
    Example:ADD AL,[0301]   //add the contents of offset address 0301 to AL
    ```
    
- **Indirect addressing Mode (symbol @ or () )**:In this mode address field of instruction contains the address of effective address.Here two references are required.1st reference to get effective address.2nd reference to access the data.
    
    Based on the availability of Effective address, Indirect mode is of two kind:
    
    1. Register Indirect:In this mode effective address is in the register, and
    corresponding register name will be maintained in the address field of
    an instruction.*Here one register reference,one memory reference is required to access the data.*
    2. Memory Indirect:In this mode effective address is in the memory, and
    corresponding memory address will be maintained in the address field of
    an instruction.*Here two memory reference is required to access the data.*
- **Indexed addressing mode**: The operand’s offset is the sum of the content of an index register SI or DI and an 8 bit or 16 bit displacement.
    
    ```
    Example:MOV AX, [SI +05]
    ```
    
- **Based Indexed Addressing:** The operand’s offset is sum of the content of a base register BX or BP and an index register SI or DI.
    
    ```
    Example: ADD AX, [BX+SI]
    ```
    

**Based on Transfer of control, addressing modes are:**

- **PC relative addressing mode:** PC relative addressing mode is used to implement intra segment transfer of control, In this mode effective address is obtained by adding
displacement to PC.
    
    ```
    EA= PC + Address field value
    PC= PC + Relative value.
    ```
    
- **Base register addressing mode:**Base register addressing mode is used to implement inter segment transfer of control.In this mode effective address is obtained by adding base
register value to address field value.
    
    ```
    EA= Base register + Address field value.
    PC= Base register + Relative value.
    
    ```
    
    **Note:**
    
    1. PC relative nad based register both addressing modes are suitable for program relocation at runtime.
    2. Based register addressing mode is best suitable to write position independent codes.

**Advantages of Addressing Modes**

1. To give programmers to facilities such as Pointers, counters for loop controls, indexing of data and program relocation.
2. To reduce the number bits in the addressing field of the Instruction.

**Sample GATE Question**

Match
 each of the high level language statements given on the left hand side 
with the most natural addressing mode from those listed on the right 
hand side.

```
1. A[1] = B[J];         a. Indirect addressing
2. while [*A++];        b. Indexed  addressing
3. int temp = *x;       c. Autoincrement
```

**(A**) (1, c), (2, b), (3, a)**(B)** (1, a), (2, c), (3, b)**(C)** (1, b), (2, c), (3, a)**(D)** (1, a), (2, b), (3, c)

**Answer:** **(C)**

**Explanation:**

```
List 1                           List 2
1) A[1] = B[J];      b) Index addressing
Here indexing is used

2) while [*A++];     c) auto increment
The memory locations are automatically incremented

3) int temp = *x;    a) Indirect addressing
Here temp is assigned the value of int type stored
at the address contained in X
```

Hence (C) is correct solution.

*** Difference between Memory based and Register based Addressing Modes
Prerequisite – [Addressing Modes](https://www.geeksforgeeks.org/addressing-modes/) **Addressing modes**
 are the operations field specifies the operations which need to be 
performed. The operation must be executed on some data which is already 
stored in computer registers or in the memory. The way of choosing 
operands during program execution is dependent on addressing modes of 
instruction. “The addressing mode specifies a rule for interpreting or 
modifying the address field of the instruction before the operand is 
actually referenced. “Basically how we are interpreting the operand 
which is given in the instruction is known as addressing mode.

Addressing mode very much depends on the type of CPU organization. There are three types of CPU organization:

1. Single Accumulator organization
2. General register organization
3. Stack organization

Addressing modes is used for one or both of the purposes. These can also be said as the **advantages** of using addressing mode:

1. To give programming versatility to the user by providing such facilities
as pointers to memory, counter for loop control, indexing of data, and
program relocation.
2. To reduce the number of bits in the addressing field of the instruction.

There
 is a number of addressing modes available and it depends on the 
architecture and CPU organization which of the addressing modes can be 
applied.

### Difference between Memory based and Register based Addressing Modes:

[Untitled Database](https://www.notion.so/5ba61fb9292a4e2296f8399ac7510771?pvs=21)

Memory-based addressing 
modes are mostly relying on Memory address and content present at some 
memory location. Register-based addressing modes are mostly relying on 
registers and content present at some register whether it is data or 
some memory address.
*** Von Neumann architecture
Historically there have been 2 types of Computers:

1. **Fixed Program Computers –** Their function is very specific and they couldn’t be programmed, e.g. Calculators.
2. **Stored Program Computers –** These can be programmed to carry out many different tasks, applications are stored on them, hence the name.

The
 modern computers are based on a stored-program concept introduced by 
John Von Neumann. In this stored-program concept, programs and data are 
stored in a separate storage unit called memories and are treated the 
same. This novel idea meant that a computer built with this architecture
 would be much easier to reprogram.

The basic structure is like,

!https://media.geeksforgeeks.org/wp-content/uploads/basic_structure.png

It is also known as **IAS** computer and is having three basic units:

1. The Central Processing Unit (CPU)
2. The Main Memory Unit
3. The Input/Output Device

Let’s consider them in details.

- **Control Unit –** A control unit (CU) handles all processor control signals. It directs all input and output flow, fetches code for instructions, and controls how
data moves around the system.
- **Arithmetic and Logic Unit (ALU) –** The arithmetic logic unit is that part of the CPU that handles all the
calculations the CPU may need, e.g. Addition, Subtraction, Comparisons.
It performs Logical Operations, Bit Shifting Operations, and Arithmetic
operations.

!https://media.geeksforgeeks.org/wp-content/uploads/vn_cpu.png

**Figure –** Basic CPU structure, illustrating ALU

- **Main Memory Unit (Registers) –**
    1. **Accumulator:** Stores the results of calculations made by ALU.
    2. **Program Counter (PC):** Keeps track of the memory location of the next instructions to be dealt with. The PC then passes this next address to Memory Address Register
    (MAR).
    3. **Memory Address Register (MAR):** It stores the memory locations of instructions that need to be fetched from memory or stored into memory.
    4. **Memory Data Register (MDR):** It stores instructions fetched from memory or any data that is to be transferred to, and stored in, memory.
    5. **Current Instruction Register (CIR):** It stores the most recently fetched instructions while it is waiting to be coded and executed.
    6. **Instruction Buffer Register (IBR):** The instruction that is not to be executed immediately is placed in the instruction buffer register IBR. 
- **Input/Output Devices –** Program or data is read into main memory from the *input device* or secondary storage under the control of CPU input instruction. *Output devices* are used to output the information from a computer. If some results are evaluated by computer and it is stored in the computer, then with the
help of output devices, we can present them to the user.
- **Buses –** Data is transmitted from one part of a computer to another, connecting
all major internal components to the CPU and memory, by the means of
Buses. Types:
    1. **Data Bus:** It carries data among the memory unit, the I/O devices, and the processor.
    2. **Address Bus:** It carries the address of data (not the actual data) between memory and processor.
    3. **Control Bus:** It carries control commands from the CPU (and status signals from other devices) in order to control and coordinate all the activities within
    the computer.

**Von Neumann bottleneck –** Whatever
 we do to enhance performance, we cannot get away from the fact that 
instructions can only be done one at a time and can only be carried out 
sequentially. Both of these factors hold back the competence of the CPU.
 This is commonly referred to as the ‘Von Neumann bottleneck’. We can 
provide a Von Neumann processor with more cache, more RAM, or faster 
components but if original gains are to be made in CPU performance then 
an influential inspection needs to take place of CPU configuration.

This architecture is very important and is used in our PCs and even in Super Computers.
*** Harvard Architecture
In a
 normal computer that follows von Neumann architecture, instructions and
 data both are stored in the same memory. So same buses are used to 
fetch instructions and data. This means the CPU cannot do both things 
together (read the instruction and read/write data). **Harvard Architecture**
 is the computer architecture that contains separate storage and 
separate buses (signal path) for instruction and data. It was basically 
developed to overcome the bottleneck of Von Neumann Architecture. The 
main advantage of having separate buses for instruction and data is that
 the CPU can access instructions and read/write data at the same time.

### Structure of Harvard Architecture:

!https://media.geeksforgeeks.org/wp-content/uploads/20200427063304/harvard.png

Structure of Harvard Architecture

### Buses

Buses
 are used as signal pathways. In Harvard architecture, there are 
separate buses for both instruction and data. Types of Buses:

- **Data Bus:** It carries data among the main memory system, processor, and I/O devices.
- **Data Address Bus:** It carries the address of data from the processor to the main memory system.
- **Instruction Bus:** It carries instructions among the main memory system, processor, and I/O devices.
- **Instruction Address Bus:** It carries the address of instructions from the processor to the main memory system.

### Operational Registers

There are different types of registers involved in it which are used for storing addresses of different types of instructions. *For example*, the Memory Address Register and Memory Data Register are operational registers.

### Program Counter

It
 has the location of the next instruction to be executed. The program 
counter then passes this next address to the memory address register.

### Arithmetic and Logic Unit

The
 arithmetic logic unit is that part of the CPU that operates all the 
calculations needed. It performs addition, subtraction, comparison, 
logical Operations, bit Shifting Operations, and various arithmetic 
operations.

### Control Unit

The Control Unit is the part of
 the CPU that operates all processor control signals. It controls the 
input and output devices and also controls the movement of instructions 
and data within the system.

### Input/Output System

Input 
devices are used to read data into main memory with the help of CPU 
input instruction. The information from a computer as output is given 
through Output devices. The computer gives the results of computation 
with the help of output devices.

### Advantage of Harvard Architecture:

Harvard
 architecture has two separate buses for instruction and data. Hence, 
the CPU can access instructions and read/write data at the same time. 
This is the major advantage of Harvard architecture.

In practice,
 Modified Harvard Architecture is used where we have two separate caches
 (data and instruction). This is common and used in X86 and ARM 
processors.
*** Interaction of a Program with Hardware
When a Programmer writes a program, it is fed into the computer and how does it actually work?

So,
 this article is about the process of how the program code that is 
written on any text editor is fed to the computer and gets executed. As 
we all know computers work with only two numbers,i.e. 0s or 1s. Let’s 
have a look at the entire procedure of how a code that is written in 
general language is translated into 0’s and 1’s.

### Interaction of a Program with Hardware- The step-by-Step Procedure

1. We write code in a text editor using any language like C++, JAVA, Python, etc.

2. This code is given to the [compiler and it actually converts it to assembly code](https://www.geeksforgeeks.org/compiling-a-c-program-behind-the-scenes/)
 that is very close to machine hardware as it depends on an instruction 
set which is then converted to the binary that is 0s and 1s which 
actually represent digital voltage fed to transistors inside the chip.

3.
 Now we have voltages that are actually required to run the hardware. 
 These voltages actually connect the correct circuitry inside the chip 
and perform that specific task for example addition, subtraction, etc. 
 All these operations are done by the combination of little transistors 
if we go into low level or flip-flops which are the combination of gates
 and gates are a combination of transistors. So, it all started with the
 invention of transistors.

4. The chip or microprocessor has a 
lot of circuits inside it to perform various tasks like arithmetic and 
logical task. The computer hardware also contains RAM which is another 
chip that can store data temporarily and a Hard disk that can 
permanently store data.

5. The operating system is also 
responsible for feeding the software to the right hardware like the 
keyboard, mouse, screen, etc.

The following image depicts the entire procedure:

!https://media.geeksforgeeks.org/wp-content/uploads/123-6.png

Interaction of a Program with Hardware

Initially, a *programmer writes code in the text editor*, then the code is compiled, after successful compilation, the code is *translated into Assembly language*, Once the code is translated into assembly language, it is the responsibility of the assembler to *translate the code into machine understandable form or binary form*. So, the *assembler converts the code into 0’s and 1’s series*. After this, the *series of 0’s and 1’s is executed on a Hardware chip or microprocessor* to produce the **result**.
*** Simplified Instructional Computer (SIC)
*A* simplified *Instructional Computer (SIC)*
 is a hypothetical computer that has hardware features that are often 
found in real machines. There are two versions of this machine:

1. SIC standard Model
2. SIC/XE(extra equipment or expensive)

Object programs for SIC can be properly executed on SIX/XE which is known as upward compatibility.

**SIC Machine Architecture/Components –**

**1. Memory –**

- Memory is byte-addressable that is words are addressed by the location of their lowest numbered byte.
- There are 2^15 bytes in computer memory (1 byte = 8 bits) 3 consecutive byte = 1 word (24 bits = 1 word)

**2. Registers –**

There
 are 5 registers in SIC. Every register has an address associated with 
it known as a registration number. The size of each register is 3 bytes.
 On basis of register size, integer size is dependent. I. A(Accumulator-0): It is used for mathematical operations. II. X(Index Register-1): It is used for addressing. III. L(Linkage Register-2): It stores the return address of the instruction in case of subroutines. IV. PC(Program Counter-8): It holds the address of the next instruction to be executed. V. SW(Status Word-9): It contains a variety of information

Status Word Register:

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/33333.png

- **mode** bit refers to user mode(value=0) or supervising mode(value=1). It occupies 1 bit.[0]
- **state** bit refers whether process is in running state(value=0) or idle state(value=1). It also occupies 1 bit.[1]
- **id** bit refers to process id(PID). It occupies 3 bits.[2-5]
- **CC** bit refers to condition code i.e. It tells whether the device is ready or not. It occupies 2 bits.[6-7] **Mask** bit refers to interrupt mask. It occupies 4 bits.[8-11]
- **X** refers to unused bit. It also occupies 4 bits.[12-15]
- **ICode** refers to interrupt code i.e. Interrupt Service Routine. It occupies the remaining bits.[16-23]

**3. Data Format –**

- Integers are represented by 24 bits.
- Negative numbers are represented in 2’s complement.
- Characters are represented by 8 bit ASCII values.
- No floating-point representation is available.

**4. Instruction Format –** All instructions in SIC have a 24-bit format.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/666666.png

- If x=0 it means direct addressing mode.
- If x=1 it means indexed addressing mode.

**5. Instruction Set –**

- Load And Store Instructions: To move or store data from accumulator to
memory or vice-versa. For example LDA, STA, LDX, STX, etc.
- Comparison Instructions: Used to compare data in memory by contents in accumulator. For example COMP data.
- Arithmetic Instructions: Used to perform operations on accumulator and memory and
store results in the accumulator. For example ADD, SUB, MUL, DIV, etc.
- Conditional Jump: compare the contents of accumulator and memory and performs task
based on conditions. For example JLT, JEQ, JGT
- Subroutine Linkage: Instructions related to subroutines. For example JSUB, RSUB

**6. Input and Output –** It
 is performed by transferring 1 byte at a time from or to the rightmost 8
 bits of the accumulator. Each device has an 8-bit unique code. There are 3 I/O instructions:

- Test Device (TD) tests whether the device is ready or not. Condition code in Status Word Register is used for this purpose. If cc is < then the
device is ready otherwise the device is busy.
- Read data(RD) reads a byte from the device and stores it in register A.
- Write data(WD) writes a byte from register A to the device.

**References:**

Leland.L.Beck: An introduction to systems programming, 3rd Edition, Addison-Wesley, 1997.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Instruction Set used in simplified instructional Computer (SIC)
Prerequisite – [Simplified Instructional Computer (SIC)](https://www.geeksforgeeks.org/simplified-instructional-computer-sic/) These are the instructions used in programming the Simplified Instructional Computer(SIC).

Here, A stands for Accumulator M stands for Memory CC stands for Condition Code PC stands for Program Counter RMB stands for Right Most Byte L stands for Linkage Register

[Untitled Database](https://www.notion.so/6457132214d24c238ed790ab24f1b3cf?pvs=21)
*** Instruction Set used in SIC||XE
https://www.geeksforgeeks.org/instruction-set-used-in-sic-xe/
*** Computer Organization | RISC and CISC
**Reduced Instruction Set Architecture (RISC) –** The
 main idea behind this is to make hardware simpler by using an 
instruction set composed of a few basic steps for loading, evaluating, 
and storing operations just like a load command will load data, a store 
command will store the data.

**Complex Instruction Set Architecture (CISC) –** The
 main idea is that a single instruction will do all loading, evaluating,
 and storing operations just like a multiplication command will do stuff
 like loading data, evaluating, and storing it, hence it’s complex.

Both approaches try to increase the CPU performance

- **RISC:** Reduce the cycles per instruction at the cost of the number of instructions per program.
- **CISC:** The CISC approach attempts to minimize the number of instructions per
program but at the cost of an increase in the number of cycles per
instruction.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-210.png

Earlier
 when programming was done using assembly language, a need was felt to 
make instruction do more tasks because programming in assembly was 
tedious and error-prone due to which CISC architecture evolved but with 
the uprise of high-level language dependency on assembly reduced RISC 
architecture prevailed.

**Characteristic of RISC –**

1. Simpler instruction, hence simple instruction decoding.
2. Instruction comes undersize of one word.
3. Instruction takes a single clock cycle to get executed.
4. More general-purpose registers.
5. Simple Addressing Modes.
6. Fewer Data types.
7. A pipeline can be achieved.

**Characteristic of CISC –**

1. Complex instruction, hence complex instruction decoding.
2. Instructions are larger than one-word size.
3. Instruction may take more than a single clock cycle to get executed.
4. Less number of general-purpose registers as operations get performed in memory itself.
5. Complex Addressing Modes.
6. More Data types.

**Example –** Suppose we have to add two 8-bit numbers:

- **CISC approach:** There will be a single command or instruction for this like ADD which will perform the task.
- **RISC approach:** Here programmer will write the first load command to load data in
registers then it will use a suitable operator and then it will store
the result in the desired location.

So, add operation is 
divided into parts i.e. load, operate, store due to which RISC programs 
are longer and require more memory to get stored but require fewer 
transistors due to less complex command.

**Difference –**

[Untitled Database](https://www.notion.so/7190d2e4b003450c817eaedf7c00a582?pvs=21)
*** Difference between RISC and CISC processor | Set 2
A **microprocessor**
 is a processing unit on a single chip. It is an integrated circuit that
 performs the core functions of a computer CPU. It is a multipurpose 
programmable silicon chip constructed using Metal Oxide Semiconductor 
(MOS) technology which is clock-driven and register-based. It accepts 
binary data as input and provides output after processing it as per the 
specification of instructions stored in the memory. These 
microprocessors are capable of processing 128 bits at a time at the 
speed of one billion instructions per second.

**Characteristics of a microprocessor:**

- **Instruction Set –** The set of complete instructions that the microprocessor executes is termed the instruction set.
- **Word Length –** The number of bits processed in a single instruction is called word length
or word size. Greater the word size, the larger the processing power of
the CPU.
- **System Clock Speed –** Clock speed
determines how fast a single instruction can be executed in a processor. The microprocessor’s pace is controlled by the System Clock. Clock
speeds are generally measured in millions of cycles per second (MHz) and thousand million cycles per second (GHz). Clock speed is considered to
be a very important aspect of predicting the performance of a processor.

**Classification of Microprocessors:** Besides
 the classification based on the word length, the classification is also
 based on the architecture i.e. Instruction Set of the microprocessor. 
These are categorized into RISC and CISC.

1. **RISC:** It stands for Reduced Instruction Set Computer. It is a type of
microprocessor architecture that uses a small set of instructions of
uniform length. These are simple instructions that are generally
executed in one clock cycle. RISC chips are relatively simple to design
and inexpensive. The setback of this design is that the computer has to
repeatedly perform simple operations to execute a larger program having a large number of processing operations. **Examples:** SPARC, POWER PC, etc.
2. **CISC:** It stands for Complex Instruction Set Computer. These processors offer the users, hundreds of instructions of variable sizes. CISC architecture
includes a complete set of special-purpose circuits that carry out these instructions at a very high speed. These instructions interact with
memory by using complex addressing modes. CISC processors reduce the
program size and hence lesser number of memory cycles are required to
execute the programs. This increases the overall speed of execution. **Examples:** Intel architecture, AMD
3. **EPIC:** It stands for Explicitly Parallel Instruction Computing. The best features of RISC and CISC processors are combined in the architecture. It
implements parallel processing of instructions rather than using
fixed-length instructions. The working of EPIC processors is supported
by using a set of complex instructions that contain both basic
instructions as well as the information of execution of parallel
instructions. It substantially increases the efficiency of these
processors.

Below are a few differences between RISC and CISC:

[Untitled Database](https://www.notion.so/ce37338e49094f5ba2d3559d70f14a06?pvs=21)

Refer for Set-1: [RISC and CISC](https://www.geeksforgeeks.org/computer-organization-risc-and-cisc/)
*** Vector processor classification
According
 to from where the operands are retrieved in a vector processor, pipe 
lined vector computers are classified into two architectural 
configurations:

1. **Memory to memory architecture –**In memory to memory architecture, source operands, intermediate and final
results are retrieved (read) directly from the main memory. For memory
to memory vector instructions, the information of the base address, the
offset, the increment, and the vector length must be specified in order
to enable streams of data transfers between the main memory and
pipelines. The processors like *TI-ASC, CDC STAR-100, and Cyber-205* have vector instructions in memory to memory formats. The main points about memory to memory architecture are:
    - There is no limitation of size
    - Speed is comparatively slow in this architecture
2. **Register to register architecture –**In register to register architecture, operands and results are retrieved
indirectly from the main memory through the use of large number of
vector registers or scalar registers. The processors like *Cray-1 and the Fujitsu VP-200* use vector instructions in register to register formats. The main points about register to register architecture are:
    - Register to register architecture has limited size.
    - Speed is very high as compared to the memory to memory architecture.
    - The hardware cost is high in this architecture.

A block diagram of a modern multiple pipeline vector computer is shown below:

!https://media.geeksforgeeks.org/wp-content/uploads/v1-2.png

A typical pipe lined vector processor.

# Vector processor classification

- Difficulty Level :
[Medium](https://www.geeksforgeeks.org/medium/)
- Last Updated :
01 Nov, 2019

According
 to from where the operands are retrieved in a vector processor, pipe 
lined vector computers are classified into two architectural 
configurations:

1. **Memory to memory architecture –**In memory to memory architecture, source operands, intermediate and final
results are retrieved (read) directly from the main memory. For memory
to memory vector instructions, the information of the base address, the
offset, the increment, and the vector length must be specified in order
to enable streams of data transfers between the main memory and
pipelines. The processors like *TI-ASC, CDC STAR-100, and Cyber-205* have vector instructions in memory to memory formats. The main points about memory to memory architecture are:
    - There is no limitation of size
    - Speed is comparatively slow in this architecture
2. **Register to register architecture –**In register to register architecture, operands and results are retrieved
indirectly from the main memory through the use of large number of
vector registers or scalar registers. The processors like *Cray-1 and the Fujitsu VP-200* use vector instructions in register to register formats. The main points about register to register architecture are:
    - Register to register architecture has limited size.
    - Speed is very high as compared to the memory to memory architecture.
    - The hardware cost is high in this architecture.

A block diagram of a modern multiple pipeline vector computer is shown below:

!https://media.geeksforgeeks.org/wp-content/uploads/v1-2.png

A typical pipe lined vector processor.
*** Essential Registers for Instruction Execution
These
 are various registers required for the execution of instruction: 
Program Counter (PC), Instruction Register (IR), Memory Buffer (or Data)
 Register (MBR or MDR), and Memory Address Register (MAR).

These are explained as follows below.

!https://media.geeksforgeeks.org/wp-content/uploads/20200502152418/11144.png

1. **Program Counter (PC) :** It contains the address of an instruction to be executed next. The PC is
updated by the CPU after each instruction is executed so that it always
points to the next instruction to be executed. A branch or skip
instruction will also modify the content of the PC.
2. **Instruction Register (IR) :** it contains the instruction most recently fetched or executed. The fetched instruction is loaded into an IR, where the opcode and operand
specifier is analyzed.
3. **Memory Buffer (or Data) Register (MBR or MDR) :** it contains a word of data to be written to memory are the words most
recently read. Contents of MBR are directly connected to the data bus.
4. **Memory Address Register (MAR) :** It contains the address of a location of main memory from where
information has to be fetched for information has to be stored. Contents of MAR are directly connected to the address bus.

Apart from these registers, we may use other registers which may be invisible to the user, e.g., temporary Buffering registers.
*** Introduction of Single Accumulator based CPU organization
The 
computers, present in the early days of computer history, had 
accumulator-based CPUs. In this type of CPU organization, the 
accumulator register is used implicitly for processing all instructions 
of a program and storing the results into the accumulator. The 
instruction format that is used by this CPU Organisation is the **One address field**. Due to this, the CPU is known as **One Address Machine**.

The main points about Single Accumulator based CPU Organisation are:

1. In this CPU Organization, the first ALU operand is always stored into the
Accumulator and the second operand is present either in Registers or in
the Memory.
2. Accumulator is the default address thus after data manipulation the results are stored into the accumulator.
3. One address instruction is used in this type of organization.

```
The format of instruction is: Opcode + Address
```

Opcode indicates the type of operation to be performed. Mainly two types of operation are performed in a single accumulator based CPU organization:

**1. Data transfer operation –** In this type of operation, the data is transferred from a source to a destination.

```
For ex: LOAD X, STORE Y
```

Here
 LOAD is a memory read operation that is data is transferred from memory
 to accumulator and STORE is a memory write operation that is data is 
transferred from the accumulator to memory.

**2. ALU operation –** In this type of operation, arithmetic operations are performed on the data.

```
For ex: MULT X
```

where X is the address of the operand. The MULT instruction in this example performs the operation,

```
AC <-- AC * M[X]
```

AC is the Accumulator and M[X] is the memory word located at location X.

This type of CPU organization is first used in **PDP-8 processors** and
 is used for process control and laboratory applications. It has been 
totally replaced by the introduction of the new general register-based 
CPU.

**Advantages –**

- One of the operands is always held by the accumulator register. This results in short instructions and less memory space.
- The instruction cycle takes less time because it saves time in instruction fetching from memory.

**Disadvantages –**

- When complex expressions are computed, program size increases due to the
usage of many short instructions to execute it. Thus memory size
increases.
- As the number of instructions increases for a program, the execution time increases.
*** Introduction of Stack based CPU Organization
The computers which use Stack-based CPU Organization are based on a data structure called ****a **stack**. The stack is a list of data words. It uses the **Last In First Out (LIFO)**
 access method which is the most popular access method in most of the 
CPU. A register is used to store the address of the topmost element of 
the stack which is known as **Stack pointer (SP)**. In this
 organization, ALU operations are performed on stack data. It means both
 the operands are always required on the stack. After manipulation, the 
result is placed in the stack.

The main two operations that are performed on the operators of the stack are **Push** and **Pop**. These two operations are performed from one end only.

**1. Push –** This
 operation results in inserting one operand at the top of the stack and 
it decreases the stack pointer register. The format of the PUSH 
instruction is:

```
PUSH
```

It inserts the data word at a specified address to the top of the stack. It can be implemented as:

```
//decrement SP by 1
SP <-- SP - 1

//store the content of specified memory address
//into SP; i.e, at top of stack
SP <-- (memory address)
```

**2. Pop –** This 
operation results in deleting one operand from the top of the stack and 
increasing the stack pointer register. The format of the POP instruction
 is:

```
POP
```

It deletes the data word at the top of the stack to the specified address. It can be implemented as:

```
//transfer the content of  SP (i.e, at top most data)
//into specified memory location
(memory address) <-- SP

//increment SP by 1
SP <-- SP + 1
```

Operation type instruction does not need the 
address field in this CPU organization. This is because the operation is
 performed on the two operands that are on the top of the stack. For 
example:

```
SUB
```

This instruction contains the opcode only
 with no address field. It pops the two top data from the stack, 
subtracting the data, and pushing the result into the stack at the top.

**PDP-11, Intel’s 8085, and HP 3000** are some examples of stack-organized computers.

**The advantages of Stack-based CPU organization –**

- Efficient computation of complex arithmetic expressions.
- Execution of instructions is fast because operand data are stored in consecutive memory locations.
- The length of instruction is short as they do not have an address field.

**The disadvantages of Stack-based CPU organization –**

- The size of the program increases.

**Note:** Stack-based *CPU* organization *uses zero address instruction.*

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Introduction of General Register based CPU Organization
When
 we are using multiple general-purpose registers, instead of a single 
accumulator register, in the CPU Organization then this type of 
organization is known as General register-based CPU Organization. In 
this type of organization, the computer uses two or three address fields
 in their instruction format. Each address field may specify a general 
register or a memory word. If many CPU registers are available for 
heavily used variables and intermediate results, we can avoid memory 
references much of the time, thus vastly increasing program execution 
speed, and reducing program size.

For example:

```
MULT R1, R2, R3
```

This
 is an instruction of an arithmetic multiplication written in assembly 
language. It uses three address fields R1, R2, and R3. The meaning of 
this instruction is:

```
R1 <-- R2 * R3
```

This instruction also can be written using only two address fields as:

```
MULT R1, R2
```

In this instruction, the destination register is the same as one of the source registers. This means the operation

```
R1 <-- R1 * R2
```

The use of a large number of registers results in a short program with limited instructions.

Some examples of General register-based CPU Organizations are **IBM 360 and PDP- 11**.

**The advantages of General register-based CPU organization –**

- The efficiency of the CPU increases as large number of registers are used in this organization.
- Less memory space is used to store the program since the instructions are written in a compact way.

**The disadvantages of General register based CPU organization –**

- Care should be taken to avoid unnecessary usage of registers. Thus, compilers need to be more intelligent in this aspect.
- Since a large number of registers are used, thus extra cost is required in this organization.

**General register CPU organization of two types:**

1. Register-memory reference architecture (CPU with less register) – In this organization Source 1 is always required in the register, source 2 can be present either in the register or in memory. Here two address
instruction formats are compatible instruction formats.
2. Register-register reference architecture (CPU with more register) – In this organization, ALU operations are performed only on registered
data. So operands are required in the register. After manipulation, the
result is also placed in a register. Here three address instruction
formats are the compatible instruction format.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Data Transfer instructions in AVR microcontroller
[Data transfer instructions](https://www.geeksforgeeks.org/data-transfer-instructions-8085-microprocessor/) are the instructions which are used to transfer data into [micro-controller](https://www.geeksforgeeks.org/introduction-to-8051-microcontroller/).

**These instructions can be used to transfer data from :**

- **[Register to Register](https://www.geeksforgeeks.org/register-transfer-language-rtl/) :**In register to register transfer, data transfer from one register to
another register. Consider an example where you have to perform binary
addition.**Example –**In this example first data will transfer to B register and after that It will transfer from B register to Accumulator register.
    
    ```
    MVI B, 05
    MOV A, B
    
    ```
    
- **Register to Memory :**In this data transfer,
Data will be transferred from register to the given memory location.
consider an example where given location is 201k and you have to copy
data from the accumulator.**Example –**
    
    ```
    STA 201K
    
    ```
    
- **Memory to Register :**In this data transfer,
Data will be transferred from memory to the register. consider an
example where given location is 201k and you have to load data from this memory location to the accumulator.**Example –**
    
    ```
    LDA 2020k
    
    ```
    
- **Constant to Register :**In this data transfer,
Data will be transferred to the immediate given register. consider an
example where given Data is 05 and you have to load data to the
accumulator.**Example –**
    
    ```
    MVI A, 05
    
    ```
    

**The following table shows different transfer instructions :**

InstructionOperandExplanationExampleMOVD, SD = SMOV D, SLDSD, K(memory location)D = Value at KLDS D, KLDD, SD = Value at memory location stored in SLD D, SLDID, K(constant)D = KLDI D, KLPMD, Z(flash memory)Store the value in register Z from flash memory into the memory location stored in the D registerLPM D, ZIND, AStores the value in register A in D.where A is from [0, 63](64 I/O Registers)IN D, AOUTA, DStores the value in register D in A.where A is from [0, 63](64 I/O Registers)OUT A, DSTSK, SStores the value in register S into memory location K.STS K, SSTD, SStore the value in register S into the memory location stored in the D registerST D, SPUSHDPushes the content of D on the top of the stackPUSH DPOPDRemoves the topmost entry from the stackand transfers that value to DPOP D

D and S are registers. PUSH and POP instructions are strictly to maintain the stack.
*** Arithmetic instructions in AVR microcontroller
https://www.geeksforgeeks.org/arithmetic-instructions-in-avr-microcontroller/
*** Conditional Branch Instructions in AVR Microcontroller
https://www.geeksforgeeks.org/conditional-branch-instructions-in-avr-microcontroller/
*** CALL Instructions and Stack in AVR Microcontroller
CALL
 is a control transfer instruction that is used to call a particular 
subroutine. A subroutine is a block of instructions that need to be 
performed frequently.

In AVR, there are 4 instructions for the call subroutine as following.

1. **[CALL (call subroutine)](https://www.geeksforgeeks.org/subroutine-subroutine-nesting-and-stack-memory/)**
2. **RCALL (relative call subroutine)**
3. **ICALL (indirect call to Z)**
4. **EICALL (extended indirect call to Z)**

**[CALL](https://www.geeksforgeeks.org/subroutine-subroutine-nesting-and-stack-memory/) :**In
 this 4-byte instruction, 10 bits are used for the opcode and the other 
22 bits are used for the address of the target subroutine just as in the
 JMP instruction. In this, 4M address space of 000000-$3FFFFF for AVR 
and can be used to call subroutines within the given range of address.

To
 make sure that the AVR knows where to come back after the execution of 
the subroutine, the microcontroller automatically saves the address of 
the instruction just below the CALL instruction on the stack. After 
finishing the execution of the subroutine, the RET instruction transfers
 control back to the caller. Hence, every subroutine has a RET 
instruction at the end.

**[Stack](https://www.geeksforgeeks.org/subroutine-subroutine-nesting-and-stack-memory/) :**Stack
 is the part in the RAM of CPU to store information temporarily. The CPU
 needs this storage because there is only a limited number of registers.
 The register used to access the stack is called the stack pointer (SP) 
register.In I/O memory space, there are 2 registers named SPL (the low byte of SP) and SPH (the high byte ofSP). The SP is implemented by these 2 registers.

In
 AVRs with more than 256 bytes of memory have two 8-bit registers. On 
the other hand, if the memory is less than 256 bytes, SP is made up of 
only SPL, as an 8-bit register can only address 256 bytes of memory.

The
 storing of the CPU information on the stack is called the PUSH 
operation, and the loading of the stack contents back into the CPU is 
known as POP operation.

**Pushing onto the stack :**The 
stack pointer (SP) points to the top of the stack. As we push the data 
onto the stack, the data is saved where the SP is pointing to and the SP
 is decremented by one.

To push a register onto a stack, we use PUSH instruction.

```
PUSH  Rr;
Rr can be any general-purpose register (R0 - R31)

```

**Popping from the stack :**Popping the contents of the 
stack back into the register is the opposite function of pushing. When 
the POP instruction is executed, the SP is incremented by one and the 
top location of the stack is copied back to the register. This means 
that the stack is LIFO ( Last In First Out).

To retrieve back the data from the stack, we use POP instruction.

```
POP  Rr;
Rr can be any general-purpose register (R0 - R31)

```

**Initializing stack pointers :**Different AVRs have 
different amounts of RAM. In the AVR assembler, RAMEND specifies the 
address of the last RAM location. So, if we want to initialize the SP so
 that it points to the last memory location, we can simply load RAMEND 
into the SP. Notice that SP is made up of 2 registers, SPH and SPL. So, 
we load the high byte of RAMEND into SPH and the low byte of RAMEND into
 SPL.

**CALL instruction, RET instruction and the role of stack :**When
 the CALL instruction is executed, the address of the instruction below 
the CALL instruction is pushed onto the stack. When the execution of 
that subroutine is finished and RET is executed, the address of the 
instruction below the CALL instruction is loaded in the program counter 
and it is executed.
*** Logical Instructions in AVR Microcontroller
https://www.geeksforgeeks.org/logical-instructions-in-avr-microcontroller/
*** Data Manipulation Instructions in Computer Organization
**Data Manipulation Instructions :**Data
 manipulation instructions perform operations on data and provide the 
computational capabilities for the computer. The data manipulation 
instructions in a typical computer usually divided into three basic 
types as follows.

1. Arithmetic instructions
2. Logical and bit manipulation instructions
3. Shift instructions

Let’s discuss one by one.

1. **Arithmetic instructions :**The four basic arithmetic operations are addition, subtraction,
multiplication, and division. Most computers provide instructions for
all four operations.
    
    **Typical Arithmetic Instructions –**
    
    [Untitled Database](https://www.notion.so/11b20efa4ad741bf8b8b5a9b8b51d0d7?pvs=21)
    
2. **Logical and Bit Manipulation Instructions :**Logical instructions perform binary operations on strings of bits stored in
registers. They are useful for manipulating individual bits or a group
of bits.
    
    **Typical Logical and Bit Manipulation Instructions –**
    
    [Untitled Database](https://www.notion.so/cb341d1caec04f1781b44cb8fab37437?pvs=21)
    
3. **Shift Instructions :**Shifts are operations in which the bits of a word are moved to the left or
right. Shift instructions may specify either logical shifts, arithmetic
shifts, or rotate-type operations.
    
    **Typical Shift Instructions –**
    
    [Untitled Database](https://www.notion.so/153d64b4a9a143c295e403a72498c99d?pvs=21)
    
    For Shift Instructions, refer to this [Reference for Shift Instructions](https://www.geeksforgeeks.org/shift-micro-operations-in-computer-architecture/)
    
*** Machine Control Instructions in Microprocessor
These
 type of instructions control machine functions such as Halt, Interrupt,
 or do nothing. This type of instructions alters the different type of 
operations executed in the processor.

Following are the type of Machine control instructions: 

```
1. NOP (No operation)
2. HLT (Halt)
3. DI (Disable interrupts)
4. EI (Enable interrupts)
5. SIM (Set interrupt mask)
6. RIM (Reset interrupt mask)
```

1. **NOP (No operation) –** 

```
Opcode- NOP
Operand- None
Length- 1 byte
M-Cycles- 1
T-states- 4
Hex code- 00
```

1. It is used when no operation is performed.
No flags are affected during the execution of NOP. The instruction is
used to fill in time delay or to delete and insert instructions while
troubleshooting.
2. **HLT (Halt and enter wait state) –** 

```
Opcode- HLT
Operand- None
Length- 1 byte
M-Cycles- 2 or more
T-states- 5 or more
Hex code- 76
```

1. The Microprocessor finishes executing the
current instruction and halts any further execution. The contents of the registers are unaffected during the HLT state.
2. **DI (Disable interrupts) –** 

```
Opcode- DI
Operand- None
Length- 1 byte
M-Cycles- 1
T-states- 4
Hex code- F3
```

1. Disable interrupt is used when the
execution of a code sequence cannot be interrupted. For example, in
critical time delays, this instruction is used at the beginning of the
code and the interrupts are enabled at the end of the code. The 8085
TRAP cannot be disabled.
2. **EI (Enable interrupts) –** 

```
Opcode- EI
Operand- None
Length- 1 byte
M-Cycles- 1
T-states- 4
Hex code- FB
```

1. After a system reset or the acknowledgement of an interrupt, the Interrupt Enable the flip-flop is reset, thus
disabling the interrupts.
2. **SIM (Set interrupt mask) –** 

```
Opcode- SIM
Operand- None
Length- 1 byte
M-Cycles- 1
T-states- 4
Hex code- 30
```

1. This SIM instruction is used to
implementation of different interrupts of 8085 microprocessor like RST
7.5, 6.5 and 5.5 and also serial data output. It does not affect TRAP
interrupt.
2. **RIM (Reset interrupt mask) –** 

```
Opcode- RIM
Operand- None
Length- 1 byte
M-Cycles- 1
T-states- 4
Hex code- 20
```

1. This is a multipurpose instruction used to read the status of 8085 interrupts 7.5, 6.5, 5.5 and to read serial data input bit.
*** Very Long Instruction Word (VLIW) Architecture
The 
limitations of the Superscalar processor are prominent as the difficulty
 of scheduling instruction becomes complex. The intrinsic parallelism in
 the instruction stream, complexity, cost, and the branch instruction 
issue get resolved by a higher instruction set architecture called the **Very Long Instruction Word (VLIW)** or **VLIW Machines**.

VLIW uses [Instruction Level Parallelism](https://www.geeksforgeeks.org/instruction-level-parallelism/),
 i.e. it has programs to control the parallel execution of the 
instructions. In other architectures, the performance of the processor 
is improved by using either of the following methods: pipelining (break 
the instruction into subparts), superscalar processor (independently 
execute the instructions in different parts of the processor), 
out-of-order-execution (execute orders differently to the program) but 
each of these methods add to the complexity of the hardware very much. 
VLIW Architecture deals with it by depending on the compiler. The 
programs decide the parallel flow of the instructions and to resolve 
conflicts. This increases compiler complexity but decreases hardware 
complexity by a lot.

**Features :**

- The
processors in this architecture have multiple functional units, fetch
from the Instruction cache that have the Very Long Instruction Word.
- Multiple independent operations are grouped together in a single VLIW Instruction. They are initialized in the same clock cycle.
- Each operation is assigned an independent functional unit.
- All the functional units share a common register file.
- Instruction words are typically of the length 64-1024 bits depending on the number
of execution unit and the code length required to control each unit.
- Instruction scheduling and parallel dispatch of the word is done statically by the compiler.
- The compiler checks for dependencies before scheduling parallel execution of the instructions.

!https://media.geeksforgeeks.org/wp-content/uploads/20201030224132/BlockDiagramofVLIW.jpg

Block Diagram of VLIW Architecture

!https://media.geeksforgeeks.org/wp-content/uploads/20201030224223/TimeSpaceDrawing.jpg

Time Space Diagram of VLIW Processor where 4 instructions are executed in parallel in a single instruction word

**Advantages :**

- Reduces hardware complexity.
- Reduces power consumption because of reduction of hardware complexity.
- Since compiler takes care of data dependency check, decoding, instruction issues, it becomes a lot simpler.
- Increases potential clock rate.
- Functional units are positioned corresponding to the instruction pocket by compiler.

**Disadvantages :**

- Complex compilers are required which are hard to design.
- Increased program code size.
- Larger memory bandwidth and register-file bandwidth.
- Unscheduled events, for example a cache miss could lead to a stall which will stall the entire processor.
- In case of un-filled opcodes in a VLIW, there is waste of memory space and instruction bandwidth.
** Instruction Design and Format
*** Computer Organization | Different Instruction Cycles
Prerequisite – [Execution, Stages and Throughput](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-1-execution-stages-and-throughput/)

Registers Involved In Each Instruction Cycle: 

- **Memory address registers(MAR)** : It is connected to the address lines of the system bus. It specifies the address in memory for a read or write operation.
- **Memory Buffer Register(MBR)** : It is connected to the data lines of the system bus. It contains the
value to be stored in memory or the last value read from the memory.
- **Program Counter(PC)** : Holds the address of the next instruction to be fetched.
- **Instruction Register(IR)** : Holds the last instruction fetched.

**The Instruction Cycle –**

Each
 phase of Instruction Cycle can be decomposed into a sequence of 
elementary micro-operations. In the above examples, there is one 
sequence each for the *Fetch, Indirect, Execute and Interrupt Cycles*. 

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-29-00-57-59.png

The *Indirect Cycle* is always followed by the *Execute Cycle*. The *Interrupt Cycle* is always followed by the *Fetch Cycle*. For both fetch and execute cycles, the next cycle depends on the state of the system.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-29-09-49-05.png

We assumed a new 2-bit register called *Instruction Cycle Code* (ICC). The ICC designates the state of processor in terms of which portion of the cycle it is in:-

00 : Fetch Cycle 01 : Indirect Cycle 10 : Execute Cycle 11 : Interrupt Cycle

At the end of the each cycles, the ICC is set appropriately. The above flowchart of *Instruction Cycle*
 describes the complete sequence of micro-operations, depending only on 
the instruction sequence and the interrupt pattern(this is a simplified 
example). The operation of the processor is described as the performance
 of a sequence of micro-operation.

Different Instruction Cycles:

- **The Fetch Cycle –** At the beginning of the fetch cycle, the address of the next instruction to be executed is in the *Program Counter*(PC).

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-27-23-37-15.png

- Step 1: The address in the program counter is moved to the memory address
register(MAR), as this is the only register which is connected to
address lines of the system bus.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-27-23-45-16.png

- Step 2: The address in MAR is placed on the address bus, now the control
unit issues a READ command on the control bus, and the result appears on the data bus and is then copied into the memory buffer register(MBR).
Program counter is incremented by one, to get ready for the next
instruction. (These two action can be performed simultaneously to save
time)

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-27-23-49-10.png

- Step 3: The content of the MBR is moved to the instruction register(IR).

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-27-23-58-06.png

- Thus, a simple *Fetch Cycle* consist of three steps and four micro-operation. Symbolically, we can write these sequence of events as follows:-

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-00-16-10-2.png

- Here ‘I’ is the instruction length. The notations (t1, t2, t3) represents
successive time units. We assume that a clock is available for timing
purposes and it emits regularly spaced clock pulses. Each clock pulse
defines a time unit. Thus, all time units are of equal duration. Each
micro-operation can be performed within the time of a single time unit. First time unit: Move the contents of the PC to MAR. Second time unit: Move contents of memory location specified by MAR to MBR. Increment content of PC by I. Third time unit: Move contents of MBR to IR. **Note:** Second and third micro-operations both take place during the second time unit.
- **The Indirect Cycles –**
    
    Once an instruction is fetched, the next step is to fetch source operands. *Source Operand* is being fetched by indirect addressing( it can be fetched by any [addressing mode](https://www.geeksforgeeks.org/addressing-modes/),
     here its done by indirect addressing). Register-based operands need not
     be fetched. Once the opcode is executed, a similar process may be 
    needed to store the result in main memory. Following *micro-operations* takes place:-
    

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-23-10-36.png

- Step 1: The address field of the instruction is transferred to the MAR. This is used to fetch the address of the operand. Step 2: The address field of the IR is updated from the MBR.(So that it now
contains a direct addressing rather than indirect addressing) Step 3: The IR is now in the state, as if indirect addressing has not been occurred.
    
    **Note:** Now IR is ready for the execute cycle, but it skips that cycle for a moment to consider the *Interrupt Cycle* .
    
- **The Execute Cycle**
    
    The other three cycles(*Fetch, Indirect and Interrupt*)
     are simple and predictable. Each of them requires simple, small and 
    fixed sequence of micro-operation. In each case same micro-operation are
     repeated each time around. Execute Cycle is different from them. 
    Like, for a machine with N different opcodes there are N different 
    sequence of micro-operations that can occur. Lets take an hypothetical example :- consider an add instruction:
    

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-22-50-12.png

- Here, this instruction adds the content of location X to register R. Corresponding micro-operation will be:-

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-23-07-52.png

- We begin with the IR containing the ADD instruction. Step 1: The address portion of IR is loaded into the MAR. Step 2: The address field of the IR is updated from the MBR, so the reference memory location is read. Step 3: Now, the contents of R and MBR are added by the ALU.
    
    Lets take a complex example :-
    

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-23-15-27.png

- Here, the content of location X is incremented by 1. If the result is 0, the
next instruction will be skipped. Corresponding sequence of
micro-operation will be :-

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-28-23-22-31.png

- Here, the PC is incremented if (MBR) = 0. This test (is MBR equal to zero or
not) and action (PC is incremented by 1) can be implemented as one
micro-operation. **Note** : This test and action
micro-operation can be performed during the same time unit during which
the updated value MBR is stored back to memory.
- **The Interrupt Cycle**: At the completion of the Execute Cycle, a test is made to determine
whether any enabled interrupt has occurred or not. If an enabled
interrupt has occurred then Interrupt Cycle occurs. The nature of this
cycle varies greatly from one machine to another. Lets take a sequence of micro-operation:-

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-29-00-07-19.png

- Step 1: Contents of the PC is transferred to the MBR, so that they can be saved for return. Step 2: MAR is loaded with the address at which the contents of the PC are to be saved. PC is loaded with the address of the start of the interrupt-processing routine. Step 3: MBR, containing the old value of PC, is stored in memory.
    
    **Note:**
     In step 2, two actions are implemented as one micro-operation. However,
     most processor provide multiple types of interrupts, it may take one or
     more micro-operation to obtain the save_address and the routine_address
     before they are transferred to the MAR and PC respectively.
     
*** Essential Registers for Instruction Execution
*** Machine Instructions-set
Machine Instructions are commands or programs written in machine code of a machine (computer) that it can recognize and execute.

- A machine instruction consists of several bytes in memory that tells the processor to perform one machine operation.
- The processor looks at machine instructions in main memory one after
another, and performs one machine operation for each machine
instruction.
- The collection of machine instructions in main memory is called a **machine language program.**

Machine
 code or machine language is a set of instructions executed directly by a
 computer’s central processing unit (CPU). Each instruction performs a 
very specific task, such as a load, a jump, or an ALU operation on a 
unit of data in a CPU register or memory. Every program directly 
executed by a CPU is made up of a series of such instructions.

The general format of a **machine instruction** is

**[Label:] Mnemonic [Operand, Operand] [; Comments]**

- Brackets indicate that a field is optional
- Label is an identifier that is assigned the address of the first byte of the
instruction in which it appears. It must be followed by **“:”**
- Inclusion of spaces is arbitrary, except that at least one space must be inserted; no space would lead to an ambiguity.
- Comment field begins with a semicolon **“ ; ”**

**Example:**

**Here: MOV R5,#25H;load 25H into R5**

**Machine instructions used in 8086 microprocessor**

**1. Data transfer instructions**– move, load exchange, input, output.

- MOV: Move byte or word to register or memory .
- IN, OUT: Input byte or word from port, output word to port.
- LEA: Load effective address
- LDS, LES Load pointer using data segment, extra segment .
- PUSH, POP: Push word onto stack, pop word off stack.
- XCHG: Exchange byte or word.
- XLAT: Translate byte using look-up table.

**2. Arithmetic instructions** – add, subtract, increment, decrement, convert byte/word and compare.

- ADD, SUB: Add, subtract byte or word
- ADC, SBB: Add, subtract byte or word and carry (borrow).
- INC, DEC: Increment, decrement byte or word.
- NEG: Negate byte or word (two’s complement).
- CMP: Compare byte or word (subtract without storing).
- MUL, DIV: Multiply, divide byte or word (unsigned).
- IMUL, IDIV: Integer multiply, divide byte or word (signed)
- CBW, CWD: Convert byte to word, word to double word
- AAA, AAS, AAM ,AAD: ASCII adjust for add, sub, mul, div .
- DAA, DAS: Decimal adjust for addition, subtraction (BCD numbers)

**3. Logic instructions** – AND, OR, exclusive OR, shift/rotate and test

- NOT: Logical NOT of byte or word (one’s complement)
- AND: Logical AND of byte or word
- OR: Logical OR of byte or word.
- XOR: Logical exclusive-OR of byte or word
- TEST: Test byte or word (AND without storing).
- SHL, SHR: Logical Shift rotate instruction shift left, right byte or word? by 1or CL
- SAL, SAR: Arithmetic shift left, right byte or word? by 1 or CL
- ROL, ROR: Rotate left, right byte or word? by 1 or CL.
- RCL, RCR: Rotate left, right through carry byte or word? by 1 or CL.
1. **String manipulation instruction** – load, store, move, compare and scan for byte/word
- MOVS: Move byte or word string
- MOVSB, MOVSW: Move byte, word string.
- CMPS: Compare byte or word string.
- SCAS S: can byte or word string (comparing to A or AX)
- LODS, STOS: Load, store byte or word string to AL.

**5. Control transfer instructions** – conditional, unconditional, call subroutine and return from subroutine.

- JMP: Unconditional jump .it includes loop transfer and subroutine and interrupt instructions.
- JNZ: jump till the counter value decreases to zero. It runs the loop till the value stored in CX becomes zero

**6. Loop control instructions-**

- LOOP: Loop unconditional, count in CX, short jump to target address.
- LOOPE (LOOPZ): Loop if equal (zero), count in CX, short jump to target address.
- LOOPNE (LOOPNZ): Loop if not equal (not zero), count in CX, short jump to target address.
- JCXZ: Jump if CX equals zero (used to skip code in loop).
- Subroutine and Interrupt instructions-
- CALL, RET: Call, return from procedure (inside or outside current segment).
- INT, INTO: Software interrupt, interrupt if overflow.IRET: Return from interrupt.

**7. Processor control instructions-**

Flag manipulation:

- STC, CLC, CMC: Set, clear, complement carry flag.
- STD, CLD: Set, clear direction flag.STI, CLI: Set, clear interrupt enable flag.
- PUSHF, POPF: Push flags onto stack, pop flags off stack.

**Sample GATE Question**

Consider the sequence of machine instructions given below:

```
  MUL R5, R0, R1
  DIV R6, R2, R3
  ADD R7, R5, R6
  SUB R8, R7, R4
```

In the above sequence, R0 to R8 are general 
purpose registers. In the instructions shown, the first register stores 
the result of the operation performed on the second and the third 
registers. This sequence of instructions is to be executed in a 
pipelined instruction processor with the following 4 stages: (1) 
Instruction Fetch and Decode (IF), (2) Operand Fetch (OF), (3) Perform 
Operation (PO) and (4) Write back the Result (WB). The IF, OF and WB 
stages take 1 clock cycle each for any instruction. The PO stage takes 1
 clock cycle for ADD or SUB instruction, 3 clock cycles for MUL 
instruction and 5 clock cycles for DIV instruction. The pipelined 
processor uses operand forwarding from the PO stage to the OF stage. The
 number of clock cycles taken for the execution of the above sequence of
 instructions is ___________**(A)** 11**(B)** 12**(C)** 13**(D)** 14

Answer: (C)

Explanation:

```
 1   2   3   4   5   6   7   8   9   10   11   12   13
  IF  OF  PO  PO  PO  WB
      IF  OF          PO  PO  PO  PO  PO   WB
          IF          OF                   PO   WB
              IF          OF                    PO   WB
```

Article
 Contributed by Pooja Taneja. Please write comments if you find anything
 incorrect, or you want to share more information about the topic 
discussed above.
*** Computer Organization | Instruction Formats (Zero, One, Two and Three Address Instruction)
A 
computer performs a task based on the instruction provided. Instruction 
in computers comprises groups called fields. These fields contain 
different information as for computers everything is in 0 and 1 so each 
field has different significance based on which a CPU decides what to 
perform. The most common fields are:

- Operation field specifies the operation to be performed like addition.
- Address field which contains the location of the operand, i.e., register or memory location.
- Mode field which specifies how operand is to be founded.

Instruction
 is of variable length depending upon the number of addresses it 
contains. Generally, CPU organization is of three types based on the 
number of address fields:

1. Single Accumulator organization
2. General register organization
3. Stack organization

In
 the first organization, the operation is done involving a special 
register called the accumulator. In second on multiple registers are 
used for the computation purpose. In the third organization the work on 
stack basis operation due to which it does not contain any address 
field. Only a single organization doesn’t need to be applied, a blend of
 various organizations is mostly what we see generally.

Based on the number of address, instructions are classified as:

Note that we will use X = (A+B)*(C+D) expression to showcase the procedure.

1. **Zero Address Instructions –** 

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing4-1.jpg

A
 stack-based computer does not use the address field in the instruction.
 To evaluate an expression first it is converted to reverse Polish 
Notation i.e. Postfix Notation.

```
Expression: X = (A+B)*(C+D)
Postfixed : X = AB+CD+*
TOP means top of stack
M[X] is any memory location
```

[Untitled Database](https://www.notion.so/be54e429eb8e4e9b9ef4bcc58cbeb850?pvs=21)

**2 .One Address Instructions –** This
 uses an implied ACCUMULATOR register for data manipulation. One operand
 is in the accumulator and the other is in the register or memory 
location. Implied means that the CPU already knows that one operand is 
in the accumulator so there is no need to specify it.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing2.png

```
Expression: X = (A+B)*(C+D)
AC is accumulator
M[] is any memory location
M[T] is temporary location
```

[Untitled Database](https://www.notion.so/26856e7e3444499d856ba44c33967a1a?pvs=21)

**3.Two Address Instructions –** This
 is common in commercial computers. Here two addresses can be specified 
in the instruction. Unlike earlier in one address instruction, the 
result was stored in the accumulator, here the result can be stored at 
different locations rather than just accumulators, but require more 
number of bit to represent address. 

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing6-1.png

Here destination address can also contain operand.

```
Expression: X = (A+B)*(C+D)
R1, R2 are registers
M[] is any memory location
```

[Untitled Database](https://www.notion.so/2ffb567239f4446a889a1652346531b8?pvs=21)

**4.Three Address Instructions –** This
 has three address field to specify a register or a memory location. 
Program created are much short in size but number of bits per 
instruction increase. These instructions make creation of program much 
easier but it does not mean that program will run much faster because 
now instruction only contain more information but each micro operation 
(changing content of register, loading address in address bus etc.) will
 be performed in one cycle only.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing5.png

```
Expression: X = (A+B)*(C+D)
R1, R2 are registers
M[] is any memory location
```

[Untitled Database](https://www.notion.so/dc1fa33b7e9c43b3aa26d8b876ca2158?pvs=21)
*** Difference between 2-address instruction and 1-address instructions
Prerequisite – [Instruction Formats](https://www.geeksforgeeks.org/computer-organization-instruction-formats-zero-one-two-three-address-instruction/)**1. Two-Address Instructions :**Two-address
 instruction is a format of machine instruction. It has one opcode and 
two address fields. One address field is common and can be used for 
either destination or source and other address field for source.

!https://media.geeksforgeeks.org/wp-content/uploads/20200514155633/2-address1.png

**Example:**

```
X = (A + B) x (C + D)
```

**Solution:**

```
MOV R1, A      R1 <- M[A]
ADD R1, B      R1 <- R1 + M[B]
MOV R2, C      R2 <- M[C]
ADD R2, D      R2 <- R2 + D
MUL R1, R2     R1 <- R1 x R2
MOV X, R1      M[X] <- R1
```

**2. One-Address Instructions :**One-Address instruction is also a format of machine instruction. It has only two fields. One for opcode and other for operand.

!https://media.geeksforgeeks.org/wp-content/uploads/20200516094331/1-address-1.png

**Example:**

```
X = (A + B) x (C + D)
```

**Solution:**

```
LOAD A      AC <- M[A]
ADD B       AC <- AC + M[B]
STORE T     M[T] <- AC
LOAD C      AC <- M[C]
ADD D       AC <- AC + M[D]
MUL T       AC <- AC x M[T]
STORE X     M[X] <- AC
```

**Difference between Two-Address Instruction and One-Address Instruction :**

TWO-ADDRESS INSTRUCTIONONE-ADDRESS INSTRUCTIONIt has three fields.It has only two fields.It has one field for opcode and two fields for address.It has one field for opcode and one field for address.It has long instruction length as compared to one-address.While it has shorter instruction length.It is slower accessing location inside processor than memory.It is faster accessing location inside processor than memory.It generally needs two memory accesses.It generally needs one memory accesses.There may be three memory accesses needed for an instruction.There is a single memory access needed for an instruction.It can’t completely eliminate three memory access.It eliminates two memory access completely.
*** Difference between 3-address instruction and 0-address instruction
Prerequisite – [Instruction Formats](https://www.geeksforgeeks.org/computer-organization-instruction-formats-zero-one-two-three-address-instruction/)**1. Three-Address Instructions :**Three-address
 instruction is a format of machine instruction. It has one opcode and 
three address fields. One address field is used for destination and two 
address fields for source.

!https://media.geeksforgeeks.org/wp-content/uploads/20200514155557/3-address1.png

**Example:**

```
X = (A + B) x (C + D)
```

**Solution:**

```
ADD R1, A, B      R1 <- M[A] + M[B]
ADD R2, C, D      R2 <- M[C] + M[D]
MUL X, R1, R2     M[X] <- R1 x R2
```

**2. Zero-Address Instructions :**Zero-address instruction is a format of machine instruction. It has one opcode and no address fields.

!https://media.geeksforgeeks.org/wp-content/uploads/20200514164137/0-address.png

**Example:**

```
X = (A + B) x (C + D)
```

**Solution:**

```
LOAD A      AC <- M[A]
PUSH A      TOS <- A
PUSH B      TOS <- B
ADD         TOS <- (A + B)
PUSH C      TOS <- C
PUSH D      TOS <- D
ADD         TOS <- (C + D)
MUL         TOS <- (C + D) x (A + B)
POP X       M[X] <- TOS
```

**Difference between Three-Address Instruction and Zero-Address Instruction :**

THREE-ADDRESS INSTRUCTIONZERO-ADDRESS INSTRUCTIONIt has four fields.It has only one field.It has one field for opcode and three fields for address.It has one field for opcode and no fields for address.It has long instruction length.It has shorter instruction.It is slower accessing location inside processor than memory.It is faster accessing location inside processor than memory.There is distinct address fields for destination and source.There is no address field common for destination and source.In 3-address format, destination address can not contain operand.While in 0-address format, there is no field for operand.In 3-address format, number of instructions are less.While in 0-address format, number of instructions are more.It may need three memory accesses for one instruction.It does not need three memory accesses.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Difference between 3-address instruction and 2-address instructions
Prerequisite – [Instruction Formats](https://www.geeksforgeeks.org/computer-organization-instruction-formats-zero-one-two-three-address-instruction/)**1. Three-Address Instructions :**Three-address
 instruction is a format of machine instruction. It has one opcode and 
three address fields. One address field is used for destination and two 
address fields for source.

!https://media.geeksforgeeks.org/wp-content/uploads/20200514155557/3-address1.png

**Example –**

```
X = (A + B) x (C + D)
```

**Solution:**

```
ADD R1, A, B      R1 <- M[A] + M[B]
ADD R2, C, D      R2 <- M[C] + M[D]
MUL X, R1, R2     M[X] <- R1 x R2
```

**2. Two-Address Instructions :**Two-address
 instruction is a format of machine instruction. It has one opcode and 
two address fields. One address field is common and can be used for 
either destination or source and other address field for source.

!https://media.geeksforgeeks.org/wp-content/uploads/20200514155633/2-address1.png

**Example –**

```
X = (A + B) x (C + D)
```

**Solution:**

```
MOV R1, A      R1 <- M[A]
ADD R1, B      R1 <- R1 + M[B]
MOV R2, C      R2 <- M[C]
ADD R2, D      R2 <- R2 + D
MUL R1, R2     R1 <- R1 x R2
MOV X, R1      M[X] <- R1
```

**Difference between Three-Address Instruction and Two-Address Instruction :**

THREE-ADDRESS INSTRUCTIONTWO-ADDRESS INSTRUCTIONIt has four fields.It has three fields.It has one field for opcode and three fields for address.It has one field for opcode and two fields for address.It has long instruction length.It has shorter instruction.It is slower accessing location inside processor than memory.It is faster accessing location inside processor than memory.There is distinct address fields for destination and source.There is one address field common for destination and source.In 3-address format, destination address can not contain operand.While in 2-address format, destination address can have operand.In 3-address format, instructions are less.While in 2-address format, instructions are more.It generally requires three memory access.It generally requires two memory access but in some cases it requires three memory access too.In 3-address format, three memory access is required.While in 2-address format, it eliminated three memory access but not completely.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Register content and Flag status after Instructions
Basically, you are given a set of instructions and the initial content of the registers and flags of [8085 microprocessor](https://www.geeksforgeeks.org/pin-diagram-8085-microprocessor/). You have to find the content of the [registers](https://www.geeksforgeeks.org/registers-8085-microprocessor/) and [flag status](https://www.geeksforgeeks.org/flag-register-8085-microprocessor/) after each instruction.

**Initially,**

!https://media.geeksforgeeks.org/wp-content/uploads/1-347.png

Below is the set of the instructions:

```
SUB A
MOV B, A
DCR B
INR B
SUI 01H
HLT
```

**Assumption:**Each 
instruction will use the result of the previous instruction for 
registers. Following is the description of each instruction with 
register content and flag status:

- **Instruction-1:SUB A** instruction will subtract the content of the accumulator itself. It is
used to clear the content of the accumulator. After this operation the
content of the registers and flags will be like figure given below.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/2-257.png
    
- **Instruction-2:MOV B, A** will copy the content from source register (A) to the destination
register (B). Since it is the Data Transfer instruction so it will not
affect any flag. After this operation the content of the registers and
flags will be like figure given below.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/3-184.png
    
- **Instruction-3:DCR B** will decrease the content of the register B by 1. DCR operation doesn’t affect Carry flag(CY).
    
    ```
    B-00H 0 0 0 0  0 0 0 0
    ```
    
    For DCR B takes the 2’s complement of the 01H, 2’s Complement of 01H:
    
    ```
     0 0 0 0  0 0 0 1
     1 1 1 1  1 1 1 0  (1's complement)
                  + 1
    ------------------
     1 1 1 1  1 1 1 1
    ------------------
    
    +(00)  0 0 0 0  0 0 0 0
    -----------------------
           1 1 1 1  1 1 1 1
    ----------------------
    ```
    
    (FFH) this will be the content of the 
    B. So after this operation the content of the registers and flag will be
     like figure given below.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/4-116.png
    
- **Instruction-4:INR B** will increase the content of the register B by 1. INR operation doesn’t affect Carry flag(CY).
    
    ```
    B(FFH)
            1 1 1 1  1 1 1 1
    +(01)   0 0 0 0  0 0 0 1
           ------------------
    CY=1    0 0 0 0  0 0 0 0
           ------------------
    ```
    
    (0 0 0 0 0 0 0 0) will be the content
     of the register B. So after this operation the content of the registers
     and flag will be like figure given below.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/5-87.png
    
- **Instruction-5:SUI 01H** will subtract 01H from the content of the accumulator and store the result in the accumulator.
    
    ```
    A-00H  0 0 0 0  0 0 0 0
    ```
    
    For SUI 01H takes the 2’s complement of the 01H, 2’s Complement of 01H:
    
    ```
           0 0 0 0  0 0 0 1
           1 1 1 1  1 1 1 0  (1's complement)
                        + 1
          ------------------
           1 1 1 1  1 1 1 1
          ------------------
    +(00)  0 0 0 0  0 0 0 0   (Content of the accumulator)
        -----------------------
           1 1 1 1  1 1 1 1
    ```
    
    (FFH) this will store in the 
    Accumulator. After this operation the content of the registers and flag 
    will be like figure given below.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/6-64.png
    
    HLT will terminate the execution of program.
    
*** Debugging a machine level program
Debugging
 is the process of identifying and removing bug from software or 
program. It refers to identification of errors in the program logic, 
machine codes, and execution. It gives step by step information about 
the execution of code to identify the fault in the program.

**Debugging of machine code:**
 Translating the assembly language to machine code is similar to 
building a circuit from a schematic diagram. Debugging can help in 
determining:

- Values of register.
- Flow of program.
- Entry and exit point of a function.
- Entry into *if* or *else* statement.
- Looping of code.
- Calculation check.

**Common sources of error:**

- Selecting a wrong code
- Forgetting second or third byte of instruction
- Specifying wrong jump locations
- Not reversing the order of high and low bytes in a Jump instruction
- Writing memory addresses in decimal instead of hexadecimal
- Failure to clear accumulator when adding two numbers
- Failure to clear carry registers
- Failure to set flag before Jump instruction
- Specifying wrong memory address on Jump instruction
- Use of improper combination of rotate instructions

The debugging process is divided into two parts:

1. **Static Debugging:** It is similar to visual inspection of circuit board, it is done by a paper and pencil to check the flowchart and machine codes. It is used to the
understanding of code logic and structure of program.
2. **Dynamic Debugging:** It involves observing the contents of register or output after
execution of each instruction (in single step technique) or a group of
instructions (in breakpoint technique).In a single board microprocessor, techniques and tools commonly used in dynamic debugging are:
    1. **Single Step:** This technique allows to execute one instruction at a time and observe
    the results of each instruction. Generally, this is build using
    hard-wired logic circuit. As we press the single step run key we will be able to observe the contents of register and memory location. This
    helps to spot:
        - incorrect addresses
        - incorrect jump location in loops
        - incorrect data or missing codes
        
        However,
         if there is large loop then single step debugging can be very tiring 
        and time-consuming. So instead of running the loop *n* times, we 
        can reduce the number of iteration to check the effectiveness of the 
        loop. The single step technique is very useful for short programs.
        
    2. **Breakpoint:** The breakpoint facility is usually a software routine that allows users to execute a program in sections. The breakpoints can be set using RST
    instruction. When we push the Execute key, the program will be executed
    till the breakpoint. The registers can be examined for the expected
    result. With the breakpoint facility, isolate the segment of program
    with errors. Then that segment can be debugged using the single-step
    facility. It is usually used to check:
        - Timing loop
        - I/O section
        - Interrupts
    3. **Register Examine:** The register examine key allows you to examine the contents of the
    microprocessor register. This technique is used in conjunction with
    either single-step or breakpoint facility.
    
*** Vector Instruction Format in Vector Processors
Different **Instruction formats**
 are used by different vector processors. Vector instructions are 
generally specified by some fields. The main fields that are used in **vector instruction set** are given below:

1. **Operations Code (Opcode) –**The operation code must be specified to select the functional unit or to
reconfigure a multi-functional unit to perform the specified operation
dictated by this field. Usually, microcode control is used to set up the required resources.
    
    **For example:***Opcode – 0001 mnemonic – ADD operation – add the content of memory to the content of accumulatorOpcode – 0010 mnemonic – SUB operation – subtract the content of memory to the content of accumulatorOpcode – 1111 mnemonic – HLT operation – stop processing*
    
2. **Base addresses –**For a memory reference instruction, the base addresses are needed for both
source operands and result vectors. The designated vector registers must be specified in the instruction, if the operands and results are
located in the vector register file, i.e., collection of registers.
    
    **For example:**
    
    ```
    ADD R1, R2
    ```
    
    Here, R1 and R2 are the addresses of the register.
    
3. **Offset (or Displacement) –**This field is required to get the effective memory address of operand
vector. The address offset relative to the base address should be
specified. Using the base address and the offset (positive or negative), the **effective address** is calculated.
4. **Address Increment –**The address increment between the scalar elements of vector operand must be specified. Some computers, i.e., the increment is always 1. Some other
computers, like **TI-ASC**, can have a variable increment, which offers higher flexibility in application.
    
    **For example:**
    
    ```
    R1 <- 400
    ```
    
    Auto incr-R1 is incremented the value of R1 by 1.
    
    ```
    R1 = 399
    ```
    
5. **Vector length –**The vector length (positive integer) is needed to determine the termination of a vector instruction.
*** Vector instruction types
A **Vector operand** contains an ordered set of n elements, where n is called the **length of the vector**.
 All elements in a vector are same type scalar quantities, which may be a
 floating point number, an integer, a logical value, or a character.

Four primitive types of vector instructions are:

```
f1 : V --> V
f2 : V --> S
f3 : V x V --> V
f4 : V x S --> V
```

Where V and S denotes a vector operand and a scalar operand, respectively.

The instructions, f1 and f2 are unary operations and f3 and f4 are binary operations.The **VCOM (vector complement)**,
 which complements each complement of the vector, is an f1 operation. 
The pipe lined implementation of f1 operation is shown in the figure:

!https://media.geeksforgeeks.org/wp-content/uploads/F1-300x286.png

The **VMAX (vector maximum)**,
 which finds the maximum scalar quantity from all the complements in the
 vector, is an f2 operation. The pipe lined implementation of f2 
operation is shown in the figure:

!https://media.geeksforgeeks.org/wp-content/uploads/F2-300x286.png

The **VMPL (vector multiply)**,
 which multiply the respective scalar components of two vector operands 
and produces another product vector, is an f3 operation. The pipe lined 
implementation of f3 operation is shown in the figure:

!https://media.geeksforgeeks.org/wp-content/uploads/F3-300x286.png

The **SVP (scalar vector product)**,
 which multiply one constant value to each component of the vector, is 
f4 operation. The pipe lined implementation of f4 operation is shown in 
the figure:

!https://media.geeksforgeeks.org/wp-content/uploads/F4-1-300x286.png

The
 Inputs are given as scalar components in the pipeline. Apart from these
 basic types of instructions, some special instructions may be used to 
facilitate the manipulation of vector data.

*** Branch Prediction in Pentium
**Why do we need branch prediction?**

1. The gain produced by [Pipelining](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-1-execution-stages-and-throughput/) can be reduced by the presence of program transfer instructions eg JMP, CALL, RET etc
2. They change the sequence causing all the instructions that entered the pipeline after program transfer instructions invalid
3. Thus no work is done as the pipeline stages are reloaded.

**Branch prediction logic:**To
 avoid this problem, Pentium uses a scheme called Dynamic Branch 
Prediction. In this scheme, a prediction is made for the branch 
instruction currently in the pipeline. The prediction will either be 
taken or not taken. If the prediction is true then the pipeline will not
 be flushed and no clock cycles will be lost. If the prediction is false
 then the pipeline is flushed and starts over with the current 
instruction.

It is implemented using 4 way set associated cache with 256 entries. This is called **Branch Target Buffer (BTB)**. The directory entry for each line consists of:

- **Valid bit:** Indicates whether the entry is valid or not.
- **History bit:** Track how often bit has been taken.

Source
 memory address is from where the branch instruction was fetched. If the
 directory entry is valid then the target address of the branch is 
stored in corresponding data entry in BTB.

**Working of Branch Prediction:**

1. BTB is a lookaside cache that sits to the side of Decode Instruction(DI)
stage of 2 pipelines and monitors for branch instructions.
2. The first time that a branch instruction enters the pipeline, the BTB uses its source memory to perform a lookup in the cache.
3. Since the instruction was never seen before, it is BTB miss. It predicts that the branch will not be taken even though it is unconditional jump
instruction.
4. When the instruction reaches the EU(execution
unit), the branch will either be taken or not taken. If taken, the next
instruction to be executed will be fetched from the branch target
address. If not taken, there will be a sequential fetch of instructions.
5. When a branch is taken for the first time, the execution unit provides
feedback to the branch prediction. The branch target address is sent
back which is recorded in BTB.
6. A directory entry is made containing the source memory address and history bit is set as strongly taken.

!https://media.geeksforgeeks.org/wp-content/uploads/222-15.png

The diagram is explained by the following table:

History BitsResulting DescriptionPrediction madeIf branch takenIf branch not taken11Strongly TakenBranch TakenRemains in same stateDowngraded to weakly taken10Weakly TakenBranch TakenUpgraded to strongly takenDowngraded to weakly not taken01Weakly Not TakenBranch Not TakenUpgraded to weakly takenDowngraded to strongly not taken00Strongly Not TakenBranch Not TakenUpgraded to weakly not takenRemains in same state
*** Instruction Word Size in Microprocessor
The 
8085 instruction set is classified into 3 categories by considering the 
length of the instructions. In 8085, the length is measured in terms of 
“byte” rather then “word” because 8085 microprocessor has 8-bit data 
bus. Three types of instruction are: 1-byte instruction, 2-byte 
instruction, and 3-byte instruction.

**1. One-byte instructions –**In 1-byte instruction, the opcode and the operand of an instruction are represented in one byte.

- **Example-1:**Task- Copy the contents of accumulator in register B.
    
    ```
    Mnemonic- MOV B, A
    Opcode- MOV
    Operand- B, A
    Hex Code- 47H
    Binary code- 0100 0111
    ```
    
- **Example-2:**Task- Add the contents of accumulator to the contents of register B.
    
    ```
     Mnemonic- ADD B
    Opcode- ADD
    Operand- B
    Hex Code- 80H
    Binary code- 1000 0000
    ```
    
- **Example-3:**Task- Invert (complement) each bit in the accumulator.
    
    ```
    Mnemonic- CMA
    Opcode- CMA
    Operand-  NA
    Hex Code- 2FH
    Binary code- 0010 1111
    ```
    

**Note –** The 
length of these instructions is 8-bit; each requires one memory 
location. The mnemonic is always followed by a letter (or two letters) 
representing the registers (such as A, B, C, D, E, H, L and SP).

**2. Two-byte instructions –**Two-byte
 instruction is the type of instruction in which the first 8 bits 
indicates the opcode and the next 8 bits indicates the operand.

- **Example-1:**Task- Load the hexadecimal data 32H in the accumulator.
    
    ```
    Mnemonic- MVI A, 32H
    Opcode- MVI
    Operand- A, 32H
    Hex Code- 3E
    32
    Binary code- 0011 1110
    0011 0010
    ```
    
- **Example-2:**Task- Load the hexadecimal data F2H in the register B.
    
    ```
    Mnemonic- MVI B, F2H
    Opcode- MVI
    Operand- B, F2H
    Hex Code- 06
    F2
    Binary code- 0000 0110
    1111 0010
    ```
    

**Note –** This type of instructions need two bytes to store the binary codes. The mnemonic is always followed by 8-bit (byte) data.

**3. Three-byte instructions –**Three-byte
 instruction is the type of instruction in which the first 8 bits 
indicates the opcode and the next two bytes specify the 16-bit address. 
The low-order address is represented in second byte and the high-order 
address is represented in the third byte.

- **Example-1:**Task- Load contents of memory 2050H in the accumulator.
    
    ```
    Mnemonic- LDA 2050H
    Opcode- LDA
    Operand- 2050H
    Hex Code- 3A
    50
    20
    Binary code- 0011 1010
    0101 0000
    0010 0000
    ```
    
- **Example-2:**Task- Transfer the program sequence to the memory location 2050H.
    
    ```
    Mnemonic- JMP 2085H
    Opcode- JMP
    Operand- 2085H
    Hex Code- C3
    85
    20
    Binary code- 1100 0011
    1000 0101
    0010 0000
    ```
    

**Note –** These instructions 
would require three memory locations to store the binary codes. The 
mnemonic is always followed by 16-bit (or adr).
*** Computer Organization | Problem Solving on Instruction Format
Prerequisite – [Basic Computer Instructions](https://www.geeksforgeeks.org/computer-organization-basic-computer-instructions/), [Instruction Formats](https://www.geeksforgeeks.org/computer-organization-instruction-formats-zero-one-two-three-address-instruction/) An
 instruction format defines the different component of an instruction. 
The main components of an instruction are opcode (which instruction to 
be executed) and operands (data on which instruction to be executed). 
Here are the different terms related to instruction format:

- **Instruction set size –** It tells the total number of instructions defined in the processor.
- **Opcode size –** It is the number of bits occupied by the opcode which is calculated by taking log of instruction set size.
- **Operand size –** It is the number of bits occupied by the operand.
- **Instruction size –** It is calculated as sum of bits occupied by opcode and operands.

In
 this article, we will discuss different types of problems based on 
instruction format which are asked in GATE. For details about different 
types of instruction formats, you can refer: [Instruction Formats](https://www.geeksforgeeks.org/computer-organization-instruction-formats-zero-one-two-three-address-instruction/)

**Type 1:** Given instruction set size and operands size and their count, find the size of the instruction. In
 this type of questions, you will be given the size of instruction set, 
number of operands and their size, you have to find out the size of the 
instruction.

**Que-1.** Consider a processor with 64
 registers and an instruction set of size twelve. Each instruction has 
five distinct fields, namely, opcode, two source register identifiers, 
one destination register identifier, and a twelve-bit immediate value. 
Each instruction must be stored in memory in a byte-aligned fashion. If a
 program has 100 instructions, the amount of memory (in bytes) consumed 
by the program text is ____________. (GATE 2016) (A) 100 (B) 200 (C) 400 (D) 500

**Solution:** It can be approached as: 

- The instruction consists of opcode and operands. Given the instruction set
of size 12, 4 bits are required for opcode (2^4 = 16).
- As there are total 64 registers, 6 bits are required for identifying a register.
- As the instruction contains 3 registers (2 source + 1 designation), 3 * 6 = 18 bit are required for register identifiers.
- 12 bits are required for immediate value as given.
- Total bits for an instruction = 4 + 18 + 12 = 34 bits
- The instructions are required to be stored in a byte-aligned fashion. The
nearest byte boundary after 34 bits is at 40 bits (5 bytes).
- Hence, for 100 instructions, the memory required is 5 * 100 = 500 bytes, and the correct option is (D).

**Type 2:** Given instruction size, opcode size and size of some operands, find the size and maximum value of remaining operands. In
 this type of questions, you will be given the size of instruction, size
 of opcode, number of operands and size of some operands, you have to 
find out the size or maximum value of remaining operands.

**Que-2.**
 A processor has 40 distinct instructions and 24 general purpose 
registers. A 32-bit instruction word has an opcode, two registers 
operands and an immediate operand. The number of bits available for the 
immediate operand field is_______. (GATE CS 2016)

**Solution:** It can be approached as: 

- As the processor has 40 instructions, number of bits for opcode = 6 (2^6 = 64)
- As the processor has 24 register, number of bits for one register = 5 (2^5 = 32)
- Total bits occupied by 2 registers and opcode = 6 + 5 + 5 =16.
- As instruction size given is 32 bits, remaining bit left for operand = 32-16 = 16 bits.

**Que-3.**
 A machine has a 32-bit architecture, with 1-word long instructions. It 
has 64 registers, each of which is 32 bits long. It needs to support 45 
instructions, which have an immediate operand in addition to two 
register operands. Assuming that the immediate operand is an unsigned 
integer, the maximum value of the immediate operand is ___________. 
(GATE CS 2014)

**Solution:** It can be approached as: 

- As machine has 32-bit architecture, therefore, 1 word = 32 bits = instruction size
- As the processor has 64 register, number of bits for one register = 6 (2^6 = 64)
- As the processor has 45 instructions, number of bits for opcode = 6 (2^6 = 64)
- Total bits occupied by 2 registers and opcode = 6 + 6 + 6 =18.
- As instruction size given is 32 bits, remaining bit left for immediate operand = 32-18 = 14 bits.
- Maximum unsigned value using 14 bits = 2^14 – 1 = 16383 which is the answer.

**Type 3:** Instruction format with different categories of instruction In
 this type of questions, you will be given different categories of 
instructions. You have to find maximum possible instructions of a given 
type.

**Que-4.** A processor has 16 integer 
registers (R0, R1, … , R15) and 64 floating point registers (F0, F1, … ,
 F63). It uses a 2 byte instruction format. There are four categories of
 instructions: Type-1, Type-2, Type-3, and Type 4. Type-1 category 
consists of four instructions, each with 3 integer register operands 
(3Rs). Type-2 category consists of eight instructions, each with 2 
floating point register operands (2Fs). Type-3 category consists of 
fourteen instructions, each with one integer register operand and one 
floating point register operand (1R+1F). Type-4 category consists of N 
instructions, each with a floating point register operand (1F).

The maximum value of N is ________. (GATE-CS-2018)

**Solution:** It can be approached as: 

- As machine has 2 byte = 16 bits instruction format, therefore, possible encodings = 2^16.
- As the processor has 16 integer register, number of bits for one integer register = 4 (2^4 = 16)
- As the processor has 64 floating point register, number of bits for one floating point register = 6 (2^6 = 64).
- For type-1 category having 4 instructions each having 3 integer register
operands (4*3 = 12 bits) will consume 4 * 2^12 = 2^14 encodings.
- For type-2 category having 8 instructions each having 2 floating point
register operands (2*6 = 12 bits) will consume 8 * 2^12 = 2^15
encodings.
- For type-3 category having 14 instructions each having 1 integer register and 1 floating point register operands (4 + 6 = 10 bits) will consume 14 * 2^10 = 14336 encodings.
- For type-4 category instructions, number of encodings left = 2^16 – 2^14 – 2^15 – 14336 = 2048.
- For type-4 category having N instructions each having 1 floating point
register operand (6 bits) will consume N* 2^6 = 2048 (calculated from
previous step). Therefore, N = 32.
** Computer Arithmetic
*** Introduction of ALU and Data Path
Representing
 and storing numbers were the basic operation of the computers of 
earlier times. The real go came when computation, manipulating numbers 
like adding, multiplying came into the picture. These operations are 
handled by the computer’s **arithmetic logic unit (ALU)**. 
The ALU is the mathematical brain of a computer. The first ALU was INTEL
 74181 implemented as a 7400 series is a TTL integrated circuit that was
 released in 1970.

The **ALU**
 is a digital circuit that provides arithmetic and logic operations. It 
is the fundamental building block of the central processing unit of a 
computer. A modern CPU has a very powerful ALU and it is complex in 
design. In addition to ALU modern CPU contains a control unit and a set 
of registers. Most of the operations are performed by one or more ALU’s,
 which load data from the input register. Registers are a small amount 
of storage available to the CPU. These registers can be accessed very 
fast. The control unit tells ALU what operation to perform on the 
available data. After calculation/manipulation, the ALU stores the 
output in an output register.

!https://media.geeksforgeeks.org/wp-content/uploads/888-1.png

The
 CPU can be divided into two sections: the data section and the control 
section. The DATA section is also known as the data path.**BUS:**
 In early computers “BUS” were parallel electrical wires with multiple 
hardware connections. Therefore a bus is a communication system that 
transfers data between components inside a computer, or between 
computers. It includes hardware components like wires, optical fibers, 
etc and software, including communication protocols. The Registers, ALU,
 and the interconnecting BUS are collectively referred to as data 
paths.

Types of the bus are: 

1. **Address bus:** The buses which are used to carry address. 
2. **Data bus:** The buses which are used to carry data. 
3. **Control bus:** If the bus is carrying control signals. 
4. **Power bus:** If it is carrying clock pulse, power signals it is known as a power bus, and so on. 

The
 bus can be dedicated, i.e., it can be used for a single purpose or it 
can be multiplexed, i.e., it can be used for multiple purposes. When we 
would have different kinds of buses, different types of bus 
organizations will take place. 

- **Program Counter –** A program counter (PC) is a CPU register in the computer processor which
has the address of the next instruction to be executed from memory. As
each instruction gets fetched, the program counter increases its stored
value by 1. It is a digital counter needed for faster execution of tasks as well as for tracking the current execution point.
- **Instruction Register –** In computing, an instruction register (IR) is the part of a CPU’s control
unit that holds the instruction currently being executed or decoded. An
instruction register is the part of a CPU’s control unit that holds the
instruction currently being executed or decoded. The instruction
register specifically holds the instruction and provides it to the
instruction decoder circuit.
- **Memory Address Register –** The Memory Address Register (MAR) is the CPU register that either stores
the memory address from which data will be fetched from the CPU, or the
address to which data will be sent and stored. It is a temporary storage component in the CPU(central processing unit) that temporarily stores
the address (location) of the data sent by the memory unit until the
instruction for the particular data is executed.
- **Memory Data Register –** The memory data register (MDR) is the register in a computer’s processor,
or central processing unit, CPU, that stores the data being transferred
to and from the immediate access storage. Memory data register (MDR) is
also known as memory buffer register (MBR).
- **General Purpose Register –** General-purpose registers are used to store temporary data within the microprocessor.
It is a multipurpose register. They can be used either by a programmer
or by a user.

**One Bus organization –**

!https://media.geeksforgeeks.org/wp-content/uploads/12112.png

In
 one bus organization, a single bus is used for multiple purposes. A set
 of general-purpose registers, program counters, instruction registers, 
memory address registers (MAR), memory data registers (MDR) are 
connected with the single bus. Memory read/write can be done with MAR 
and MDR. The program counterpoints to the memory location from where the
 next instruction is to be fetched. Instruction register is that very 
register will hold the copy of the current instruction. In the case of 
one bus organization, at a time only one operand can be read from the 
bus.

As a result, if the requirement is to read two operands for 
the operation then the read operation needs to be carried twice. So 
that’s why it is making the process a little longer. One of the 
advantages of one bus organization is that it is one of the simplest and
 also this is very cheap to implement. At the same time a disadvantage 
lies that it has only one bus and this “one bus” is accessed by all 
general-purpose registers, program counter, instruction register, MAR, 
MDR making each and every operation sequential. No one recommends this 
architecture nowadays. **Two Bus organizations –** To 
overcome the disadvantage of one bus organization another architecture 
was developed known as two bus organization. In two bus organizations, 
there are two buses. The general-purpose register can read/write from 
both the buses. In this case, two operands can be fetched at the same 
time because of the two buses. One bus fetch operand for ALU and another
 bus fetch for register. The situation arises when both buses are busy 
fetching operands, the output can be stored in a temporary register and 
when the buses are free, the particular output can be dumped on the 
buses.

There are two versions of two bus organizations, i.e., 
in-bus and out-bus. From in-bus, the general-purpose register can read 
data and to the out bus, the general-purpose registers can write data. 
Here buses get dedicated.

!https://media.geeksforgeeks.org/wp-content/uploads/33333.png

!https://media.geeksforgeeks.org/wp-content/uploads/6666-2.png

**Three Bus organization –** In
 three bus organizations we have three buses, OUT bus1, OUT bus2, and an
 IN bus. From the out buses, we can get the operand which can come from 
the general-purpose register and evaluated in ALU and the output is 
dropped on In Bus so it can be sent to respective registers. This 
implementation is a bit complex but faster in nature because in parallel
 two operands can flow into ALU and out of ALU. It was developed to 
overcome the “busy waiting” problem of two bus organizations. In this 
structure after execution, the output can be dropped on the bus without 
waiting because of the presence of an extra bus. The structure is given 
below in the figure.

!https://media.geeksforgeeks.org/wp-content/uploads/7777.png

The main **advantages** of multiple bus organizations over the single bus are as given below. 

1. Increase in size of the registers. 
2. Reduction in the number of cycles for execution. 
3. Increases the speed of execution or we can say faster execution.
*** Computer Arithmetic
**Negative Number Representation**

- **Sign Magnitude**

Sign
 magnitude is a very simple representation of negative numbers. In sign 
magnitude the first bit is dedicated to represent the sign and hence it 
is called sign bit.

Sign bit ‘1’ represents negative sign.

Sign bit ‘0’ represents positive sign.

In
 sign magnitude representation of a n – bit number, the first bit will 
represent sign and rest n-1 bits represent magnitude of number.

For example,

- +25 = 011001

Where 11001 = 25

And 0 for ‘+’

- 25 = 111001

Where 11001 = 25

And 1 for ‘-‘.

**Range of number represented by sign magnitude method** = -(2n-1-1) to +(2n-1-1) (for n bit number)

But there is one problem in sign magnitude and that is we have two representations of 0

+0 = 000000

– 0 = 100000

- **2’s complement method**

To
 represent a negative number in this form, first we need to take the 1’s
 complement of the number represented in simple positive binary form and
 then add 1 to it.

For example:

(-8)10 = (1000)2

1’s complement of 1000 = 0111

Adding 1 to it, 0111 + 1 = 1000

So, (-8)10 = (1000)2

Please don’t get confused with (8)10 =1000 and (-8)10=1000 as with 4 bits, we can’t represent a positive number more than 7. So, 1000 is representing -8 only.

**Range of number represented by 2’s complement =** (-2n-1 to 2n-1 – 1)

**Floating point representation of numbers**

- **32-bit representation floating point numbers IEEE standard**

**Normalization**

!https://media.geeksforgeeks.org/wp-content/uploads/32-bit-representation-floating-point-numbers-IEEE-standard.jpg

- Floating point numbers are usually normalized
- Exponent is adjusted so that leading bit (MSB) of mantissa is 1
- Since it is always 1 there is no need to store it
- Scientific notation where numbers are normalized to give a single digit before the decimal point like in decimal system e.g. 3.123 x 10
    
    3
    

For example, we represent 3.625 in 32 bit format.

Changing 3 in binary=11

Changing .625 in binary

```
.625 X 2       1.25 X 2         0.5 X 2           1
```

Writing in binary exponent form

3.625=11.101 X 20

On normalizing

11.101 X 20=1.1101 X 21

On biasing exponent = 127 + 1 = 128

(128)10=(10000000) 2

**For getting significand**

Digits after decimal = 1101

Expanding to 23 bit = 11010000000000000000000

**Setting sign bit**

As it is a positive number, sign bit = 0

Finally we arrange according to representation

```
Sign bit      exponent      significand

0            10000000      11010000000000000000000
```

- **64-bit representation floating point numbers IEEE standard**

Again we follow the same procedure upto normalization. After that, we add 1023 to bias the exponent.

!https://media.geeksforgeeks.org/wp-content/uploads/64-bit-representation-floating-point-numbers-IEEE-standard.jpg

For example, we represent -3.625 in 64 bit format.

Changing 3 in binary = 11

Changing .625 in binary

```
.625 X 2     1

.25 X 2       0

.5 X 2         1
```

Writing in binary exponent form

3.625 = 11.101 X 20

On normalizing

11.101 X 20 = 1.1101 X 21

On biasing exponent 1023 + 1 = 1024

(1024)10 = (10000000000)2

So 11 bit exponent = 10000000000

52 bit significand = 110100000000 …………. making total 52 bits

Setting sign bit = 1 (number is negative)

So, final representation

1 10000000000 110100000000 …………. making total 52 bits by adding further 0’s

**Converting floating point into decimal**

Let’s convert a FP number into decimal

1 01111100 11000000000000000000000

The decimal value of an IEEE number is given by the formula:

**(1 -2s) * (1 + f) * 2( e – bias )**

where

- s, f and e fields are taken as decimal here.
- (1 -2s) is 1 or -1, depending upon sign bit 0 and 1
- add an implicit 1 to the significand (fraction field f), as in formula

Again, the bias is either 127 or 1023, for single or double precision respectively.

First convert each individual field to decimal.

- The sign bit s is 1
- The e field contains 01111100 = (124)10
- The mantissa is 0.11000 … = (0.75)10

Putting these values in formula

(1 – 2) * (1 + 0.75) * 2124 – 127 = ( – 1.75 * 2-3 ) = – 0.21875

This article has been contributed by Anuj Batham.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
**** Computer Arithmetic | Set – 2

**FLOATING POINT ADDITION AND SUBTRACTION** 

- **FLOATING POINT ADDITION**

To understand floating point addition, first we see addition of real numbers in decimal as same logic is applied in both cases.

**For example,** we have to add **1.1 * 103** and **50.**

We cannot add these numbers directly. First, we need to align the exponent and then, we can add significant.

After aligning exponent, we get **50 = 0.05 * 103**

Now adding significant, **0.05 + 1.1 = 1.15**

So, finally we get **(1.1 * 103 + 50) = 1.15 * 103**

Here, notice that we shifted **50** and made it **0.05** to add these numbers.

**Now let us take example of floating point number addition**

We follow these steps to add two numbers:

1. Align the significant

2. Add the significant

3. Normalize the result

**Let the two numbers be**

x = 9.75 y = 0.5625

Converting them into 32-bit floating point representation,

**9.75**’s representation in 32-bit format = **0 10000010 00111000000000000000000**

**0.5625**’s representation in 32-bit format = **0 01111110 00100000000000000000000**

Now we get the difference of exponents to know how much shifting is required.

(**10000010 – 01111110**)2 = (**4**)10

Now, we shift the mantissa of lesser number right side by 4 units.

Mantissa of **0.5625 = 1.00100000000000000000000**

(note that 1 before decimal point is understood in 32-bit representation)

Shifting right by **4** units, we get **0.00010010000000000000000**

Mantissa of **9.75** = **1. 00111000000000000000000**

Adding mantissa of both

**0. 00010010000000000000000**

**+ 1. 00111000000000000000000**

————————————————-

**1. 01001010000000000000000**

In final answer, we take exponent of bigger number

So, final answer consist of :

Sign bit = **0**

Exponent of bigger number = **10000010**

Mantissa = **01001010000000000000000**

32 bit representation of answer = **x + y** = **0 10000010 01001010000000000000000**

- **FLOATING POINT SUBTRACTION**

Subtraction
 is similar to addition with some differences like we subtract mantissa 
unlike addition and in sign bit we put the sign of greater number.

**Let the two numbers be**

x = 9.75 y = – 0.5625

Converting them into 32-bit floating point representation

**9.75**’s representation in 32-bit format = **0 10000010 00111000000000000000000**

**– 0.5625**’s representation in 32-bit format = **1 01111110 00100000000000000000000**

Now, we find the difference of exponents to know how much shifting is required.

(**10000010 – 01111110**)2 = (**4**)10 Now, we shift the mantissa of lesser number right side by 4 units.

Mantissa of **–** **0.5625 = 1.00100000000000000000000**

(note that 1 before decimal point is understood in 32-bit representation)

Shifting right by **4** units, **0.00010010000000000000000**

Mantissa of **9.75**= **1. 00111000000000000000000**

Subtracting mantissa of both

**0. 00010010000000000000000**

**– 1. 00111000000000000000000**

————————————————

**1. 00100110000000000000000**

Sign bit of bigger number = **0**

So, finally the answer = **x – y = 0 10000010 00100110000000000000000**

!https://media.geeksforgeeks.org/wp-content/uploads/floating-point-arithmetic.jpg

This article has been contributed by Anuj Batham.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Difference between 1’s Complement representation and 2’s Complement representation Technique
Prerequisite – [Representation of Negative Binary Numbers](https://www.geeksforgeeks.org/representation-of-negative-binary-numbers/)

**1’s complement**
 of a binary number is another binary number obtained by toggling all 
bits in it, i.e., transforming the 0 bit to 1 and the 1 bit to 0. 
Examples:

```
Let numbers be stored using 4 bits

1's complement of 7 (0111) is 8 (1000)
1's complement of 12 (1100) is 3 (0011)
```

**2’s complement** of a binary number is 1 added to the 1’s complement of the binary number. Examples:

```
Let numbers be stored using 4 bits

2's complement of 7 (0111) is 9 (1001)
2's complement of 12 (1100) is 4 (0100)
```

These representations are used for signed numbers.

The **main difference**
 between 1′ s complement and 2′ s complement is that 1′ s complement has
 two representations of 0 (zero) — 00000000, which is positive zero 
(+0), and 11111111, which is negative zero (-0); whereas in 2′ s 
complement, there is only one representation for zero — 00000000 (0) 
because if we add 1 to 11111111 (-1), we get 100000000, which is nine 
bits long. Since only eight bits are allowed, the left-most bit is 
discarded(or overflowed), leaving 00000000 (-0) which is the same as 
positive zero. This is the reason why 2′ s complement is generally used.

Another
 difference is that while adding numbers using 1′ s complement, we first
 do binary addition, then add in an end-around carry value. But, 2′ s 
complement has only one value for zero and doesn’t require carry values.

Range of 1’s complement for n bit number is from -2n-1-1 to 2n-1-1 whereas the range of 2’s complement for n bit is from -2n-1 to 2n-1-1.

There are 2n-1 valid numbers in 1’s complement and 2n valid numbers in 2’s complement.

Please write comments if you find anything incorrect, or if you want to share more information about the topic discussed above
*** Restoring Division Algorithm For Unsigned Integer
A division algorithm provides a quotient and a remainder when we divide two number. They are generally of two type **slow algorithm and fast algorithm**.
 Slow division algorithm are restoring, non-restoring, non-performing 
restoring, SRT algorithm and under fast comes Newton–Raphson and 
Goldschmidt.

In this 
article, will be performing restoring algorithm for unsigned integer. 
Restoring term is due to fact that value of register A is restored after
 each iteration.

!https://media.geeksforgeeks.org/wp-content/uploads/restoring-hardware.png

Here,
 register Q contain quotient and register A contain remainder. Here, 
n-bit dividend is loaded in Q and divisor is loaded in M. Value of 
Register is initially kept 0 and this is the register whose value is 
restored during iteration due to which it is named Restoring.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing.png

Let’s pick the step involved:

- **Step-1:** First the registers are initialized with corresponding values (Q =
Dividend, M = Divisor, A = 0, n = number of bits in dividend)
- **Step-2:** Then the content of register A and Q is shifted left as if they are a single unit
- **Step-3:** Then content of register M is subtracted from A and result is stored in A
- **Step-4:** Then the most significant bit of the A is checked if it is 0 the least
significant bit of Q is set to 1 otherwise if it is 1 the least
significant bit of Q is set to 0 and value of register A is restored i.e the value of A before the subtraction with M
- **Step-5:** The value of counter n is decremented
- **Step-6:** If the value of n becomes zero we get of the loop otherwise we repeat from step 2
- **Step-7:** Finally, the register Q contain the quotient and A contain remainder

**Examples:**

```
Perform Division Restoring Algorithm
Dividend = 11
Divisor  = 3
```

[Untitled Database](https://www.notion.so/5fa3cf35e3b94346b89e5accc43a8f63?pvs=21)

Remember
 to restore the value of A most significant bit of A is 1. As that 
register Q contain the quotient, i.e. 3 and register A contain remainder
 2.
 
*** Non-Restoring Division For Unsigned Integer
In earlier post [Restoring Division](https://www.geeksforgeeks.org/restoring-division-algorithm-unsigned-integer/)
 learned about restoring division. Now, here perform Non-Restoring 
division, it is less complex than the restoring one because simpler 
operation are involved i.e. addition and subtraction, also now restoring
 step is performed. In the method, rely on the sign bit of the register 
which initially contain zero named as A.

Here is the flow chart given below.

!https://media.geeksforgeeks.org/wp-content/uploads/non-restoring3.jpg

Let’s pick the step involved:

- **Step-1:** First the registers are initialized with corresponding values (Q =
Dividend, M = Divisor, A = 0, n = number of bits in dividend)
- **Step-2:** Check the sign bit of register A
- **Step-3:** If it is 1 shift left content of AQ and perform A = A+M, otherwise
shift left AQ and perform A = A-M (means add 2’s complement of M to A
and store it to A)
- **Step-4:** Again the sign bit of register A
- **Step-5:** If sign bit is 1 Q[0] become 0 otherwise Q[0] become 1 (Q[0] means least significant bit of register Q)
- **Step-6:** Decrements value of N by 1
- **Step-7:** If N is not equal to zero go to **Step 2** otherwise go to next step
- **Step-8:** If sign bit of A is 1 then perform A = A+M
- **Step-9:** Register Q contain quotient and A contain remainder

**Examples:** Perform Non_Restoring Division for Unsigned Integer

```
Dividend =11
Divisor  =3
-M =11101

```

[Untitled Database](https://www.notion.so/907cb0aee51542d2b1e4221dd77ddd02?pvs=21)

```
Quotient  = 3 (Q)
Remainder = 2 (A)

```

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Computer Organization | Booth’s Algorithm
Booth algorithm gives a procedure for **multiplying binary integers** in signed 2’s complement representation **in efficient way**,
 i.e., less number of additions/subtractions required. It operates on 
the fact that strings of 0’s in the multiplier require no addition but 
just shifting and a string of 1’s in the multiplier from bit weight 2^k 
to weight 2^m can be treated as 2^(k+1 ) to 2^m. As in all 
multiplication schemes, booth algorithm requires examination **of the multiplier bits**
 and shifting of the partial product. Prior to the shifting, the 
multiplicand may be added to the partial product, subtracted from the 
partial product, or left unchanged according to following rules:

1. The multiplicand is subtracted from the partial product upon encountering
the first least significant 1 in a string of 1’s in the multiplier
2. The multiplicand is added to the partial product upon encountering the
first 0 (provided that there was a previous ‘1’) in a string of 0’s in
the multiplier.
3. The partial product does not change when the multiplier bit is identical to the previous multiplier bit.

**Hardware Implementation of Booths Algorithm –** The hardware implementation of the booth algorithm requires the register configuration shown in the figure below.

**Booth’s Algorithm Flowchart –**

!https://media.geeksforgeeks.org/wp-content/uploads/20190405105227/booth2.png

We name the register as A, B and Q, AC, BR and QR respectively. Qn 
designates the least significant bit of multiplier in the register QR. 
An extra flip-flop Qn+1is appended to QR to facilitate a double 
inspection of the multiplier.The flowchart for the booth algorithm is 
shown below.

!https://media.geeksforgeeks.org/wp-content/uploads/20220611174802/Boot1-257x300.png

AC
 and the appended bit Qn+1 are initially cleared to 0 and the sequence 
SC is set to a number n equal to the number of bits in the multiplier. 
The two bits of the multiplier in Qn and Qn+1are inspected. If the two 
bits are equal to 10, it means that the first 1 in a string has been 
encountered. This requires subtraction of the multiplicand from the 
partial product in AC. If the 2 bits are equal to 01, it means that the 
first 0 in a string of 0’s has been encountered. This requires the 
addition of the multiplicand to the partial product in AC. When the two 
bits are equal, the partial product does not change. An overflow cannot 
occur because the addition and subtraction of the multiplicand follow 
each other. As a consequence, the 2 numbers that are added always have a
 opposite signs, a condition that excludes an overflow. The next step is
 to shift right the partial product and the multiplier (including Qn+1).
 This is an arithmetic shift right (ashr) operation which AC and QR ti 
the right and leaves the sign bit in AC unchanged. The sequence counter 
is decremented and the computational loop is repeated n times. **Example –** A numerical example of booth’s algorithm is shown below for n = 4. It shows the step by step multiplication of -5 and -7.

```
BR = -5 = 1011, BR' = 0100, BR'+1 = 0101
QR = -7 = 1001
The explanation of first step is as follows: Qn+1
AC = 0000, QR = 1001, Qn+1 = 0,  SC = 4
Qn Qn+1 = 10
So, we do AC + (BR)'+1, which gives AC = 0101
On right shifting AC and QR, we get
AC = 0010, QR = 1100 and Qn+1 = 1
```

[Untitled Database](https://www.notion.so/8e8db89b708d4319ab74bcebd68d680d?pvs=21)

Product is calculated as follows:

```
Product = AC QR
Product = 0010 0011 =  35
```

**Best Case and Worst Case Occurrence:** Best
 case is when there is a large block of consecutive 1’s and 0’s in the 
multipliers, so that there is minimum number of logical operations 
taking place, as in addition and subtraction. Worst case is when there 
are pairs of alternate 0’s and 1’s, either 01 or 10 in the multipliers, 
so that maximum number of additions and subtractions are required. **GATE Practice Questions –**

1. [GATE IT 2008 | Question 40](https://www.geeksforgeeks.org/gate-gate-it-2008-question-40/)
2. [GATE IT 2006 | Question 38](https://www.geeksforgeeks.org/gate-gate-it-2006-question-38/)
3. [GATE IT 2005 | Question 8](https://www.geeksforgeeks.org/gate-gate-it-2005-question-8/)
4. [GATE CS 1996 | Question 23](https://www.geeksforgeeks.org/gate-gate-cs-1996-question-23/)
*** Overflow in Arithmetic Addition in Binary Number System
In 
computer architecture 2’s Complement Number System is widely used. The 
discussion of overflow here mainly will we with respect to 2’s 
Complimentary System.

N-bit 2’s Complement number System can represent Number from

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d7d5bd565e765386a40076663c41901c_l3.svg

to

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a73dc030767fd4335807aaf188ac641c_l3.svg

4 Bit can represent numbers from

**( -8 to 7 )**

5 Bit can represent numbers from

**( -16 to 15 )**

in 2’s Complimentary System.

Overflow
 Occurs with respect to addition when 2 N-bit 2’s Complement Numbers are
 added and the answer is too large to fit into that N-bit Group.

A
 computer has N-Bit Fixed registers. Addition of two N-Bit Number will 
result in a max N+1 Bit number. That Extra Bit is stored in carry Flag. 
But Carry does not always indicate overflow.

!https://media.geeksforgeeks.org/wp-content/uploads/111-5.png

Adding
 7 + 1 in 4-Bit must be equal to 8. But 8 cannot be represented with 4 
bit 2’s complement number as it is out of range. Two Positive numbers 
were added and the answer we got is negative (-8). Here Carry is also 0.
 It is normally left to the programmer to detect overflow and deal with 
this situation.

**Overflow Detection –** Overflow occurs when:

1. Two negative numbers are added and an answer comes positive or
2. Two positive numbers are added and an answer comes as negative.

So
 overflow can be detected by checking Most Significant Bit(MSB) of two 
operands and answer. But Instead of using 3-bit Comparator Overflow can 
also be detected using 2 Bit Comparator just by checking Carry-in(C-in) 
and Carry-Out(C-out) from MSB’s. Consider N-Bit Addition of 2’s 
Complement number.

!https://media.geeksforgeeks.org/wp-content/uploads/222-7.png

Overflow Occurs when C-in

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2a703b556a43faacce479cd8f77263a4_l3.svg

C-out. Above expression for overflow can be explained from below Analysis.

!https://media.geeksforgeeks.org/wp-content/uploads/333-3.png

In first Figure the MSB of two numbers are 0 which means they are positive. Here if

**C-in is 1**

we get answer’s MSB as 1 means answer is negative (Overflow) and

**C-out as 0.**

C-in

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2a703b556a43faacce479cd8f77263a4_l3.svg

C-out hence overflow. In second Figure the MSB of two numbers are 1 which means they are negative. Here if

**C-in is 0**

we get answer MSB as 0 means answer is positive(Overflow) and

**C-out as 1.**

C-in

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2a703b556a43faacce479cd8f77263a4_l3.svg

C-out hence overflow.

Readers can also try out other combinations of c-in c-out and MSB’s to check overflow.

So Carry-in and Carry-out at MSB’s are enough to detect Overflow.

!https://media.geeksforgeeks.org/wp-content/uploads/444-2.png

Above XOR Gate can be used to detect overflow.
*** How the negative numbers are stored in memory?
Prerequisite – [Base conversions](https://www.geeksforgeeks.org/number-system-and-base-conversions/), [1’s and 2’s complement of a binary number](https://www.geeksforgeeks.org/1s-2s-complement-binary-number/), [2’s complement of a binary string](https://www.geeksforgeeks.org/efficient-method-2s-complement-binary-string/)Suppose
 the following fragment of code, int a = -34; Now how will this be 
stored in memory. So here is the complete theory. Whenever a number with
 minus sign is encountered, the number (ignoring minus sign) is 
converted to its binary equivalent. Then the two’s complement of the 
number is calculated. That two’s complement is kept at place allocated 
in memory and the sign bit will be set to 1 because the binary being 
kept is of a negative number. Whenever it comes on accessing that value 
firstly the sign bit will be checked if the sign bit is 1 then the 
binary will be two’s complemented and converted to equivalent decimal 
number and will be represented with a minus sign.

Let us take an example:**Example –**int a = -2056;Binary of 2056 will be calculated which is:

00000000000000000000100000001000 (32 bit representation, according of storage of int in C)2’s complement of the above binary is:11111111111111111111011111111000.

So finally the above binary will be stored at memory allocated for variable a.When
 it comes on accessing the value of variable a, the above binary will be
 retrieved from the memory location, then its sign bit that is the left 
most bit will be checked as it is 1 so the binary number is of a 
negative number so it will be 2’s complemented and when it will be 2’s 
complemented will be get the binary of 2056 which is:00000000000000000000100000001000

The
 above binary number will be converted to its decimal equivalent which 
is 2056 and as the sign bit was 1 so the decimal number which is being 
gained from the binary number will be represented with a minus sign. In 
our case -2056.
*** Conventional Computing vs Quantum Computing
We’ve
 been using computers since early 19th century. We’re currently in the 
fourth generation of computers with the microprocessors after vacuum 
tubes, transistors and integrated circuits. They were all based on 
conventional computing which is based on the classical phenomenon of 
electrical circuits being in a single state at a given time, either on 
or off.The fifth generation of computers is currently under development of which [quantum computing](https://www.geeksforgeeks.org/introduction-quantum-computing/)
 or quantum computers being most popular. Quantum computers are totally 
different from conventional computers on how they work. Quantum 
computers are based on the phenomenon of Quantum Mechanics, the 
phenomenon where it is possible to be in more than one state at a time.

**Difference between conventional computing and quantum computing:**

[Untitled Database](https://www.notion.so/8ab8bb2e1ae4456d9a4182d4235ecec8?pvs=21)
** Microprogrammed Control
*** Computer Organization | Micro-Operation
In computer central processing units, **micro-operations**
 (also known as micro-ops) are the functional or atomic, operations of a
 processor. These are low level instructions used in some designs to 
implement complex machine instructions. They generally perform 
operations on data stored in one or more registers. They transfer data 
between registers or between external buses of the CPU, also performs 
arithmetic and logical operations on registers.In executing a 
program, operation of a computer consists of a sequence of instruction 
cycles, with one machine instruction per cycle. Each instruction cycle 
is made up of a number of smaller units – *Fetch, Indirect, Execute and Interrupt cycles.*
 Each of these cycles involves series of steps, each of which involves 
the processor registers. These steps are referred as micro-operations. 
the prefix micro refers to the fact that each of the step is very simple
 and accomplishes very little. Figure below depicts the concept being 
discussed here.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-03-23-01-48-18-1.png

**Summary:**
 Execution of a program consists of sequential execution of 
instructions. Each instruction is executed during an instruction cycle 
made up of shorter sub-cycles(example – fetch, indirect, execute, 
interrupt). The performance of each sub-cycle involves one or more 
shorter operations, that is, *micro-operations.*

In my next article I will give detailed information of each Instruction Cycle.
*** Microarchitecture and Instruction Set Architecture
In this article, we look at what an *Instruction Set Architecture (ISA)* is and what is the difference between an **‘ISA’** and *Microarchitecture*. An **ISA** is defined as the design of a computer from the *Programmer’s Perspective*.

This basically means that an **ISA** describes the **design of a Computer** in terms of the **basic operations**
 it must support. The ISA is not concerned with the 
implementation-specific details of a computer. It is only concerned with
 the set or collection of basic operations the computer must support. 
For example, the AMD Athlon and the Core 2 Duo processors have entirely 
different implementations but they support more or less the same set of 
basic operations as defined in the x86 Instruction Set.

Let us try to understand the Objectives of an ISA by taking the example of the **MIPS ISA**. MIPS is one of the most widely used ISAs in education due to its simplicity.

1. The ISA defines the **types of instructions** to be supported by the processor. Based on the type of operations they perform MIPS Instructions are classified into 3 types:
    - **Arithmetic/Logic Instructions:** These Instructions perform various Arithmetic & Logical operations on one or more operands.
    - **Data Transfer Instructions:** These instructions are responsible for the transfer of instructions from memory to the processor registers and vice versa.
    - **Branch and Jump Instructions:** These instructions are responsible for breaking the sequential flow of
    instructions and jumping to instructions at various other locations,
    this is necessary for the implementation of *functions* and *conditional statements*.
2. The ISA defines the **maximum length** of each type of instruction.
    
    Since the MIPS is a 32 bit ISA, each instruction must be accommodated within 32 bits. 
    
3. The ISA defines the **Instruction Format** of each type of instruction. The **Instruction Format** determines how the entire instruction is encoded within 32 bits There are 3 types of Instruction Formats in the MIPS ISA:
    - R-Instruction Format
    - I-Instruction Format
    - J-Instruction Format

If we look at the Abstraction Hierarchy:

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram1-1.png

**Figure –** The Abstraction Hierarchy

We note that the **Microarchitectural** level lies just below the **ISA** level and hence is concerned with the implementation of the basic operations to be supported by the Computer as defined by the **ISA**.
 Therefore we can say that the AMD Athlon and Core 2 Duo processors are 
based on the same ISA but have different microarchitectures with 
different performance and efficiencies.

Now one may ask the need to distinguish between **Microarchitecture** and **ISA**?

The
 answer to this lies in the need to standardize and maintain the 
compatibility of programs across different hardware implementations 
based on the same **ISA**. Making different machines 
compatible with the same set of basic instructions (The ISA) allows the 
same program to run smoothly on many different machines thereby making 
it easier for the programmers to document and maintain code for many 
different machines simultaneously and efficiently.

This 
Flexibility is the reason we first define an ISA and then design 
different microarchitectures complying with this ISA for implementing 
the machine. The design of a lower-level ISA is one of the major tasks 
in the study of Computer Architecture.

[Untitled Database](https://www.notion.so/e2f376395e1b422e84097f9fcf59dc00?pvs=21)

The *x86* was developed by Intel, but we see that almost every year Intel comes up with a new generation of i-series processors. The *x86*
 architecture on which most of the Intel Processors are based 
essentially remains the same across all these generations but, where 
they differ is in the underlying Microarchitecture. They differ in their
 implementation and hence are claimed to have improved Performance. 
These various Microarchitectures developed by Intel are codenamed as 
‘Nehalem’, ‘Sandybridge’, ‘Ivybridge’, and so on.

Therefore, in conclusion, we can say that different machines may be based on the same ISA but have different Microarchitectures.
*** Types of Program Control Instructions
**Program Control Instructions**
 are the machine code that are used by machine or in assembly language 
by user to command the processor act accordingly. These instructions are
 of various types. These are used in assembly language by user also. But
 in level language, user code is translated into machine code and thus 
instructions are passed to instruct the processor do the task.

**Types of Program Control Instructions:** There are different types of Program Control Instructions:

**1. Compare Instruction:** Compare
 instruction is specifically provided, which is similar to a subtract 
instruction except the result is not stored anywhere, but flags are set 
according to the result. 

```
Example:
CMP R1, R2 ;
```

**2. Unconditional Branch Instruction:** It causes an unconditional change of execution sequence to a new location. 

```
Example:
JUMP L2
Mov R3, R1 goto L2
```

**3. Conditional Branch Instruction:** A
 conditional branch instruction is used to examine the values stored in 
the condition code register to determine whether the specific condition 
exists and to branch if it does. 

```
Example:
Assembly Code : BE R1, R2, L1
Compiler allocates R1 for x and R2 for y
High Level Code: if (x==y) goto L1;
```

**4. Subroutines:** A
 subroutine is a program fragment that lives in user space, performs a 
well-defined task. It is invoked by another user program and returns 
control to the calling program when finished. 

```
Example:
CALL and RET
```

**5. Halting Instructions:** 

- **NOP Instruction –** NOP is no operation. It cause no change in the processor state other
than an advancement of the program counter. It can be used to
synchronize timing.
- **HALT –** It brings the
processor to an orderly halt, remaining in an idle state until restarted by interrupt, trace, reset or external action.

**6. Interrupt Instructions:** Interrupt
 is a mechanism by which an I/O or an instruction can suspend the normal
 execution of processor and get itself serviced. 

- **RESET –** It reset the processor. This may include any or all setting registers
to an initial value or setting program counter to standard starting
location.
- **TRAP –** It is non-maskable edge and level triggered interrupt. TRAP has the highest priority and vectored interrupt.
- **INTR –** It is level triggered and maskable interrupt. It has the lowest priority. It can be disabled by resetting the processor.
*** Difference between CALL and JUMP instructions
**CALL**
 instruction is used to call a subroutine. Subroutines are often used to
 perform tasks that need to be performed frequently. The **JMP** instruction is used to cause the PLC (Programmable Logic Control) to skip over rungs.

The differences Between **CALL** and **JUMP** instructions are:

[Untitled Database](https://www.notion.so/b312a024c6614d62a9e1242c37f86070?pvs=21)
** Computer Organization | Hardwired v
*** Computer Organization | Hardwired vs Micro-programmed Control Unit
Read
Discuss

Introduction :

In computer architecture, the control unit is responsible for directing the flow of data and instructions within the CPU. There are two main approaches to implementing a control unit: hardwired and micro-programmed.

A hardwired control unit is a control unit that uses a fixed set of logic gates and circuits to execute instructions. The control signals for each instruction are hardwired into the control unit, so the control unit has a dedicated circuit for each possible instruction. Hardwired control units are simple and fast, but they can be inflexible and difficult to modify.

On the other hand, a micro-programmed control unit is a control unit that uses a microcode to execute instructions. The microcode is a set of instructions that can be modified or updated, allowing for greater flexibility and ease of modification. The control signals for each instruction are generated by a microprogram that is stored in memory, rather than being hardwired into the control unit.

Micro-programmed control units are slower than hardwired control units because they require an extra step of decoding the microcode to generate control signals, but they are more flexible and easier to modify. They are commonly used in modern CPUs because they allow for easier implementation of complex instruction sets and better support for instruction set extensions.

To execute an instruction, the control unit of the CPU must generate the required control signal in the proper sequence. There are two approaches used for generating the control signals in proper sequence as Hardwired Control unit and the Micro-programmed control unit. 

Hardwired Control Unit: The control hardware can be viewed as a state machine that changes from one state to another in every clock cycle, depending on the contents of the instruction register, the condition codes, and the external inputs. The outputs of the state machine are the control signals. The sequence of the operation carried out by this machine is determined by the wiring of the logic elements and hence named “hardwired”. 

    Fixed logic circuits that correspond directly to the Boolean expressions are used to generate the control signals.
    Hardwired control is faster than micro-programmed control.
    A controller that uses this approach can operate at high speed.
    RISC architecture is based on the hardwired control unit

1

Micro-programmed Control Unit –  

    The control signals associated with operations are stored in special memory units inaccessible by the programmer as Control Words.
    Control signals are generated by a program that is similar to machine language programs.
    The micro-programmed control unit is slower in speed because of the time it takes to fetch microinstructions from the control memory.

Some Important Terms

    Control Word: A control word is a word whose individual bits represent various control signals.
    Micro-routine: A sequence of control words corresponding to the control sequence of a machine instruction constitutes the micro-routine for that instruction.
    Micro-instruction: Individual control words in this micro-routine are referred to as microinstructions.
    Micro-program: A sequence of micro-instructions is called a micro-program, which is stored in a ROM or RAM called a Control Memory (CM).
    Control Store: the micro-routines for all instructions in the instruction set of a computer are stored in a special memory called the Control Store.

2

The differences between hardwired and micro-programmed control units:
 	Hardwired Control Unit 	Micro-programmed Control Unit
 
Implementation 	Fixed set of logic gates and circuits 	Microcode stored in memory
 
Flexibility 	Less flexible, difficult to modify 	More flexible, easier to modify
 
Instruction Set 	Supports limited instruction sets 	Supports complex instruction sets
 
Complexity of Design 	Simple design, easy to implement 	Complex design, more difficult to implement
 
Speed 	Fast operation 	Slower operation due to microcode decoding
 
Debugging and Testing 	Difficult to debug and test 	Easier to debug and test
 
Size and Cost 	Smaller size, lower cost 	Larger size, higher cost
 
Maintenance and Upgradability 	Difficult to upgrade and maintain 	Easier to upgrade and maintain
 

Types of Micro-programmed Control Unit – Based on the type of Control Word stored in the Control Memory (CM), it is classified into two types : 

1. Horizontal Micro-programmed Control Unit : 
The control signals are represented in the decoded binary format that is 1 bit/CS. Example: If 53 Control signals are present in the processor then 53 bits are required. More than 1 control signal can be enabled at a time. 

    It supports longer control words.
    It is used in parallel processing applications.
    It allows a higher degree of parallelism. If degree is n, n CS is enabled at a time.
    It requires no additional hardware(decoders). It means it is faster than Vertical Microprogrammed.
    It is more flexible than vertical microprogrammed

2. Vertical Micro-programmed Control Unit : 
The control signals are represented in the encoded binary format. For N control signals- Log2(N) bits are required.  

    It supports shorter control words.
    It supports easy implementation of new control signals therefore it is more flexible.
    It allows a low degree of parallelism i.e., the degree of parallelism is either 0 or 1.
    Requires additional hardware (decoders) to generate control signals, it implies it is slower than horizontal microprogrammed.
    It is less flexible than horizontal but more flexible than that of a hardwired control unit.

    Note: Types of Control Unit in descending order of speed :

Hardwired control unit > Horizontal microprogrammed CU > Vertical microprogrammed CU 

GATE CS Corner Questions 
Practicing the following questions will help you test your knowledge. All questions have been asked in GATE in previous years or in GATE Mock Tests. It is highly recommended that you practice them.  
*** Difference between Hardwired and Micro-programmed Control Unit | Set 2
Prerequisite – [Hardwired v/s Micro-programmed Control Unit](https://www.geeksforgeeks.org/computer-organization-hardwired-vs-micro-programmed-control-unit/)To execute an instruction, there are two types of control units Hardwired Control unit and Micro-programmed control unit.

1. Hardwired control units are generally faster than microprogrammed designs. In
hardwired control, we saw how all the control signals required inside
the CPU can be generated using a state counter and a PLA circuit.
2. A microprogrammed control unit is a relatively simple logic circuit that
is capable of (1) sequencing through microinstructions and (2)
generating control signals to execute each microinstruction.

[Untitled Database](https://www.notion.so/be39c6275a67485681d4dd2c2fed0183?pvs=21)
*** Difference between Horizontal and Vertical micro-programmed Control Unit
Prerequisite – [Hardwired v/s Micro-programmed Control Unit](https://www.geeksforgeeks.org/computer-organization-hardwired-vs-micro-programmed-control-unit/)

The **control unit (CU)** is the engine that runs the entire functions of a computer with the help of control signals in the proper sequence. In the **micro-programmed**
 control unit approach, the control signals that are associated with the
 operations are stored in special memory units. It is convenient to 
think of sets of control signals that cause specific micro-operations to
 occur as being “micro-instructions”. The sequences of 
micro-instructions could be stored in an internal “*control*” memory.

The micro-programmed control unit can be classified into two types based on the type of Control Word stored in the [Control Memory](https://www.geeksforgeeks.org/introduction-of-control-unit-and-its-design/), viz., Horizontal micro-programmed control unit and Vertical micro-programmed control unit.

- In the *Horizontal micro-programmed* control unit, the control signals are represented in the decoded binary format, i.e., 1 bit/CS. Here ‘n’ control signals require n bit
encoding. On the other hand.
- In a *Vertical micro-programmed* control unit, the control signals are represented in the encoded binary format. Here ‘n’ control signals require logn bit encoding.
    
    2
    

### Difference between Horizontal and Vertical micro-programmed Control Unit:

[Untitled Database](https://www.notion.so/c1dd85a0a73548ed98fc25b07a81ac7e?pvs=21)

**Example:** Consider
 a hypothetical Control Unit that supports 4 k words. The Hardware 
contains 64 control signals and 16 Flags. What is the size of control 
word used in bits and control memory in a byte using: a) Horizontal Programming b) Vertical programming

**Solution:**

!https://media.geeksforgeeks.org/wp-content/uploads/20190328163139/hori-1.png

```
a)For Horizontal

64 bits for 64 signals % 16 bits for flags
Control Word Size = 64 + 16 = 80 bits

Control Memory = 4 kW = ( (4* 80) / 8 ) = 40 kByte

b)For Vertical
6 bits for 64 signals i.e log264
4 bits for 16 flags i.e log216
12 bits for 4K words i.e log2(4*1024)

Control Word Size = 4 + 6 + 12 = 22 bits

Control Memory = 4 kW = ( (4* 22) / 8 ) = 11 kByte
```

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Synchronous Data Transfer in Computer Organization
In **Synchronous data transfer**,
 the sending and receiving units are enabled with same clock signal. It 
is possible between two units when each of them knows the behavior of 
the other. The master performs a sequence of instructions for data 
transfer in a predefined order. All these actions are synchronized with 
the common clock. The master is designed to supply the data at a time 
when the slave is definitely ready for it. Usually, the master will 
introduce sufficient delay to take into account the slow response of the
 slave, without any request from the slave.

The
 master does not expect any acknowledgment signal from the slave when 
data is sent by the master to the slave. Similarly, when data from the 
slave is read by the master, neither the slave informs that the data has
 been placed on the data bus nor the master acknowledges that the data 
has been read. Both the master and slave perform their own task of 
transferring data at a designed clock period. Since both devices know 
the behavior (response time) of each other, no difficulty arises. Prior to transferring data, the master must logically select the slave either by sending slave’s address or sending **“device select”** signal to the slave. But there is no **acknowledgment signal** from the slave to the master if the device is selected.

Timing diagram of the synchronous read operation is given below:

!https://media.geeksforgeeks.org/wp-content/uploads/sdt.png

In
 this timing diagram, the master first places slave’s address in the 
address bus and read signal in the control line at the falling edge of 
the clock. The entire read operation is over in one clock period.

**Advantages –**

1. The design procedure is easy. The master does not wait for any acknowledges signal from the slave, though the master waits for a time equal to
slave’s response time.
2. The slave does not generate an
acknowledge signal, though it obeys the timing rules as per the protocol set by the master or system designer. 

**Disadvantages –**

1. If a slow speed unit connected to a common bus, it can degrade the overall rate of transfer in the system.
2. If the slave operates at a slow speed, the master will be idle for some time during data transfer and vice versa.
*** Implementation of Micro Instructions Sequencer
**Micro Instructions Sequencer**
 is a combination of all hardware for selecting the next 
micro-instruction address. The micro-instruction in control memory 
contains a set of bits to initiate micro operations in computer 
registers and other bits to specify the method by which the address is 
obtained.

**Implementation of Micro Instructions Sequencer –**

!https://media.geeksforgeeks.org/wp-content/uploads/20200421142426/seq.png

- **Control Address Register(CAR) :**Control address register receives the address from four different paths. For
receiving the addresses from four different paths, Multiplexer is used.
- **Multiplexer :**Multiplexer is a combinational circuit which contains many data inputs and single
data output depending on control or select inputs.
- **Branching :**Branching is achieved by specifying the branch address in one of the fields of
the micro instruction. Conditional branching is obtained by using part
of the micro-instruction to select a specific status bit in order to
determine its condition.
- **Mapping Logic :**An external address is transferred into control memory via a mapping logic circuit.
- **Incrementer :**Incrementer increments the content of the control address register by one, to select the next micro-instruction in sequence.
- **Subroutine Register (SBR) :**The return address for a subroutine is stored in a special register called
Subroutine Register whose value is then used when the micro-program
wishes to return from the subroutine.
- **Control Memory :**Control memory is a type of memory which contains addressable storage
registers. Data is temporarily stored in control memory. Control memory
can be accessed quicker than main memory.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Computer Organization | Performance of Computer
**Computer performance**
 is the amount of work accomplished by a computer system. The word 
performance in computer performance means “How well is the computer 
doing the work it is supposed to do?”. It basically depends on response 
time, throughput and execution time of a computer system.

**Response time** is the time from start to completion of a task. This also includes:

- Operating system overhead.
- Waiting for I/O and other processes
- Accessing disk and memory
- Time spent executing on the CPU or execution time.

**Throughput** is the total amount of work done in a given time.

**CPU execution time**
 is the total time a CPU spends computing on a given task. It also 
excludes time for I/O or running other programs. This is also referred 
to as simply CPU time.

Performance is determined by execution time as performance is inversely proportional to execution time.

```
Performance = (1 / Execution time)
```

And,

```
(Performance of A /  Performance of B)
= (Execution Time of B / Execution Time of A)
```

If given that 
Processor A is faster than processor B, that means execution time of A 
is less than that of execution time of B. Therefore, performance of A is
 greater than that of performance of B.

**Example –**Machine A runs a program in 100 seconds, Machine B runs the same program in 125 seconds

```
(Performance of A /  Performance of B)
= (Execution Time of B / Execution Time of A)
= 125 / 100 = 1.25
```

That means machine A is 1.25 times faster than Machine B.

And, the time to execute a given program can be computed as:

```
Execution time  = CPU clock cycles x clock cycle time
```

Since clock cycle time and clock rate are reciprocals, so,

```
Execution time  = CPU clock cycles / clock rate
```

The number of CPU clock cycles can be determined by,

```
CPU clock cycles
= (No. of instructions / Program ) x (Clock cycles / Instruction)
= Instruction Count x CPI
```

Which gives,

```
Execution time
= Instruction Count x CPI x clock cycle time
= Instruction Count x CPI / clock rate
```

The units for CPU Execution time are:

!https://media.geeksforgeeks.org/wp-content/uploads/unit.png

**How to Improve Performance?**To improve performance you can either:

- Decrease the CPI (clock cycles per instruction) by using new Hardware.
- Decrease the clock time or Increase clock rate by reducing propagation delays or by use pipelining.
- Decrease the number of required cycles or improve ISA or Compiler.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Introduction of Control Unit and its Design
**Control Unit**
 is the part of the computer’s central processing unit (CPU), which 
directs the operation of the processor. It was included as part of the [Von Neumann Architecture](https://www.geeksforgeeks.org/computer-organization-von-neumann-architecture/)
 by John von Neumann. It is the responsibility of the Control Unit to 
tell the computer’s memory, arithmetic/logic unit and input and output 
devices how to respond to the instructions that have been sent to the 
processor. It fetches internal instructions of the programs from the 
main memory to the processor instruction register, and based on this 
register contents, the control unit generates a control signal that 
supervises the execution of these instructions.

A
 control unit works by receiving input information to which it converts 
into control signals, which are then sent to the central processor. The 
computer’s processor then tells the attached hardware what operations to
 perform. The functions that a control unit performs are dependent on 
the type of CPU because the architecture of CPU varies from manufacturer
 to manufacturer. Examples of devices that require a CU are:

- Control Processing Units(CPUs)
- Graphics Processing Units(GPUs)

!https://media.geeksforgeeks.org/wp-content/uploads/controlunit.png

**Functions of the Control Unit –**

1. It coordinates the sequence of data movements into, out of, and between a processor’s many sub-units.
2. It interprets instructions.
3. It controls data flow inside the processor.
4. It receives external instructions or commands to which it converts to sequence of control signals.
5. It controls many execution units(i.e. ALU, data buffers and registers) contained within a CPU.
6. It also handles multiple tasks, such as fetching, decoding, execution handling and storing results.

**Types of Control Unit –**There are two types of control units: Hardwired control unit and Microprogrammable control unit.

1. **Hardwired Control Unit –**In the Hardwired control unit, the control signals that are important for
instruction execution control are generated by specially designed
hardware logical circuits, in which we can not modify the signal
generation method without physical change of the circuit structure. The
operation code of an instruction contains the basic data for control
signal generation. In the instruction decoder, the operation code is
decoded. The instruction decoder constitutes a set of many decoders that decode different fields of the instruction opcode.
    
    As a result, few 
    output lines going out from the instruction decoder obtains active 
    signal values. These output lines are connected to the inputs of the 
    matrix that generates control signals for execution units of the 
    computer. This matrix implements logical combinations of the decoded 
    signals from the instruction opcode with the outputs from the matrix 
    that generates signals representing consecutive control unit states and 
    with signals coming from the outside of the processor, e.g. interrupt 
    signals. The matrices are built in a similar way as a programmable logic
     arrays.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/hard.png
    
    Control
     signals for an instruction execution have to be generated not in a 
    single time point but during the entire time interval that corresponds 
    to the instruction execution cycle. Following the structure of this 
    cycle, the suitable sequence of internal states is organized in the 
    control unit.
    
    A number of signals generated by the control signal 
    generator matrix are sent back to inputs of the next control state 
    generator matrix. This matrix combines these signals with the timing 
    signals, which are generated by the timing unit based on the rectangular
     patterns usually supplied by the quartz generator. When a new 
    instruction arrives at the control unit, the control units is in the 
    initial state of new instruction fetching. Instruction decoding allows 
    the control unit enters the first state relating execution of the new 
    instruction, which lasts as long as the timing signals and other input 
    signals as flags and state information of the computer remain unaltered.
     A change of any of the earlier mentioned signals stimulates the change 
    of the control unit state.
    
    This causes that a new respective input
     is generated for the control signal generator matrix. When an external 
    signal appears, (e.g. an interrupt) the control unit takes entry into a 
    next control state that is the state concerned with the reaction to this
     external signal (e.g. interrupt processing). The values of flags and 
    state variables of the computer are used to select suitable states for 
    the instruction execution cycle.
    
    The last states in the cycle are 
    control states that commence fetching the next instruction of the 
    program: sending the program counter content to the main memory address 
    buffer register and next, reading the instruction word to the 
    instruction register of computer. When the ongoing instruction is the 
    stop instruction that ends program execution, the control unit enters an
     operating system state, in which it waits for a next user directive.
    
2. **Microprogrammable control unit –**The fundamental difference between these unit structures and the structure
of the hardwired control unit is the existence of the control store that is used for storing words containing encoded control signals mandatory
for instruction execution.
    
    In microprogrammed control units, 
    subsequent instruction words are fetched into the instruction register 
    in a normal way. However, the operation code of each instruction is not 
    directly decoded to enable immediate control signal generation but it 
    comprises the initial address of a microprogram contained in the control
     store.
    
    - **With a single-level control store:**In this, the instruction opcode from the instruction register is sent to
    the control store address register. Based on this address, the first
    microinstruction of a microprogram that interprets execution of this
    instruction is read to the microinstruction register. This
    microinstruction contains in its operation part encoded control signals, normally as few bit fields. In a set microinstruction field decoders,
    the fields are decoded. The microinstruction also contains the address
    of the next microinstruction of the given instruction microprogram and a control field used to control activities of the microinstruction
    address generator.
        
        !https://media.geeksforgeeks.org/wp-content/uploads/single.png
        
        The
         last mentioned field decides the addressing mode (addressing operation)
         to be applied to the address embedded in the ongoing microinstruction. 
        In microinstructions along with conditional addressing mode, this 
        address is refined by using the processor condition flags that represent
         the status of computations in the current program. The last 
        microinstruction in the instruction of the given microprogram is the 
        microinstruction that fetches the next instruction from the main memory 
        to the instruction register.
        
    - **With a two-level control store:**In this, in a control unit with a two-level control store, besides the
    control memory for microinstructions, a nano-instruction memory is
    included. In such a control unit, microinstructions do not contain
    encoded control signals. The operation part of microinstructions
    contains the address of the word in the nano-instruction memory, which
    contains encoded control signals. The nano-instruction memory contains
    all combinations of control signals that appear in microprograms that
    interpret the complete instruction set of a given computer, written once in the form of nano-instructions.
        
        !https://media.geeksforgeeks.org/wp-content/uploads/two-2.png
        
        In
         this way, unnecessary storing of the same operation parts of 
        microinstructions is avoided. In this case, microinstruction word can be
         much shorter than with the single level control store. It gives a much 
        smaller size in bits of the microinstruction memory and, as a result, a 
        much smaller size of the entire control memory. The microinstruction 
        memory contains the control for selection of consecutive 
        microinstructions, while those control signals are generated at the 
        basis of nano-instructions. In nano-instructions, control signals are 
        frequently encoded using 1 bit/ 1 signal method that eliminates 
        decoding.
        
*** Computer Organization | Hardwired vs Micro-programmed Control Unit
*** Difference between Hardwired and Micro-programmed Control Unit | Set 2
*** Difference between Horizontal and Vertical micro-programmed Control Unit
*** Synchronous Data Transfer in Computer Organization
*** Subprogram and its Characteristics
A **Subprogram** is a program inside any larger program that can be reused any number of times.

**Characteristics of a Subprogram:**

(1) A Subprogram is implemented using the **Call & Return** instructions in Assembly Language.

(2) The Call Instruction is present in the Main Program and the Return(Ret) Instruction is present in the subprogram itself.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-5-2.jpg

(3)
 It is important to note that the Main Program is suspended during the 
execution of any subprogram. Moreover, after the completion of the 
subprogram, the main program executes from the next sequential address 
present in the Program Counter.

(4) For the implementation of any subprogram, a **“Stack”** is used to store the **“Return Address”**
 to the Main Program . Here, Return Address means the immediately next 
instruction address after the Call Instruction in the Main program. This
 Return Address is present inside the Program Counter. Thus during the 
execution of the Call Instruction, the Program Counter value is first 
pushed to the Stack as the Return Address and then the Program Counter 
value is updated to the given address in the Call Instruction. 
Similarly, during the execution of Return(Ret) Instruction, the value 
present in the stack is popped and the Program Counter value is restored
 for further execution of the Main Program.

(5) The Main advantage of Subprogram is that it avoids repetition of Code and allows us to reuse the same code again and again.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-8-1.jpg

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
** Memory Organization
*** Introduction to memory and memory units
Memory
 devices are digital system that store data either temporarily or for a 
long term. Digital computers to hard disk have built in memory devices 
that can store data of user or manufacturers. The data either be in the 
form of control programs or programs that boot the system. Hence, to 
store such huge amount of data the memory devices must have enormous 
capacity. The challenge is to build memory devices that have large 
capacity but cost effective. The memory devices must be capable of 
storing both permanent data and instantaneous data.

Memories
 are made up of registers. Each register in the memory is one storage 
location. The storage location is also called a memory location. Memory 
locations are identified using **Address**. The total number of bits a memory can store is its **capacity**.

A storage element is called a **Cell**.
 Each register is made up of a storage element in which one bit of data 
is stored. The data in a memory are stored and retrieved by the process 
called **writing** and **reading** respectively.

!https://media.geeksforgeeks.org/wp-content/uploads/Read-and-write.png

A **word** is a group of bits where a memory unit stores binary information. A word with a group of 8 bits is called a **byte**. A
 memory unit consists of data lines, address selection lines, and 
control lines that specify the direction of transfer. The block diagram 
of a memory unit is shown below: 

!https://media.geeksforgeeks.org/wp-content/uploads/memory-unit.png

Data
 lines provide the information to be stored in memory. The control 
inputs specify the direct transfer. The k-address lines specify the word
 chosen.

When there are k address lines, 2k memory words can be accessed.

### Following are some important memory units :

**Bit (Binary Units):** bit is a logical representation of the electric state. It can be 1 or 0.

**Nibble:** it means the group of 4 bits.

**Byte:** a ****byte is a group of 8 bits.

**Word:** it
 is a fixed number of bits, it is different from computer to computer, 
but the same for each device. Compute store information in the form of 
words.****

### Following are conversations of units:

**Kilobyte (kb):** 1kb = 1024 byte

**Megabyte (mb):** 1mb = 1024 kb

**Gigabyte (gb):** 1gb = 1024 mb

**Terabyte (tb):** 1tb = 1024 gb

**Petabyte (pb):** 1pb = 1024 tb

Refer for [RAM and ROM](https://www.geeksforgeeks.org/types-computer-memory-ram-rom/), [different types of RAM](https://www.geeksforgeeks.org/different-types-ram-random-access-memory/), [cache memory](https://www.geeksforgeeks.org/cache-memory/), and [secondary memory](https://www.geeksforgeeks.org/secondary-memory/)
*** Memory Hierarchy Design and its Characteristics
In 
the Computer System Design, Memory Hierarchy is an enhancement to 
organize the memory such that it can minimize the access time. The 
Memory Hierarchy was developed based on a program behavior known as 
locality of references.The figure below clearly demonstrates the 
different levels of memory hierarchy :

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-4-4.png

This Memory Hierarchy Design is divided into 2 main types:

1. **External Memory or Secondary Memory –**Comprising of Magnetic Disk, Optical Disk, Magnetic Tape i.e. peripheral storage
devices which are accessible by the processor via I/O Module.
2. **Internal Memory or Primary Memory –**Comprising of Main Memory, Cache Memory & CPU registers. This is directly accessible by the processor.

We can infer the following characteristics of Memory Hierarchy Design from above figure:

1. **Capacity:**It is the global volume of information the memory can store. As we move
from top to bottom in the Hierarchy, the capacity increases.
2. **Access Time:**It is the time interval between the read/write request and the
availability of the data. As we move from top to bottom in the
Hierarchy, the access time increases.
3. **Performance:**Earlier when the computer system was designed without Memory Hierarchy design,
the speed gap increases between the CPU registers and Main Memory due to large difference in access time. This results in lower performance of
the system and thus, enhancement was required. This enhancement was made in the form of Memory Hierarchy Design because of which the performance of the system increases. One of the most significant ways to increase
system performance is minimizing how far down the memory hierarchy one
has to go to manipulate data.
4. **Cost per bit:**As we move from bottom to top in the Hierarchy, the cost per bit increases i.e. Internal Memory is costlier than External Memory.
*** Difference between Byte Addressable Memory and Word Addressable Memory
Memory
 is a storage component in the Computer used to store application 
programs. The Memory Chip is divided into equal parts called as *“CELLS”*. Each Cell is uniquely identified by a binary number called as *“ADDRESS”*. For example, the Memory Chip configuration is represented as **’64 K x 8′** as shown in the figure below.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-2-7.jpg

!https://videocdn.geeksforgeeks.org/geeksforgeeks/WordByteAddressableMemory/Wordbyteaddressablememory20220429164139.jpg

The following information can be obtained from the memory chip representation shown above:

**1. Data Space in the Chip =**

64K X 8

**2. Data Space in the Cell =**

8 bits

**3. Address Space in the Chip =**

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-11dbe424ac2f1d41c41052ff98202472_l3.svg

=16 bits

Now we can clearly state the difference between Byte Addressable Memory & Word Addressable Memory.

[Untitled Database](https://www.notion.so/391830b67e374840b45eeb6046490f20?pvs=21)

**NOTE :**i) The most important point to be noted is that in case of either of Byte Address or Word Address, *the address size* can be any number of bits (depends on the number of cells in the chip) but the *cell size* differs in each case.

ii)The default memory configuration in the Computer design is Byte Addressable .
*** Difference between Simultaneous and Hierarchical Access Memory Organisations
https://www.geeksforgeeks.org/difference-between-simultaneous-and-hierarchical-access-memory-organisations/
*** Register Allocations in Code Generation
[Registers](https://www.geeksforgeeks.org/registers-8085-microprocessor/)
 are the fastest locations in the memory hierarchy. But unfortunately, 
this resource is limited. It comes under the most constrained resources 
of the target processor. Register allocation is an NP-complete problem. 
However, this problem can be reduced to graph coloring to achieve 
allocation and assignment. Therefore a good register allocator computes 
an effective approximate solution to a hard problem. 

!https://media.geeksforgeeks.org/wp-content/uploads/in_out_green-1.png

**Figure –** Input-Output

The
 register allocator determines which values will reside in the register 
and which register will hold each of those values. It takes as its input
 a program with an arbitrary number of registers and produces a program 
with a finite register set that can fit into the target machine. (See 
image)

**Allocation vs Assignment:**

**Allocation –** Maps an unlimited namespace onto that register set of the target machine.

- **Reg. to Reg. Model:** Maps virtual registers to physical registers but spills excess amount to memory.
- **Mem. to Mem. Model:** Maps some subset of the memory location to a set of names that models the physical register set.

Allocation ensures that code will fit the target machine’s reg. set at each instruction.

**Assignment –** Maps an allocated name set to the physical register set of the target machine.

- Assumes allocation has been done so that code will fit into the set of physical registers.
- No more than **‘k’** values are designated into the registers, where ‘k’ is the no. of physical registers.

**General register allocation is an NP-complete problem:**

- Solved in polynomial time, when (no. of required registers) <= (no. of available physical registers).
- An assignment can be produced in linear time using Interval-Graph Coloring.

**Local Register Allocation And Assignment:** Allocation
 just inside a basic block is called Local Reg. Allocation. Two 
approaches for local reg. allocation: Top-down approach and bottom-up 
approach.

Top-Down Approach is a simple approach based on 
‘Frequency Count’. Identify the values which should be kept in registers
 and which should be kept in memory.

**Algorithm:**

1. Compute a priority for each virtual register.
2. Sort the registers into priority order.
3. Assign registers in priority order.
4. Rewrite the code.

**Moving beyond single Blocks:**

- More complicated because the control flow enters the picture.
- Liveness and Live Ranges: Live ranges consist of a set of definitions and uses
that are related to each other as they i.e. no single register can be
common in a such couple of instruction/data.

Following is a 
way to find out Live ranges in a block. A live range is represented as 
an interval [i,j], where i is the definition and j is the last use.

**Global Register Allocation and Assignment:** 1. The main issue of a register allocator is minimizing the impact of spill code;

- Execution time for spill code.
- Code space for spill operation.
- Data space for spilled values.

2. Global allocation can’t guarantee an optimal solution for the execution time of spill code. 3. Prime differences between Local and Global Allocation:

- The structure of a global live range is naturally more complex than the local one.
- Within a global live range, distinct references may execute a different number of times. (When basic blocks form a loop)

4.
 To make the decision about allocation and assignments, the global 
allocator mostly uses graph coloring by building an interference graph. 5. Register allocator then attempts to construct a k-coloring for that graph where ‘k’ is the no. of physical registers.

- In case, the compiler can’t directly construct a k-coloring for that
graph, it modifies the underlying code by spilling some values to memory and tries again.
- Spilling actually simplifies that graph which ensures that the algorithm will halt.

6.
 Global Allocator uses several approaches, however, we’ll see top-down 
and bottom-up allocations strategies. Subproblems associated with the 
above approaches.

- Discovering Global live ranges.
- Estimating Spilling Costs.
- Building an Interference graph.

**Discovering Global Live Ranges:** How to discover Live range for a variable?

!https://media.geeksforgeeks.org/wp-content/uploads/live_ranges_green.png

**Figure –** Discovering live ranges in a single block

The
 above diagram explains everything properly. Let’s take the example of 
Rarp, it’s been initialized at program point 1 and its last usage is at 
program point 11. Therefore, the Live Range of Rarp i.e. Larp is [1,11].
 Similarly, others follow up.

!https://media.geeksforgeeks.org/wp-content/uploads/dis_live_ranges_green.png

**Figure –** Discovering Live Ranges

**Estimating Global Spill Cost:**

- Essential for taking a spill decision which includes – address computation,
memory operation cost, and estimated execution frequency.
- For performance benefits, these spilled values are kept typically for the Activation records.
- Some embedded processors offer ScratchPad Memory to hold such spilled values.
- **Negative Spill Cost**: Consecutive load-store for a single address needs to be removed as it increases the burden, hence incurs negative spill cost.
- **Infinite Spill Cost**: A live range should have infinite spill cost if no other live range ends between its definition and its use.

**Interference and Interference Graph:**

!https://media.geeksforgeeks.org/wp-content/uploads/dis_live_ranges_green.png

**Figure –** Building Interference Graph from Live Ranges 

From
 the above diagram, it can be observed that the live range LRA starts in
 the first basic block and ends in the last basic block. Therefore it 
will share an edge with every other live Range i.e. Lrb, Lrc,Lrd. 
However, Lrb, Lrc, Lrd doesn’t overlap with any other live range except 
Lra so they are only sharing an edge with Lra.

**Building an Allocator:**

- Note that a k-colorable graph finding is an NP-complete problem, so we need an approximation for this.
- Try with live range splitting into some non-trivial chunks (most used ones).

**Top-Down Colouring**:

1. Tries to color live range in an order determined by some ranking functions i.e. priority based.
2. If no color is available for a live range, the allocator invokes either spilling or splitting to handle uncolored ones.
3. Live ranges having k or more neighbors are called constrained nodes and are difficult to handle.
4. The unconstrained nodes are comparatively easy to handle.
5. **Handling Spills:** When no color is found for some live ranges, spilling is needed to be
done, but this may not be a final/ultimate solution of course.
6. **Live Range Splitting:** For uncolored ones, split the live range into sub-ranges, those may
have fewer interferences than the original one so that some of them can
be colored at least.

**Chaitin’s Idea:**

- Choose an arbitrary node of ( degree < k ) and put it in the stack.
- Remove that node and all its edges from the graph. (This may decrease the
degree of some other nodes and cause some more nodes to have degree = k, some node has to be spilled.
- If no vertex needs to be spilled,
successively pop vertices off the stack and color them in a color not
used by neighbors. (reuse colors as far as possible).

**Coalescing copies to reduce degree:** The compiler can use the interference graph to coalesce two live ranges. So by coalescing, what type of benefits can you get?

!https://media.geeksforgeeks.org/wp-content/uploads/coalescing_lr_green.png

**Figure –** Coalescing Live Ranges

**Comparing Top-Down and Bottom-Up allocator:**

- Top-down allocator could adopt the ‘spill and iterate’ philosophy used in bottom-up ones.
- ‘Spill and iterate’ trades additional compile time for an allocation that potentially, uses less spill code.
- Top-Down uses priority ranking to order all the constrained nodes. (However, it
colors the unconstrained nodes in arbitrary order)
- Bottom-up constructs an order in which most nodes are colored in a graph where they are unconstrained.

!https://media.geeksforgeeks.org/wp-content/uploads/structure_of_coloring_all_green.png

**Figure –** Coalescing Live Ranges
*** Cache Memory
A faster and smaller segment of memory whose access time is as close as registers are known as Cache memory. In a [hierarchy](https://www.geeksforgeeks.org/memory-hierarchy-design-and-its-characteristics/)
 of memory, cache memory has access time lesser than primary memory. 
Generally, cache memory is very smaller and hence is used as a buffer.

### Need of cache memory

Data
 in primary memory can be accessed faster than secondary memory but 
still, access times of primary memory are generally in few microseconds,
 whereas CPU is capable of performing operations in nanoseconds. Due to 
the time lag between accessing data and acting of data performance of 
the system decreases as the CPU is not utilized properly, it may remain 
idle for some time. In order to minimize this time gap new segment of 
memory is Introduced known as Cache Memory.

### How does cache work?

In order to understand the working of cache we must understand few points:

- Cache memory is faster, they can be accessed very fast
- Cache memory is smaller, a large amount of data cannot be stored

Whenever
 CPU needs any data it searches for corresponding data in the cache 
(fast process) if data is found, it processes the data according to 
instructions, however, if data is not found in the cache CPU search for 
that data in primary memory(slower process) and loads it into the cache.
 This ensures frequently accessed data are always found in the cache and
 hence minimizes the time required to access the data.

### Cache performance

- On searching in the cache if data is found, a cache hit has occurred.
- On searching in the cache if data is not found, a cache miss has occurred.

> Performance
 of cache is measured by the number of cache hits to the number of 
searches. This parameter of measuring performance is known as the Hit Ratio.
> 
> 
> **Hit ratio=(Number of cache hits)/(Number of searches)**
> 

### Types of Cache Memory

**L1 or Level 1 Cache:** It
 is the first level of cache memory that is present inside the 
processor. It is present in a small amount inside every core of the 
processor separately. The size of this memory ranges from 2KB to 64 KB.

**L2 or Level 2 Cache:** It
 is the second level of cache memory that may present inside or outside 
the CPU. If not present inside the core, It can be shared between two 
cores depending upon the architecture and is connected to a processor 
with the high-speed bus. The size of memory ranges from 256 KB to 512 
KB.

**L3 or Level 3 Cache:** It is the third level of
 cache memory that is present outside the CPU and is shared by all the 
cores of the CPU. Some high processors may have this cache. This cache 
is used to increase the performance of the L2 and L1 cache. The size of 
this memory ranges from 1 MB to 8MB.

### Cache vs RAM

Although
 Cache and RAM both are used to increase the performance of the system 
there exists a lot of differences in which they operate to increase the 
efficiency of the system.

[Untitled Database](https://www.notion.so/e16472a6e5344731a7d150ad0b874c0c?pvs=21)
*** Cache Organization | Set 1 (Introduction)
Cache
 is close to CPU and faster than main memory. But at the same time is 
smaller than main memory. The cache organization is about mapping data 
in memory to a location in cache.

**A Simple Solution:**One
 way to go about this mapping is to consider last few bits of long 
memory address to find small cache address, and place them at the found 
address.

**Problems With Simple Solution:**The 
problem with this approach is, we loose the information about high order
 bits and have no way to find out the lower order bits belong to which 
higher order bits.

!https://media.geeksforgeeks.org/wp-content/uploads/cache.jpg

**Solution is Tag:**To
 handle above problem, more information is stored in cache to tell which
 block of memory is stored in cache. We store additional information as **Tag**

!https://media.geeksforgeeks.org/wp-content/uploads/cache-tag.jpg

**What is a Cache Block?**Since
 programs have Spatial Locality (Once a location is retrieved, it is 
highly probable that the nearby locations would be retrieved in near 
future). So a cache is organized in the form of blocks. Typical cache 
block sizes are 32 bytes or 64 bytes.

!https://media.geeksforgeeks.org/wp-content/uploads/cache-tag-3.jpg

**The above arrangement is Direct Mapped Cache and it has following problem**We
 have discussed above that last few bits of memory addresses are being 
used to address in cache and remaining bits are stored as tag. Now 
imagine that cache is very small and addresses of 2 bits. Suppose we use
 the last two bits of main memory address to decide the cache (as shown 
in below diagram). So if a program accesses 2, 6, 2, 6, 2, …, every 
access would cause a hit as 2 and 6 have to be stored in same location 
in cache.

!https://media.geeksforgeeks.org/wp-content/uploads/cache-problem.jpg

**Solution to above problem – Associativity**What
 if we could store data at any place in cache, the above problem won’t 
be there? That would slow down cache, so we do something in between.

!https://media.geeksforgeeks.org/wp-content/uploads/cache-solution.jpg

**Source:**
*** Computer Organization | Locality and Cache friendly code
[Caches](https://www.geeksforgeeks.org/cache-memory/)
 are the faster memories that are built to deal with the 
Processor-Memory gap in data read operation, i.e. the time difference in
 a data read operation in a CPU register and that in the main memory. 
Data read operation in registers is generally 100 times faster than in 
the main memory and it keeps on increasing substantially, as we go down 
in the memory hierarchy.

Caches
 are installed in the middle of CPU registers and the main memory to 
bridge this time gap in data reading. Caches serve as temporary staging 
area for a subset of data and instructions stored in relatively slow 
main memory. Since the size of cache is small, only the data which is 
frequently used by the processor during the execution of a program is 
stored in cache. Caching of this frequently used data by CPU eliminates 
the need of bringing the data from the slower main memory again and 
again which takes hundreds of CPU cycles.

The idea of caching the useful data centers around a fundamental property of computer programs known as **locality**.
 Programs with good locality tend to access the same set of data items 
over and over again from the upper levels of the memory hierarchy (i.e. 
cache) and thus run faster.

**Example:** The run 
time of different matrix multiplication kernels that perform the same 
number of arithmetic operations, but have different degrees of locality,
 can vary by a factor of 20!

**Types of Locality:**

- **Temporal locality –** Temporal locality states that the same data objects are likely to be reused
multiple times by the CPU during the execution of a program. Once a data object has been written into the cache on the first miss, a number of
subsequent hits on that object can be expected. Since the cache is
faster than the storage at the next lower level like the main memory,
these subsequent hits can be served much faster than the original miss.
- **Spatial locality –** It states that if a data object is referenced once, then there is a high
probability that it’s neighbor data objects will also be referenced in
near future. Memory blocks usually contain multiple data objects.
Because of spatial locality, we can expect that the cost of copying a
block after a miss will be amortized by subsequent references to other
objects within that block.

**Importance of Locality –** Locality
 in programs has an enormous impact on the design and performance of 
hardware and software systems. In modern computing systems, the locality
 based advantages are not only confined to the architecture but also, 
operating systems and application programs are built in a manner that 
they can exploit the locality to the full extent.

In operating 
systems, the principle of locality allows the system to use main memory 
as a cache of the most recently referenced chunk of virtual address 
space and also in case of recently used disk blocks in disk file 
systems.

Similarly, Application programs like web browsers 
exploit temporal locality by caching recently referenced documents on a 
local disk. High-volume web servers hold recently requested documents in
 the front end disk cache that satisfy requests for these documents 
without any intervention of server.

**Cache Friendly Code –** Programs
 with good locality generally run faster as they have lower cache miss 
rate in comparison with the ones with bad locality. In a good 
programming practice, cache performance is always counted as one of the 
important factor when it comes to the analysis of the performance of a 
program. The basic approach on how a code can be cache friendly is:

- **Frequently used cases need to be faster:** Programs often invest most of the time in a few core functions and
these functions in return have most to do with the loops. So, these
loops should be designed in a way that they possess a good locality.
- **Multiple loops:** If a program constitutes of multiple loops then minimize the cache
misses in the inner loop to alleviate the performance of the code.

**Example-1:**
 The above context can be understood by following the simple examples of
 multi-dimensional array code. Consider the sum_array() function which 
sums the elements of a two dimension array in row-major order:

```
int sumarrayrows(int a[8][4])
{
 int i, j, sum = 0;
 for (i = 0; i < 8; i++)
    for (j = 0; j < 4; j++)
     sum += a[i][j];
 return sum;
}
```

Assuming, the cache has a block size of 4 words each, word 
size being 4 bytes. It is initially empty and since, C stores arrays in 
row-major order so the references will result in the following pattern 
of hits and misses, independent of cache organization.

!https://media.geeksforgeeks.org/wp-content/uploads/cache1-1.png

The
 block which contains w[0]–w[3] is loaded into the cache from memory and
 reference to w[0] is a miss but the next three references are all hits.
 The reference to v[4] causes another miss as a new block is loaded into
 the cache, the next three references are hits, and so on. In general, 
three out of four references will hit, which is the best that can be 
done with a cold cache. Thus, the hit ratio is 3/4*100 = 75%

**Example-2:** Now, the sum_array() function sums the elements of a two dimension array in column-major order. 

```
int sum_array(int a[8][8])
{
 int i, j, sum = 0;
 for (j = 0; j < 8; j++)
   for (i = 0; i < 8; i++)
   sum += a[i][j];
 return sum;
}
```

The cache layout of the program will be as shown in the figure:

!https://media.geeksforgeeks.org/wp-content/uploads/cache23.png

As
 C stores arrays in row-major order but in this case array is being 
accessed in column major order, so the locality spoils in this case. the
 references will be made in order: a[0][0], a[1][0], a[2][0] and so on. 
As the cache size is smaller, with each reference there will be a miss 
due to poor locality of the program. Hence, the hit ratio will be 0. 
Poor hit ratio will eventually decrease the performance of a program and
 will lead to a slower execution. In programming, these type of 
practices should be avoided.

**Conclusion –** When
 talking about real life application programs and programming realms, 
optimized cache performance gives a good speedup to a program, even if 
the runtime complexity of the program is high. A good example is Quick 
sort. Though it has a worst case complexity of O(n2), it is 
the most popular sorting algorithm and one of the important factor is 
the better cache performance than many other sorting algorithms. Codes 
should be written in a way that they can exploit the cache to the best 
extent for a faster execution.
*** Multilevel Cache Organisation
**[Cache](https://www.geeksforgeeks.org/cache-memory/)** is a random access memory used by the CPU to reduce the average time taken to access memory. **Multilevel Caches** is one of the techniques to improve Cache Performance by reducing the *“MISS PENALTY”*. Miss Penalty refers to the extra time required to bring the data into cache from the Main memory whenever there is a ***“miss”*** in the cache. For
 clear understanding let us consider an example where the CPU requires 
10 Memory References for accessing the desired information and consider 
this scenario in the following 3 cases of System design :

**Case 1 : System Design without Cache Memory**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/121212.png

Here the CPU directly communicates with the main memory and no caches are involved. In this case, the CPU needs to access the main memory 10 times to access the desired information.

**Case 2 : System Design with Cache Memory**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/22222111111.png

Here the CPU at first checks whether the desired data is present in the Cache Memory or not i.e. whether there is a ***“hit”*** in cache or ***“miss”*** in the cache. Suppose there is *3* miss
 in Cache Memory then the Main Memory will be accessed only 3 times. We 
can see that here the miss penalty is reduced because the Main Memory is
 accessed a lesser number of times than that in the previous case.

**Case 3 : System Design with Multilevel Cache Memory**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/3333311111111.png

Here
 the Cache performance is optimized further by introducing multilevel 
Caches. As shown in the above figure, we are considering 2 level Cache 
Design. Suppose there is *3 miss* in the L1 Cache Memory and out of these 3 misses there is *2 miss*
 in the L2 Cache Memory then the Main Memory will be accessed only 2 
times. It is clear that here the Miss Penalty is reduced considerably 
than that in the previous case thereby improving the Performance of 
Cache Memory.

**NOTE :** We can observe 
from the above 3 cases that we are trying to decrease the number of Main
 Memory References and thus decreasing the Miss Penalty in order to 
improve the overall System Performance. Also, it is important to note 
that in the Multilevel Cache Design, L1 Cache is attached to the CPU and
 it is small in size but fast. Although, L2 Cache is attached to the 
Primary Cache i.e. L1 Cache and it is larger in size and slower but 
still faster than the Main Memory.

```
Effective Access Time = Hit rate * Cache access time
                      + Miss rate * Lower level access time

```

**Average access Time For Multilevel Cache:(Tavg)**

Tavg = H1 * C1 + (1 – H1) * (H2 * C2 +(1 – H2) *M )

where H1 is the Hit rate in the L1 caches. H2 is the Hit rate in the L2 cache. C1 is the Time to access information in the L1 caches. C2 is the Miss penalty to transfer information from the L2 cache to an L1 cache. M is the Miss penalty to transfer information from the main memory to the L2 cache.

**Example:** Find
 the Average memory access time for a processor with a 2 ns clock cycle 
time, a miss rate of 0.04 misses per instruction, a missed penalty of 25
 clock cycles, and a cache access time (including hit detection) of 1 
clock cycle. Also, assume that the read and write miss penalties are the
 same and ignore other write stalls.

**Solution:**

Average Memory access time(AMAT)= Hit Time + Miss Rate * Miss Penalty.

Hit Time = 1 clock cycle (Hit time = Hit rate * access time) but here Hit time is directly given so,

Miss rate = 0.04

Miss Penalty= 25 clock cycle (this is the time taken by the above level of memory after the hit)

so, AMAT= 1 + 0.04 * 25 AMAT= 2 clock cycle

according to question 1 clock cycle = 2 ns

AMAT = 4ns
*** Locality of Reference and Cache Operation in Cache MemoryLocality of Reference and Cache Operation in Cache Memory
[Locality of reference](https://www.geeksforgeeks.org/computer-organization-locality-and-cache-friendly-code/)
 refers to a phenomenon in which a computer program tends to access same
 set of memory locations for a particular time period. In other words, **Locality of Reference**
 refers to the tendency of the computer program to access instructions 
whose addresses are near one another. The property of locality of 
reference is mainly shown by loops and subroutine calls in a program.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram-14.jpg

1. In case of loops in program control processing unit repeatedly refers to the set of instructions that constitute the loop.
2. In case of subroutine calls, everytime the set of instructions are fetched from memory.
3. References to data items also get localized that means same data item is referenced again and again.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram1-2.jpg

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram2-3.jpg

In
 the above figure, you can see that the CPU wants to read or fetch the 
data or instruction. First, it will access the cache memory as it is 
near to it and provides very fast access. If the required data or 
instruction is found, it will be fetched. This situation is known as a 
cache hit. But if the required data or instruction is not found in the 
cache memory then this situation is known as a cache miss. Now the main 
memory will be searched for the required data or instruction that was 
being searched and if found will go through one of the two ways:

1. First way is that the CPU should fetch the required data or instruction and
use it and that’s it but what, when the same data or instruction is
required again.CPU again has to access the same main memory location for it and we already know that main memory is the slowest to access.
2. The second way is to store the data or instruction in the cache memory so
that if it is needed soon again in the near future it could be fetched
in a much faster way.

**Cache Operation:**It
 is based on the principle of locality of reference. There are two ways 
with which data or instruction is fetched from main memory and get 
stored in cache memory. These two ways are the following:

1. **Temporal Locality –**Temporal locality means current data or instruction that is being fetched may be needed soon. So we should store that data or instruction in the cache
memory so that we can avoid again searching in main memory for the same
data.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram3.jpg
    
    When
     CPU accesses the current main memory location for reading required data
     or instruction, it also gets stored in the cache memory which is based 
    on the fact that same data or instruction may be needed in near future. 
    This is known as temporal locality. If some data is referenced, then 
    there is a high probability that it will be referenced again in the near
     future.
    
2. **Spatial Locality –**Spatial locality means instruction or data near to the current memory location that is being
fetched, may be needed soon in the near future. This is slightly
different from the temporal locality. Here we are talking about nearly
located memory locations while in temporal locality we were talking
about the actual memory location that was being fetched.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram4.jpg
    

**Cache Performance:**The
 performance of the cache is measured in terms of hit ratio. When CPU 
refers to memory and find the data or instruction within the [Cache Memory](https://www.geeksforgeeks.org/cache-memory/),
 it is known as cache hit. If the desired data or instruction is not 
found in the cache memory and CPU refers to the main memory to find that
 data or instruction, it is known as a cache miss.

```
Hit + Miss  = Total CPU Reference
Hit Ratio(h) = Hit / (Hit+Miss)
```

Average access time of any memory system consists of two levels: Cache and Main Memory. If *Tc* is time to access cache memory and *Tm* is the time to access main memory then we can write:

```
Tavg = Average time to access memory
Tavg = h * Tc + (1-h)*(Tm + Tc)
```
*** Computer Organization | Amdahl’s law and its proof
It 
is named after computer scientist Gene Amdahl( a computer architect from
 IBM and Amdahl corporation), and was presented at the AFIPS Spring 
Joint Computer Conference in 1967. It is also known as ***Amdahl’s argument***.
 It is a formula which gives the theoretical speedup in latency of the 
execution of a task at a fixed workload that can be expected of a system
 whose resources are improved. In other words, it is a formula used to 
find the maximum improvement possible by just improving a particular 
part of a system. It is often used in *parallel computing* to predict the theoretical speedup when using multiple processors.

**Speedup-**Speedup
 is defined as the ratio of performance for the entire task using the 
enhancement and performance for the entire task without using the 
enhancement or speedup can be defined as the ratio of execution time for
 the entire task without using the enhancement and execution time for 
the entire task using the enhancement.If **Pe** is the performance for entire task using the enhancement when possible, **Pw** is the performance for entire task without using the enhancement, **Ew** is the execution time for entire task without using the enhancement and **Ee** is the execution time for entire task using the enhancement when possible then,

**Speedup = Pe/Pw**or**Speedup = Ew/Ee**

Amdahl’s law uses two factors to find speedup from some enhancement –

- **Fraction enhanced** – The fraction of the computation time in the original computer that
can be converted to take advantage of the enhancement. For example- if
10 seconds of the execution time of a program that takes 40 seconds in
total can use an enhancement , the fraction is 10/40. This obtained
value is *Fraction Enhanced*.*Fraction enhanced is always less than 1*.
- **Speedup enhanced** – The improvement gained by the enhanced execution mode; that is, how
much faster the task would run if the enhanced mode were used for the
entire program. For example – If the enhanced mode takes, say 3 seconds
for a portion of the program, while it is 6 seconds in the original
mode, the improvement is 6/3. This value is Speedup enhanced.*Speedup Enhanced is always greater than 1*.

The overall Speedup is the ratio of the execution time:-

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-06-10-23-26-41.png

**Proof :-**Let
 Speedup be S, old execution time be T, new execution time be T’ , 
execution time that is taken by portion A(that will be enhanced) is t, 
execution time that is taken by portion A(after enhancing) is t’, 
execution time that is taken by portion that won’t be enhanced is tn, Fraction enhanced is f’, Speedup enhanced is S’.

Now from the above equation,

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-306f6867775b556fb4e85599639d2f98_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-4fca58ae3eccfae1fc33cfa7d0ac001d_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-be26526ebec2d4e71bc098f6b85f00a4_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b07db43006d4c357b72a81bb51f0ec17_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-4a0924a20d0b89cb9a32ee990e888b5a_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-84bbc1c046cef786d0448957aae5b0db_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-0a68c2a4c2ff36e28e216f051101ed89_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-bc9e53dd10db7efe34e5221087043678_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b7f5db735945dac9858a1240bae4f860_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-9de5cc3c72fd888f2e24f35db3c578c7_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a480146551fe1f9b502732b44cf9fc6d_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-f696410782d249d320b3980ee16a2821_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-81a2ea74e1903be04c4f904cddf5e03c_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b7eb6f2b4b5993132c70e24fcc582fe5_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-9c21472c6014873b9eb7c7d5408d17e0_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2c252187674640a4ad80fcb0492c794a_l3.svg

Hence proved.
*** Subroutine, Subroutine nesting and Stack memory
**1. Subroutine –** A
 set of instructions that are used repeatedly in a program can be 
referred to as Subroutine. Only one copy of this Instruction is stored 
in the memory. When a Subroutine is required it can be called many times
 during the Execution of a particular program. A *call Subroutine Instruction*
 calls the Subroutine. Care Should be taken while returning a Subroutine
 as Subroutine can be called from a different place from the memory.

The content of the PC must be Saved by the call Subroutine Instruction to make a correct return to the calling program.

!https://media.geeksforgeeks.org/wp-content/uploads/call-subroutine-instruction-1.png

**Figure –** Process of a subroutine in a program

The
 subroutine linkage method is a way in which computers call and return 
the Subroutine. The simplest way of Subroutine linkage is saving the 
return address in a specific location, such as a register which can be 
called a link register call Subroutine.

**2. Subroutine Nesting –** Subroutine nesting is a common Programming practice In which one Subroutine calls another Subroutine.

!https://media.geeksforgeeks.org/wp-content/uploads/Subroutine-nesting-1.png

**Figure –** Subroutine calling another subroutine

From
 the above figure, assume that when Subroutine 1 calls Subroutine 2 the 
return address of Subroutine 2 should be saved somewhere. So if the link
 register stores the return address of Subroutine 1 this will be (*destroyed/overwritten*)
 by the return address of Subroutine 2. As the last Subroutine called is
 the first one to be returned ( Last in first out format). So stack data
 structure is the most efficient way to store the return addresses of 
the Subroutines.

!https://media.geeksforgeeks.org/wp-content/uploads/20201111073913/Subroutine-300x148.jpg

**Figure –** Return address of subroutine is stored in stack memory

**3. Stack memory –** Stack
 is a basic data structure that can be implemented anywhere in the 
memory. It can be used to store variables that may be required afterward
 in the program Execution. In a stack, the first data put will be the 
last to get out of a stack. So the last data added will be the first one
 to come out of the stack (last in first out).

!https://media.geeksforgeeks.org/wp-content/uploads/stack-memory-1.png

**Figure –** Stack memory having data A, B & C

So from the diagram above first A is added then B & C. While removing first C is Removed then B & A.
*** Difference between RAM and ROMDifference between RAM and ROM
Prerequisite – [Types of computer memory (RAM and ROM)](https://www.geeksforgeeks.org/types-computer-memory-ram-rom/)

**Random Access Memory (RAM)**
 is used to store the programs and data being used by the CPU in 
real-time. The data on the random access memory can be read, written, 
and erased any number of times. RAM is a hardware element where the data
 being currently used is stored. It is a volatile memory. Types of RAM:

1. **Static RAM**, or (SRAM) which stores a bit of data using the state of a six transistor memory cell.
2. **Dynamic RAM**, or (DRAM) which stores a bit data using a pair of transistor and capacitor which constitute a DRAM memory cell.

**Read Only Memory (ROM)**
 is a type of memory where the data has been prerecorded. Data stored in
 ROM is retained even after the computer is turned off ie, non-volatile.
 Types of ROM:

1. **Programmable ROM**, where the data is written after the memory chip has been created. It is non-volatile.
2. **Erasable Programmable ROM**, where the data on this non-volatile memory chip can be erased by exposing it to high-intensity UV light.
3. **Electrically Erasable Programmable ROM**, where the data on this non-volatile memory chip can be electrically erased using field electron emission.
4. **Mask ROM**, in which the data is written during the manufacturing of the memory chip.

The following table differentiates ROM and RAM:

[Untitled Database](https://www.notion.so/bc30af0d88704bfdae1949ac682f8684?pvs=21)
*** What’s difference between CPU Cache and TLB?
Both
 CPU Cache and TLB are hardware used in microprocessors but what’s the 
difference, especially when someone says that TLB is also a type of 
Cache?

First thing first. **CPU Cache**
 is a fast memory that is used to improve the latency of fetching 
information from Main memory (RAM) to CPU registers. So CPU Cache sits 
between Main memory and CPU. And this cache stores information 
temporarily so that the next access to the same information is faster. A
 CPU cache which used to store executable instructions, it’s called 
Instruction Cache (I-Cache). A CPU cache is used to store data, it’s 
called Data Cache (D-Cache). So I-Cache and D-cache speed up fetching 
time for instructions and data respectively. A modern processor contains
 both I-Cache and D-Cache. For completeness, let us discuss D-cache 
hierarchy as well. D-Cache is typically organized in a hierarchy i.e. 
Level 1 data cache, Level 2 data cache, etc. It should be noted that L1 
D-cache is faster/smaller/costlier as compared to L2 D-Cache. But the 
basic idea of ‘*CPU cache*‘ is to speed up instruction/data fetch time from Main memory to CPU.

**Translation Lookaside Buffer (i.e. TLB)**
 is required only if Virtual Memory is used by a processor. In short, 
TLB speeds up the translation of virtual addresses to a physical address
 by storing page-table in faster memory. In fact, TLB also sits between 
CPU and Main memory. Precisely speaking, TLB is used by MMU when a 
virtual address needs to be translated to a physical address. By keeping
 this mapping of virtual-physical addresses in fast memory, access to 
page-table improves. It should be noted that page-table (which itself is
 stored in RAM) keeps track of where virtual pages are stored in the 
physical memory. In that sense, TLB also can be considered as a cache of
 the page table.

But the scope of operation for *TLB* and *CPU Cach*e
 is different. TLB is about ‘speeding up address translation for Virtual
 memory’ so that page-table needn’t be accessed for every address. CPU 
Cache is about ‘speeding up main memory access latency’ so that RAM 
isn’t accessed always by the CPU. TLB operation comes at the time of 
address translation by MMU while CPU cache operation comes at the time 
of memory access by CPU. In fact, any modern processor deploys all 
I-Cache, L1 & L2 D-Cache, and TLB.

!https://media.geeksforgeeks.org/wp-content/uploads/20190305183915/222221.png

**Let us understand this in a Tabular Form -:**

[Untitled Database](https://www.notion.so/a854fbe384e34de785fe88a34db8789b?pvs=21)

Please
 do Like/Share if you find the above useful. Also, please do leave us a 
comment for further clarification or info. We would love to help and 
learn

!https://s.w.org/images/core/emoji/11/svg/1f642.svg
*** Different Types of RAM (Random Access Memory )
RAM(Random
 Access Memory) is a part of computer’s Main Memory which is directly 
accessible by CPU. RAM is used to Read and Write data into it which is 
accessed by CPU randomly. RAM is volatile in nature, it means if the 
power goes off, the stored information is lost. RAM is used to store the
 data that is currently processed by the CPU. Most of the programs and 
data that are modifiable are stored in RAM.

Integrated RAM chips are available in two form:

1. SRAM(Static RAM)
2. DRAM(Dynamic RAM)

The block diagram of RAM chip is given below.

!https://media.geeksforgeeks.org/wp-content/uploads/RAM.png

**1. SRAM :**The
 SRAM memories consist of circuits capable of retaining the stored 
information as long as the power is applied. That means this type of 
memory requires constant power. SRAM memories are used to build Cache 
Memory.

**SRAM Memory Cell:** Static memories(SRAM) 
are memories that consist of circuits capable of retaining their state 
as long as power is on. Thus this type of memory is called volatile 
memory. The below figure shows a cell diagram of SRAM. A latch is formed
 by two inverters connected as shown in the figure. Two transistors T1 
and T2 are used for connecting the latch with two-bit lines. The purpose
 of these transistors is to act as switches that can be opened or closed
 under the control of the word line, which is controlled by the address 
decoder. When the word line is at 0-level, the transistors are turned 
off and the latch remains its information. For example, the cell is at 
state 1 if the logic value at point A is 1 and at point, B is 0. This 
state is retained as long as the word line is not activated. 

!https://media.geeksforgeeks.org/wp-content/uploads/SRAM.png

For **Read operation**,
 the word line is activated by the address input to the address decoder.
 The activated word line closes both the transistors (switches) T1 and 
T2. Then the bit values at points A and B can transmit to their 
respective bit lines. The sense/write circuit at the end of the bit 
lines sends the output to the processor. For **Write operation**,
 the address provided to the decoder activates the word line to close 
both the switches. Then the bit value that is to be written into the 
cell is provided through the sense/write circuit and the signals in bit 
lines are then stored in the cell.

**2. DRAM :**DRAM
 stores the binary information in the form of electric charges applied 
to capacitors. The stored information on the capacitors tends to lose 
over a period of time and thus the capacitors must be periodically 
recharged to retain their usage. The main memory is generally made up of
 DRAM chips.

**DRAM Memory Cell:** Though SRAM is 
very fast, but it is expensive because of its every cell requires 
several transistors. Relatively less expensive RAM is DRAM, due to the 
use of one transistor and one capacitor in each cell, as shown in the 
below figure., where C is the capacitor and T is the transistor. 
Information is stored in a DRAM cell in the form of a charge on a 
capacitor and this charge needs to be periodically recharged. For 
storing information in this cell, transistor T is turned on and an 
appropriate voltage is applied to the bit line. This causes a known 
amount of charge to be stored in the capacitor. After the transistor is 
turned off, due to the property of the capacitor, it starts to 
discharge. Hence, the information stored in the cell can be read 
correctly only if it is read before the charge on the capacitors drops 
below some threshold value. 

!https://media.geeksforgeeks.org/wp-content/uploads/DRAM.png

**Types of DRAM :**There are mainly 5 types of DRAM:

1. **Asynchronous DRAM (ADRAM) –**The DRAM described above is the asynchronous type DRAM. The timing of the
memory device is controlled asynchronously. A specialized memory
controller circuit generates the necessary control signals to control
the timing. The CPU must take into account the delay in the response of
the memory. 
2. **Synchronous DRAM (SDRAM) –**These RAM chips’ access speed is directly synchronized with the CPU’s clock.
For this, the memory chips remain ready for operation when the CPU
expects them to be ready. These memories operate at the CPU-memory bus
without imposing wait states. SDRAM is commercially available as modules incorporating multiple SDRAM chips and forming the required capacity
for the modules. 
3. **Double-Data-Rate SDRAM (DDR SDRAM) –**This faster version of SDRAM performs its operations on both edges of the
clock signal; whereas a standard SDRAM performs its operations on the
rising edge of the clock signal. Since they transfer data on both edges
of the clock, the data transfer rate is doubled. To access the data at
high rate, the memory cells are organized into two groups. Each group is accessed separately. 
4. **Rambus DRAM (RDRAM) –**The RDRAM provides a very high data transfer rate over a narrow CPU-memory
bus. It uses various speedup mechanisms, like synchronous memory
interface, caching inside the DRAM chips and very fast signal timing.
The Rambus data bus width is 8 or 9 bits. 
5. **Cache DRAM (CDRAM) –**This memory is a special type DRAM memory with an on-chip cache memory
(SRAM) that acts as a high-speed buffer for the main DRAM. 

**Difference between SRAM and DRAM :**Below table lists some of the differences between SRAM and DRAM: 

!https://media.geeksforgeeks.org/wp-content/uploads/SRAM-DRAM-1.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Random Access Memory (RAM) and Read Only Memory (ROM)
Memory
 is the most essential element of a computing system because without it 
computer can’t perform simple tasks. Computer memory is of two basic 
types – Primary memory(RAM and ROM) and Secondary memory (hard drive, 
CD, etc). Random Access Memory (RAM) is primary-volatile memory and 
Read-Only Memory (ROM) is primary-non-volatile memory.

!https://media.geeksforgeeks.org/wp-content/uploads/memory.png

Classification of Computer Memory

### **Random Access Memory (RAM) –**

- It is also called read-write *memory* or the *main memory* or the *primary memory*.
- The programs and data that the CPU requires during the execution of a program are stored in this memory.
- It is a volatile memory as the data is lost when the power is turned off.
- RAM is further classified into two types- *[SRAM](https://www.geeksforgeeks.org/sram-full-form/) (Static Random Access Memory)* and *[DRAM](https://www.geeksforgeeks.org/dram-full-form/) (Dynamic Random Access Memory)*.

!https://media.geeksforgeeks.org/wp-content/uploads/difference-1.png

Difference between SRAM and DRAM

### Read-Only **Memory (ROM)**

- Stores crucial information essential to operate the system, like the program essential to boot the computer.
- It is not volatile.
- Always retains its data.
- Used in embedded systems or where the programming needs no change.
- Used in calculators and peripheral devices.
- ROM is further classified into four types- M*ROM*, *[PROM](https://www.geeksforgeeks.org/prom-full-form/)*, *[EPROM](https://www.geeksforgeeks.org/eprom-full-form/)*, and *[EEPROM](https://www.geeksforgeeks.org/eeprom-full-form/)*.

### **Types of Read-Only Memory (ROM)**

- **PROM (Programmable read-only memory)** – It can be programmed by the user. Once programmed, the data and instructions in it cannot be changed.
- **EPROM (Erasable Programmable read-only memory)** – It can be reprogrammed. To erase data from it, expose it to ultraviolet light. To reprogram it, erase all the previous data.
- **EEPROM (Electrically erasable programmable read-only memory)** – The data can be erased by applying an electric field, with no need
for ultraviolet light. We can erase only portions of the chip.
- **MROM(Mask ROM)** – Mask ROM is a kind of read-only memory, that is masked off at the time
of production. Like other types of ROM, mask ROM cannot enable the user
to change the data stored in it. If it can, the process would be
difficult or slow.

!https://media.geeksforgeeks.org/wp-content/uploads/ram.png
*** Hard Disk Drive (HDD) Secondary memory
A hard disk is a memory storage device that looks like this:

!https://media.geeksforgeeks.org/wp-content/uploads/harddisk1.jpg

The disk is divided into **tracks**. Each track is further divided into **sectors**.
 The point to be noted here is that outer tracks are bigger in size than
 the inner tracks but they contain the same number of sectors and have 
equal storage capacity. This is because the storage density is high in 
sectors of the inner tracks whereas the bits are sparsely arranged in 
sectors of the outer tracks. Some space of every sector is used for 
formatting. So, the actual capacity of a sector is less than the given 
capacity.

Read-Write(R-W) head moves over the rotating hard disk.
 It is this Read-Write head that performs all the read and writes 
operations on the disk and hence, the position of the R-W head is a 
major concern. To perform a read or write operation on a memory 
location, we need to place the R-W head over that position. Some 
important terms must be noted here:

1. **Seek time –** The time taken by the R-W head to reach the desired track from its current position.
2. **Rotational latency –** Time is taken by the sector to come under the R-W head.
3. **Data transfer time –** Time is taken to transfer the required amount of data. It depends upon the rotational speed.
4. **Controller time –** The processing time taken by the controller.
5. **Average Access time –** seek time + Average Rotational latency + data transfer time + controller time.

**Note:** Average Rotational latency is mostly 1/2*(Rotational latency).

In questions, if the seek time and controller time are not mentioned, take them to be zero.

If
 the amount of data to be transferred is not given, assume that no data 
is being transferred. Otherwise, calculate the time taken to transfer 
the given amount of data.

The average rotational latency is taken
 when the current position of the R-W head is not given. Because the R-W
 may be already present at the desired position or it might take a whole
 rotation to get the desired sector under the R-W head. But, if the 
current position of the R-W head is given then the rotational latency 
must be calculated.

**Example –** Consider a hard disk with: 4 surfaces 64 tracks/surface 128 sectors/track 256 bytes/sector

1. What is the capacity of the hard disk? Disk capacity = surfaces * tracks/surface * sectors/track * bytes/sector Disk capacity = 4 * 64 * 128 * 256 Disk capacity = 8 MB
2. The disk is rotating at 3600 RPM, what is the data transfer rate? 60 sec -> 3600 rotations 1 sec -> 60 rotations Data transfer rate = number of rotations per second * track capacity *
number of surfaces (since 1 R-W head is used for each surface) Data transfer rate = 60 * 128 * 256 * 4 Data transfer rate = 7.5 MB/sec
3. The disk is rotating at 3600 RPM, what is the average access time? Since seek time, controller time and the amount of data to be transferred is not given, we consider all three terms as 0. Therefore, Average Access time = Average rotational delay Rotational latency => 60 sec -> 3600 rotations 1 sec -> 60 rotations Rotational latency = (1/60) sec = 16.67 msec. Average Rotational latency = (16.67)/2 = 8.33 msec. Average Access time = 8.33 msec.
4. Another example: [GATE IT 2007 | Question 44](https://www.geeksforgeeks.org/gate-gate-it-2007-question-44/)
*** Introduction to solid-state drive (SSD)
**solid-state drive (SSD)**
 is a solid-state storage device that uses integrated circuit assemblies
 as memory to store data. SSD is also known as solid-state disk although
 SSDs do not have physical disks. Form-factors and protocols such as 
SATA and SAS of traditional hard disk drive (HDD) may be used by SSD, 
greatly simplifying usage of SSDs in computers. New form factors such as
 the M.2 form factor, and new I/O protocols such as NVM Express have 
been developed to address specific requirements of the Flash memory 
technology used in SSDs.

There are **no moving mechanical components** in SSD. This makes them different from conventional **electromechanical**
 drives such as hard disk drives (HDDs) or floppy disks, which contain 
movable read/write heads and spinning disks. SSDs are typically more 
resistant to physical shock, run silently, have quicker access time and 
lower latency compared to electromechanical devices. The price of SSDs 
has continued to decline over time but SSDs are in 2018 are still **more expensive** per unit of storage than HDDs and are expected to continue to be so into the next decade.

In 2017, most SSDs use 3D TLC NAND-based flash memory. It is a type of **non-volatile**
 memory that retains data even when power is lost. SSDs may be 
constructed from random-access memory (RAM) for applications requiring 
fast access but not necessarily data persistence after power loss. 
Batteries can be employed as integrated power sources in such devices to
 retain data for a certain amount of time after external power is lost.

SSDs
 store data in electrical charges, which slowly leak over time if left 
without power. This is the reason why SSDs are not suited for archival 
purposes as the worn out drives (that have exceeded their endurance 
rating) start to lose data typically after one (if stored at 30 °C) to 
two (at 25 °C) years in storage.

Hybrid drives or solid-state 
hybrid drives (SSHDs) contains a large hard disk drive and an SSD cache 
to improve performance of frequently accessed data. Apple’s Fusion Drive
 combines the features of SSDs and HDDs in the same unit can be seen as 
an example of hybrid drives.
*** Read and Write operations in Memory
A 
memory unit stores binary information in groups of bits called words. 
Data input lines provide the information to be stored into the memory, 
Data output lines carry the information out from the memory. The control
 lines Read and write specifies the direction of transfer of data. 
Basically, in the memory organization, there are

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d0e7119947ece970758572eb6e8167e0_l3.svg

memory locations indexing from 0 to

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-355afc3da78121f981022a3ac293c7cd_l3.svg

where l is the address buses. We can describe the memory in terms of the bytes using the following formula:

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7c31711d4af317bc4c64916d363b2985_l3.svg

Where,

l is the total address buses

N is the memory in bytes

For example, some storage can be described below in terms of bytes using the above formula:

```
 1kB= 210 Bytes
 64 kB = 26 x 210 Bytes
       = 216 Bytes
 4 GB = 22 x 210(kB) x 210(MB) x 210 (GB)
      = 232 Bytes

```

**Memory Address Register (MAR)** is the address register which is used to store the address of the memory location where the operation is being performed.**Memory Data Register (MDR)** is the data register which is used to store the data on which the operation is being performed.

1. **Memory Read Operation:**Memory read operation transfers the desired word to address lines and
activates the read control line.Description of memory read operation is
given below:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/1-343.png
    
    In
     the above diagram initially, MDR can contain any garbage value and MAR 
    is containing 2003 memory address. After the execution of read 
    instruction, the data of memory location 2003 will be read and the MDR 
    will get updated by the value of the 2003 memory location (3D).
    
2. **Memory Write Operation:**Memory write operation transfers the address of the desired word to the
address lines, transfers the data bits to be stored in memory to the
data input lines. Then it activates the write control line. Description
of the write operation is given below:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/2-253.png
    
    In
     the above diagram, the MAR contains 2003 and MDR contains 3D. After the
     execution of write instruction 3D will be written at 2003 memory 
    location.
    
*** 2D and 2.5D Memory organization
The **internal structure**
 of Memory either RAM or ROM is made up of memory cells that contain a 
memory bit. A group of 8 bits makes a byte. The memory is in the form of
 a multidimensional array of rows and columns. In which, each cell 
stores a bit and a complete row contains a word. A memory simply can be 
divided into this below form. 

```
2n = N
```

where n is the no. of address lines and N is the total memory in bytes. There will be 2n words.

**2D Memory organization –** In
 2D organization, memory is divided in the form of rows and 
columns(Matrix). Each row contains a word, now in this memory 
organization, there is a decoder. A decoder is a combinational circuit 
that contains n input lines and 2n output lines. One of the 
output lines selects the row by the address contained in the MAR and the
 word which is represented by that row gets selected and is either read 
or written through the data lines.

!https://media.geeksforgeeks.org/wp-content/uploads/222-10.png

**2.5D Memory organization –** In
 2.5D Organization the scenario is the same but we have two different 
decoders one is a column decoder and another is a row decoder. Column 
decoder is used to select the column and a row decoder is used to select
 the row. The address from the MAR goes as the decoders’ input. Decoders
 will select the respective cell through the bit outline, then the data 
from that location will be read or through the bit, inline data will be 
written at that memory location.

!https://media.geeksforgeeks.org/wp-content/uploads/33-12.png

**Read and Write Operations –**

1. If the select line is in Reading mode then the Word/bit which is
represented by the MAR will be available to the data lines and will get
read.
2. If the select line is in write mode then the data from the memory data register (MDR) will be sent to the respective cell which is addressed by the memory address register (MAR).
3. With the help of the select line, we can select the desired data and we can perform read and write operations on it. 

**Comparison between 2D & 2.5D Organizations –**

1. In 2D organization hardware is fixed but in 2.5D hardware changes.
2. 2D Organization requires more gates while 2.5D requires less.
3. 2D is more complex in comparison to the 2.5D organization.
4. Error correction is not possible in the 2D organization but in 2.5D it could be done easily.
5. 2D is more difficult to fabricate in comparison to the 2.5D organization.
   
** Input and Output Systems
*** Priority Interrupts | (S|W Polling and Daisy Chaining)
In [I/O Interface (Interrupt and DMA Mode)](https://www.geeksforgeeks.org/io-interface-interrupt-dma-mode/),
 we have discussed the concept behind the Interrupt-initiated I/O. To 
summarize, when I/O devices are ready for I/O transfer, they generate an
 interrupt request signal to the computer. The CPU receives this signal,
 suspends the current instructions it is executing, and then moves 
forward to service that transfer request. But what if multiple devices 
generate interrupts simultaneously. In that case, we have a way to 
decide which interrupt is to be serviced first. In other words, we have 
to set a priority among all the devices for systemic interrupt 
servicing. The concept of defining the priority among devices so as to 
know which one is to be serviced first in case of simultaneous requests 
is called a priority interrupt system. This could be done with either 
software or hardware methods.

SOFTWARE METHOD – POLLING

In
 this method, all interrupts are serviced by branching to the same 
service program. This program then checks with each device if it is the 
one generating the interrupt. The order of checking is determined by the
 priority that has to be set. The device having the highest priority is 
checked first and then devices are checked in descending order of 
priority. If the device is checked to be generating the interrupt, 
another service program is called which works specifically for that 
particular device. The structure will look something like this-

```
if (device[0].flag)
    device[0].service();
else if (device[1].flag)
    device[1].service();
.
.
.
.
.
.
else
    //raise error
```

The major disadvantage of this method is that 
it is quite slow. To overcome this, we can use hardware solution, one of
 which involves connecting the devices in series. This is called 
Daisy-chaining method.

HARDWARE METHOD – DAISY CHAINING

The 
daisy-chaining method involves connecting all the devices that can 
request an interrupt in a serial manner. This configuration is governed 
by the priority of the devices. The device with the highest priority is 
placed first followed by the second highest priority device and so on. 
The given figure depicts this arrangement.

!https://media.geeksforgeeks.org/wp-content/uploads/dasiychaining.png

**WORKING:**

There is an interrupt request line which is common to all the devices and goes into the CPU.

- When no interrupts are pending, the line is in HIGH state. But if any of the devices raises an interrupt, it places the interrupt request line in
the LOW state.
- The CPU acknowledges this interrupt request from
the line and then enables the interrupt acknowledge line in response to
the request.
- This signal is received at the PI(Priority in) input of device 1.
- If the device has not requested the interrupt, it passes this signal to
the next device through its PO(priority out) output. (PI = 1 & PO =
1)
- However, if the device had requested the interrupt, (PI =1 & PO = 0)
    - The device consumes the acknowledge signal and block its further use by placing 0 at its PO(priority out) output.
    - The device then proceeds to place its interrupt vector address(VAD) into the data bus of CPU.
    - The device puts its interrupt request signal in HIGH state to indicate its interrupt has been taken care of.
- If a device gets 0 at its PI input, it generates 0 at the PO output to
tell other devices that acknowledge signal has been blocked. (PI = 0
& PO = 0)

Hence, the device having PI = 1 and PO = 0 is 
the highest priority device that is requesting an interrupt. Therefore, 
by daisy chain arrangement we have ensured that the highest priority 
interrupt gets serviced first and have established a hierarchy. The 
farther a device is from the first device, the lower its priority. This 
article is contributed by **Jatin Gupta**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks. Please 
write comments if you find anything incorrect, or you want to share more
 information about the topic discussed above.

# Priority Interrupts | (S/W Polling and Daisy Chaining)

- Difficulty Level :
[Medium](https://www.geeksforgeeks.org/medium/)
- Last Updated :
23 Mar, 2022

In [I/O Interface (Interrupt and DMA Mode)](https://www.geeksforgeeks.org/io-interface-interrupt-dma-mode/),
 we have discussed the concept behind the Interrupt-initiated I/O. To 
summarize, when I/O devices are ready for I/O transfer, they generate an
 interrupt request signal to the computer. The CPU receives this signal,
 suspends the current instructions it is executing, and then moves 
forward to service that transfer request. But what if multiple devices 
generate interrupts simultaneously. In that case, we have a way to 
decide which interrupt is to be serviced first. In other words, we have 
to set a priority among all the devices for systemic interrupt 
servicing. The concept of defining the priority among devices so as to 
know which one is to be serviced first in case of simultaneous requests 
is called a priority interrupt system. This could be done with either 
software or hardware methods.

SOFTWARE METHOD – POLLING

In
 this method, all interrupts are serviced by branching to the same 
service program. This program then checks with each device if it is the 
one generating the interrupt. The order of checking is determined by the
 priority that has to be set. The device having the highest priority is 
checked first and then devices are checked in descending order of 
priority. If the device is checked to be generating the interrupt, 
another service program is called which works specifically for that 
particular device. The structure will look something like this-

```
if (device[0].flag)
    device[0].service();
else if (device[1].flag)
    device[1].service();
.
.
.
.
.
.
else
    //raise error
```

The major disadvantage of this method is that 
it is quite slow. To overcome this, we can use hardware solution, one of
 which involves connecting the devices in series. This is called 
Daisy-chaining method.

HARDWARE METHOD – DAISY CHAINING

The 
daisy-chaining method involves connecting all the devices that can 
request an interrupt in a serial manner. This configuration is governed 
by the priority of the devices. The device with the highest priority is 
placed first followed by the second highest priority device and so on. 
The given figure depicts this arrangement.

!https://media.geeksforgeeks.org/wp-content/uploads/dasiychaining.png

**WORKING:**

There is an interrupt request line which is common to all the devices and goes into the CPU.

- When no interrupts are pending, the line is in HIGH state. But if any of the devices raises an interrupt, it places the interrupt request line in
the LOW state.
- The CPU acknowledges this interrupt request from
the line and then enables the interrupt acknowledge line in response to
the request.
- This signal is received at the PI(Priority in) input of device 1.
- If the device has not requested the interrupt, it passes this signal to
the next device through its PO(priority out) output. (PI = 1 & PO =
1)
- However, if the device had requested the interrupt, (PI =1 & PO = 0)
    - The device consumes the acknowledge signal and block its further use by placing 0 at its PO(priority out) output.
    - The device then proceeds to place its interrupt vector address(VAD) into the data bus of CPU.
    - The device puts its interrupt request signal in HIGH state to indicate its interrupt has been taken care of.
- If a device gets 0 at its PI input, it generates 0 at the PO output to
tell other devices that acknowledge signal has been blocked. (PI = 0
& PO = 0)

Hence, the device having PI = 1 and PO = 0 is 
the highest priority device that is requesting an interrupt. Therefore, 
by daisy chain arrangement we have ensured that the highest priority 
interrupt gets serviced first and have established a hierarchy. The 
farther a device is from the first device, the lower its priority. This 
article is contributed by **Jatin Gupta**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks. Please 
write comments if you find anything incorrect, or you want to share more
 information about the topic discussed above.
 
*** I/O Interface (Interrupt and DMA Mode)
The 
method that is used to transfer information between internal storage and
 external I/O devices is known as I/O interface. The CPU is interfaced 
using special communication links by the peripherals connected to any 
computer system. These communication links are used to resolve the 
differences between CPU and peripheral. There exists special hardware 
components between CPU and peripherals to supervise and synchronize all 
the input and output transfers that are called interface units.

## Mode of Transfer:

The
 binary information that is received from an external device is usually 
stored in the memory unit. The information that is transferred from the 
CPU to the external device is originated from the memory unit. CPU 
merely processes the information but the source and target is always the
 memory unit. Data transfer between CPU and the I/O devices may be done 
in different modes.

Data transfer to and from the peripherals may be done in any of the three possible ways

1. Programmed I/O.
2. Interrupt- initiated I/O.
3. Direct memory access( DMA).

Now let’s discuss each mode one by one.

1. **Programmed I/O:** It is due to the result of the I/O instructions that are written in the computer program. Each data item transfer is initiated by an
instruction in the program. Usually the transfer is from a CPU register
and memory. In this case it requires constant monitoring by the CPU of
the peripheral devices.
    
    **Example of Programmed I/O:** In
     this case, the I/O device does not have direct access to the memory 
    unit. A transfer from I/O device to memory requires the execution of 
    several instructions by the CPU, including an input instruction to 
    transfer the data from device to the CPU and store instruction to 
    transfer the data from CPU to memory. In programmed I/O, the CPU stays 
    in the program loop until the I/O unit indicates that it is ready for 
    data transfer. This is a time consuming process since it needlessly 
    keeps the CPU busy. This situation can be avoided by using an interrupt 
    facility. This is discussed below.
    
2. **Interrupt- initiated I/O:** Since in the above case we saw the CPU is kept busy unnecessarily. This situation can very well be avoided by using an interrupt driven method
for data transfer. By using interrupt facility and special commands to
inform the interface to issue an interrupt request signal whenever data
is available from any device. In the meantime the CPU can proceed for
any other program execution. The interface meanwhile keeps monitoring
the device. Whenever it is determined that the device is ready for data
transfer it initiates an interrupt request signal to the computer. Upon
detection of an external interrupt signal the CPU stops momentarily the
task that it was already performing, branches to the service program to
process the I/O transfer, and then return to the task it was originally
performing.

**Note:** Both the methods programmed I/O and Interrupt-driven I/O require the active intervention of theprocessor to transfer data between memory and the I/O module, and any data transfer must transversea path through the processor. Thus both these forms of I/O suffer from two inherent drawbacks.

- The I/O transfer rate is limited by the speed with which the processor can test and service adevice.
- The processor is tied up in managing an I/O transfer; a number of instructions must be executedfor each I/O transfer.
1. **Direct Memory Access**: The data transfer between a fast storage media such as magnetic disk
and memory unit is limited by the speed of the CPU. Thus we can allow
the peripherals directly communicate with each other using the memory
buses, removing the intervention of the CPU. This type of data transfer
technique is known as DMA or direct memory access. During DMA the CPU is idle and it has no control over the memory buses. The DMA controller
takes over the buses to manage the transfer directly between the I/O
devices and the memory unit.

!https://media.geeksforgeeks.org/wp-content/uploads/99-1.png

**Bus Request :** It is used by the DMA controller to request the CPU to relinquish the control of the buses.

**Bus Grant :**
 It is activated by the CPU to Inform the external DMA controller that 
the buses are in high impedance state and the requesting DMA can take 
control of the buses. Once the DMA has taken the control of the buses it
 transfers the data. This transfer can take place in many ways.

**Types of DMA transfer using DMA controller:**

**Burst Transfer :**DMA returns the bus after complete data transfer. A register is used as a byte count,being decremented for each byte transfer, and upon the byte count reaching zero, the DMAC willrelease the bus. When the DMAC operates in burst mode, the CPU is halted for the duration of the datatransfer.Steps involved are:

1. Bus grant request time.
2. Transfer the entire block of data at transfer rate of device because the device is usually slow than thespeed at which the data can be transferred to CPU.
3. Release the control of the bus back to CPUSo, total time taken to transfer the N bytes= Bus grant request time + (N) * (memory transfer rate) + Bus release control time.

```
Where,
X µsec =data transfer time or preparation time (words/block)
Y µsec =memory cycle time or cycle time or transfer time (words/block)
% CPU idle (Blocked)=(Y/X+Y)*100
% CPU Busy=(X/X+Y)*100
```

**Cyclic Stealing :**An 
alternative method in which DMA controller transfers one word at a time 
after which it must return the control of the buses to the CPU. The CPU 
delays its operation only for one memory cycle to allow the direct 
memory I/O transfer to “steal” one memory cycle.Steps Involved are:

1. Buffer the byte into the buffer
2. Inform the CPU that the device has 1 byte to transfer (i.e. bus grant request)
3. Transfer the byte (at system bus speed)
4. Release the control of the bus back to CPU.

Before moving on transfer next byte of data, device performs step 1 again so that bus isn’t tied up andthe transfer won’t depend upon the transfer rate of device.So, for 1 byte of transfer of data, time taken by using cycle stealing mode (T).= time required for bus grant + 1 bus cycle to transfer data + time required to release the bus, it will beN x T

In
 cycle stealing mode we always follow pipelining concept that when one 
byte is getting transferred then Device is parallel preparing the next 
byte. “The fraction of CPU time to the data transfer time” if asked then
 cycle stealing mode is used.

```
Where,
X µsec =data transfer time or preparation time
(words/block)
Y µsec =memory cycle time or cycle time or transfer
time (words/block)
% CPU idle (Blocked) =(Y/X)*100
% CPU busy=(X/Y)*100

```

**Interleaved mode:** In this technique , the DMA controller takes over the system bus when themicroprocessor is not using it.An alternate half cycle i.e. half cycle DMA + half cycle processor.**Note:** In Gate Exam you can directly apply above formula for different mode of DMA transfer.
*** Direct memory access with DMA controller 8257|8237
Suppose
 any device which is connected to input-output port wants to transfer 
data to memory, first of all it will send input-output port address and 
control signal, input-output read to input-output port, then it will 
send memory address and memory write signal to memory where data has to 
be transferred. In normal input-output technique the processor becomes 
busy in checking whether any input-output operation is completed or not 
for next input-output operation, therefore this technique is slow.

This
 problem of slow data transfer between input-output port and memory or 
between two memory is avoided by implementing Direct Memory Access (DMA)
 technique. This is faster as the microprocessor/computer is bypassed 
and the control of address bus and data bus is given to the DMA 
controller. 

- HOLD – hold signal
- HLDA – hold acknowledgment
- DREQ – DMA request
- DACK – DMA acknowledgment

!https://media.geeksforgeeks.org/wp-content/uploads/dmac.png

Suppose
 a floppy drive that is connected at input-output port wants to transfer
 data to memory, the following steps are performed: 

- **Step-1:** First of all the floppy drive will send a DMA request (DREQ) to the DMAC, it means the floppy drive wants its DMA service.
- **Step-2:** Now the DMAC will send a HOLD signal to the CPU.
- **Step-3:** After accepting the DMA service request from the DMAC, the CPU will
send hold acknowledgment (HLDA) to the DMAC, it means the microprocessor has released control of the address bus the data bus to DMAC and the
microprocessor/computer is bypassed during DMA service.
- **Step-4:** Now the DMAC will send one acknowledgement (DACL) to the floppy drive
which is connected at the input-output port. It means the DMAC tells the floppy drive be ready for its DMA service.
- **Step-5:** Now with the help of input-output read and memory write signal the data is transferred from the floppy drive to the memory.

**Modes of DMAC:**

**1. Single Mode –** In this only one channel is used, means only a single DMAC is connected to the bus system.

!https://media.geeksforgeeks.org/wp-content/uploads/single-mode.png

**2. Cascade Mode –** In this multiple channels are used, we can further cascade more number of DMACs.

!https://media.geeksforgeeks.org/wp-content/uploads/cascade-1.png
*** Computer Organization | Asynchronous input output synchronization
Asynchronous
 input output is a form of input output processing that allows others 
devices to do processing before the transmission or data transfer is 
done.

**Problem faced in asynchronous input output synchronization –**It is not sure that the data on the data bus is fresh or not as their no time slot for sending or receiving data.

This problem is solved by following mechanism:

1. Strobe
2. Handshaking

Data is transferred from source to destination through data bus in between.

**1. Strobe Mechanism:**

1. **Source initiated Strobe –** When source initiates the process of data transfer. Strobe is just a signal.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/as-8.png
    
    (i) First, source puts data on the data bus and ON the strobe signal.(ii) Destination on seeing the ON signal of strobe, read data from the data bus.(iii) After reading data from the data bus by destination, strobe gets OFF.
    
    Signals can be seen as:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/22-4.jpg
    
    It shows that first data is put on the data bus and then strobe signal gets active.
    
2. **Destination initiated signal –** When destination initiates the process of data transfer.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/as-9.png
    
    (i) First, the destination ON the strobe signal to ensure the source to put the fresh data on the data bus.(ii) Source on seeing the ON signal puts fresh data on the data bus.(iii) Destination reads the data from the data bus and strobe gets OFF signal.
    
    Signals can be seen as:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/ao.png
    
    It shows that first strobe signal gets active then data is put on the data bus.
    

**Problems faced in Strobe based asynchronous input output –**

1. In Source initiated Strobe, it is assumed that destination has read the data from the data bus but their is no surety.
2. In Destination initiated Strobe, it is assumed that source has put the data on the data bus but their is no surety.

This problem is overcome by **Handshaking**.

**2. Handshaking Mechanism:**

1. **Source initiated Handshaking –** When source initiates the data transfer process. It consists of signals:**DATA VALID:** if ON tells data on the data bus is valid otherwise invalid.**DATA ACCEPTED:** if ON tells data is accepted otherwise not accepted.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/ap-2.png
    
    (i) Source places data on the data bus and enable Data valid signal.(ii) Destination accepts data from the data bus and enable Data accepted signal.(iii) After this, disable Data valid signal means data on data bus is invalid now.(iv) Disable Data accepted signal and the process ends.
    
    Now there is surety that destination has read the data from the data bus through data accepted signal.
    
    Signals can be seen as:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/ai.png
    
    It
     shows that first data is put on the data bus then data valid signal 
    gets active and then data accepted signal gets active. After accepting 
    the data, first data valid signal gets off then data accepted signal 
    gets off.
    
2. **Destination initiated Handshaking –** When destination initiates the process of data transfer.**REQUEST FOR DATA:** if ON requests for putting data on the data bus.**DATA VALID:** if ON tells data is valid on the data bus otherwise invalid data.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/ap-1.png
    
    (i) When destination is ready to receive data, Request for Data signal gets activated.(ii) source in response puts data on the data bus and enabled Data valid signal.(iii) Destination then accepts data from the data bus and after accepting data, disabled Request for Data signal.(iv) At last, Data valid signal gets disabled means data on the data bus is no more valid data.
    
    Now there is surety that source has put the data on the data bus through data valid signal.
    
    Signals can be seen as:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/au.png
    
    It
     shows that first Request for Data signal gets active then data is put 
    on data bus then Data valid signal gets active. After reading data, 
    first Request for Data signal gets off then Data valid signal.
    
*** Programmable peripheral interface 8255
PPI 
8255 is a general purpose programmable I/O device designed to interface 
the CPU with its outside world such as ADC, DAC, keyboard etc. We can 
program it according to the given condition. It can be used with almost 
any microprocessor.

It 
consists of three 8-bit bidirectional I/O ports i.e. PORT A, PORT B and 
PORT C. We can assign different ports as input or output functions.

**Block diagram –**

!https://media.geeksforgeeks.org/wp-content/uploads/8255.png

It
 consists of 40 pins and operates in +5V regulated power supply. Port C 
is further divided into two 4-bit ports i.e. port C lower and port C 
upper and port C can work in either BSR (bit set rest) mode or in mode 0
 of input-output mode of 8255. Port B can work in either mode 0 or in 
mode 1 of input-output mode. Port A can work either in mode 0, mode 1 or
 mode 2 of input-output mode.

It has two control groups, control 
group A and control group B. Control group A consist of port A and port C
 upper. Control group B consists of port C lower and port B.

Depending
 upon the value if CS’, A1 and A0 we can select different ports in 
different modes as input-output function or BSR. This is done by writing
 a suitable word in control register (control word D0-D7).

[Untitled Database](https://www.notion.so/068cd796066e4698b1ad0160da0b8176?pvs=21)

**Pin diagram –**

!https://media.geeksforgeeks.org/wp-content/uploads/PPI8255.png

- **PA0 – PA7 –** Pins of port A
- **PB0 – PB7 –** Pins of port B
- **PC0 – PC7 –** Pins of port C
- **D0 – D7 –** Data pins for the transfer of data
- **RESET –** Reset input
- **RD’ –** Read input
- **WR’ –** Write input
- **CS’ –** Chip select
- **A1 and A0 –** Address pins

**Operating modes –**

1. **Bit set reset (BSR) mode –**If MSB of control word (D7) is 0, PPI works in BSR mode. In this mode only port C bits are used for set or reset.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/control-reg.png
    
2. **Input-Output mode –**If MSB of control word (D7) is 1, PPI works in input-output mode. This is further divided into three modes:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/control-reg-1.png
    
    - **Mode 0 –**In this mode all the three ports (port A, B, C) can work as simple input
    function or simple output function. In this mode there is no interrupt
    handling capacity.
    - **Mode 1 –** Handshake I/O mode
    or strobed I/O mode. In this mode either port A or port B can work as
    simple input port or simple output port, and port C bits are used for
    handshake signals before actual data transmission. It has interrupt
    handling capacity and input and output are latched.
        
        Example: A CPU 
        wants to transfer data to a printer. In this case since speed of 
        processor is very fast as compared to relatively slow printer, so before
         actual data transfer it will send handshake signals to the printer for 
        synchronization of the speed of the CPU and the peripherals.
        
        !https://media.geeksforgeeks.org/wp-content/uploads/mode1.png
        
    - **Mode 2 –** Bi-directional data bus mode. In this mode only port A works, and port B can work either in mode 0 or mode 1. 6 bits port C are used as
    handshake signals. It also has interrupt handling capacity.
    
*** Interface 8255 with 8085 microprocessor for 1’s and 2’s complement of a number
**Problem –**
 Interface 8255 with 8085 microprocessor and write an assembly language 
program to display 99 in Port A, 1’s complement of 99 in Port B, and 2’s
 complement of 99 in Port C. If Port addresses are 30H, 32H, and 33H 
resp.

**Example –**

**D7D6D5D4D3D2D1D010000000**

**Algorithm –**

1. Construct the control word register.
2. Input value of accumulator A.
3. Display value of A in Port A.
4. Now 1’s complement of A is calculated and result is displayed in Port B.
5. Now 2’s complement of A is calculated by adding 1 to 1’s complement of A. The result is displayed in Port C.

**Program –**

**MnemonicsComments**MVI A, 80A<–80OUT 33Control Register<–AMVI A, 99A<–99OUT 30Port A<–ACMA1’s complement of AOUT 31Port B<–AINR AA<–A+1OUT 32Port C<–ARETReturn

**Explanation –**

1. **.MVI A, 80:** Value of control register is 80.
2. **OUT 33:** Putting the value of A in 33H which is port number of port control register.
3. **.MVI A, 99:** Value of A is equals to 99.
4. **OUT 30:** Displaying the value of A in 30H which is port number of Port A.
5. **CMA:** Calculates 1’s complement of A.
6. **OUT 31:** Displaying 1’s complement of A in 31H which is port number of Port B.
7. **INR A:** 1’s complement of A is incremented by 1 i.e. 2’s complement of A is calculated.
8. **OUT 32**: Displaying 2’s complement of A in 32 H which is port number of Port C.
9. **RET:** Return.
*** Microprocessor | 8255 (programmable peripheral interface)
**8255**
 is a popularly used parallel, programmable input-output device. It can 
be used to transfer data under various condition from simple 
input-output to interrupt input-output. This is economical, functional, 
flexible but is a little complex and general purpose i/o device that can
 be used with almost any microprocessor.

**8255 pin diagram –** It
 has 24 pins that can be grouped in two 8-bit parallel ports: A and B 
called Port A(PA) and Port B(PB) with the remaining eight known as Port 
C(PC). Port C can be further divided into groups of 4-bits ports named 
Cupper(Cu) and Clower(Cl). There are 40 pins and operates in +5 
regulated power supply.

!https://media.geeksforgeeks.org/wp-content/uploads/IO-ports.-geeksforgeeks.jpg

**Modes of 8255 –** It works in two modes: 

1. Bit set reset (BSR) mode
2. Input/output (I/O) mode

To know in which mode the interface is working we need to know the value of **Control word**.
 Control word is a part of control register in 8255 which specify an I/O
 function for each port. This is format of control word 8255.

!https://media.geeksforgeeks.org/wp-content/uploads/Controlword2geeksforgeeks.jpg

If
 the most significant bit of control word or D7 is 1 then 8255 works in 
I/O mode else, if it’s value is 0 it works in BSR mode. 

1. **BSR Mode –** When MSB of the control register is zero(0), 8255 works in Bit Set-Reset mode.in this only PC bit are used for set and reset. 
2. **I/O Mode –** When MSB of the control register is one(1), 8255 works in Input-Output mode.it is further divided into three categories. 
3. **Mode 0 –** In this mode all three ports (PA, PB, PC) can work as simple input
function or output function also in this mode there is no interrupt
handling capabilities. 
4. **Mode 1 –** In this
either port A or port B can work and port C bits are used as Handshake
signal before actual data transmission plus it has interrupt handling
capabilities. 
5. **Mode 2 –** In this only port A works and port B can work either in Mode 0 or Mode 1 and the 6 bits of
port C are used as Handshake signal plus it also has to interrupt
handling capability. 

To communicate with peripherals through 8255 three steps are necessary: 

1. Determine the addresses of Port A, B, C and Control register according to Chip Select Logic and the Address lines A0 and A1.
2. Write a control word in control register.
3. Write I/O instructions to communicate with peripherals through port A, B, C.

The common applications of 8255 are: 

- Traffic light control
- Generating square wave
- Interfacing with DC motors and stepper motors

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Introduction of Microcomputer System
The [8085 microprocessor](https://www.geeksforgeeks.org/pin-diagram-8085-microprocessor/)
 is an example of a Microcomputer System. A microprocessor system 
contains two types of memory that are EPROM and R/WM, Input and Output 
devices, and the buses that are used to link all the peripherals (memory
 and I/Os) to the MPU.

In 8085, we 16 address lines ranging from A0 to A15 that are used to address the memory. The lower order **address bus A0-A7** is used in the identification of the input and output devices. This microcomputer system has 8 **data lines D0-D7** which are bidirectional and common to all the devices.

It generates four **control signals**:
 Memory Read, Memory Write, I/O Read, and I/O Write, and they are 
connected to different peripheral devices. The MPU communicates with 
only one peripheral at a time by enabling that peripheral through its 
control signal.

For example, sending data to the output device, 
the MPU places the device address (or output port number) on the address
 bus, data on the data bus, and enables the output device by using its 
control signal I/O Write. After that, the output device displays the 
result.

The other peripheral that is not enabled remain in a high
 impedance state called Tri-state. The bus drivers increase the current 
driving capacity of the buses, the decoder decodes the address to 
identify the output port, and the latch holds data output for display. 
These devices are called Interfacing devices. These Interfacing devices 
are semiconductor chips that are needed to connect peripherals to the 
bus system.

The block diagram of a microcomputer system is shown below:

!https://media.geeksforgeeks.org/wp-content/uploads/mcom.png
*** Working of 8085-based Single board microcomputer
Prerequisites – [Pin diagram of 8085 microprocessor](https://www.geeksforgeeks.org/pin-diagram-8085-microprocessor/), [Bus organization of 8085 microprocessor](https://www.geeksforgeeks.org/bus-organization-of-8085-microprocessor/)**Single board microcomputers**
 are the system which have a program called Key Monitor or Key Executive
 permanently stored in memory. This program is stored either in EPROM or
 in ROM, beginning at the memory location 0000H.

Hardware
 is the skeleton of the computer and software is its life. The software 
(programs) makes the computer live; without it the hardware is a dead 
piece of semiconductor material.

When the power is turned on, the 
monitor program comes alive. Initially, the program counter has a random
 address. When the system is reset, the program counter in the 8085 is 
cleared and it holds the address 0000H. The trainer system includes a 
“power on” reset circuit, which reset the system and clears the program 
counter when system is turned on. The MPU places the address 0000H on 
the address bus. The instruction code stored in location 0000H is 
fetched and executed, and the execution continues according to the 
instructions in the monitor program.

**The primary functions of monitor program are as follows:**

1. Reading the Hex keyboard and checking for a key closure. Continuing to check the keyboard until a key is pressed.
2. Displaying the Hex equivalent of the key pressed at the output port, such as the seven segment LEDs.
3. Identifying the key pressed and storing its binary equivalent in memory, if necessary.
4. Transferring the program execution sequence to the user program when the Execute key is pressed.

The
 programmer enters a program in R/W memory in sequential memory 
locations by using the data keys (0 to F) and the function key called 
Enter. When the system is reset, the program counter is cleared, and the
 monitor program begins to check a key closure again. By using the 
keyboard, the programmer enters the first memory address where the user 
program is stored in R/W memory and directs the MPU to execute the 
program by pressing the Run key. The MPU fetches, decodes and executes 
one instruction code at a time and continues to do so until it fetches 
the Halt instruction.

The Key monitor program is a critical 
element in entering, storing and executing a program. Until the Execute 
key is pushed, the monitor program in the EPROM (or [ROM](https://www.geeksforgeeks.org/read-memory-rom-classification-programming/))
 directs all the operations of the MPU. After the Execute key is pushed,
 the user program directs the MPU to perform the functions written in 
the program.
*** Interface 8254 PIT with 8085 microprocessor
Prerequisite – [8254 Control Register and Operating modes](https://www.geeksforgeeks.org/8254-control-word-operating-modes/)**Problem –**
 Write an assembly language program in 8085 microprocessor which 
generates 1 KHz square waveform by using counter 1 as a binary counter 
if clock frequency of 8254 is 2 MHz.

**Assumption –** Assume the port addresses are 80 H, 81 H, 82 H, 83 H for C0(Counter 0), C1(Counter 1), C2(Counter 2), CR(Control Register).

For the above problem, 8254 must work in Mode 3 which is the square wave generator.Count for register is given as clock frequency / square wave frequencycount = 2 MHz / 1 KHz= 2000= (07D0) HNow
 the data is 16 bit so value of RW1 = 1 and RW0 = 1 in Control Register.
 As we want to select C1 (Counter 1) the value of SC1 = 0 and SC0 = 1 in
 Control Register. Value of M2 = 0, M1 = 1 and M2 = 1 for Mode 3 in 
Control Register. For binary counter value of LSB in CR is 0.

Hence the Control Register(CR) is given by,

!https://media.geeksforgeeks.org/wp-content/uploads/8254_Interfacing.png

**Algorithm –**

1. Move the data 76 in A
2. Display the contents of A to port 83
3. Move the data D0 in A
4. Display the contents of A to port 81
5. Move the data 07 in A
6. Display the contents of A to port 81
7. Stop

**Program –**

MEMORY ADDRESSMNEMONICSCOMMENT2000MVI A 76A <- 762002OUT 83CR <- A2004MVI A D0A <- D02006OUT 81C1 <- A2008MVI A 07A <- 07200AOUT 81C1 <- A200CHLTStop

**Explanation–**

1. **MVI A 76** is used to move the content of CR(Control Register) to register A.
2. **OUT 83** is used to assign the value of A to port 83 which is Control Register.
3. **MVI A D0** is used to move the lower byte of data of Counter 1 to register A.
4. **OUT 81** is used to assign the value of A to port 81 which is Counter 1.
5. **MVI A 07** is used to move the higher byte of data of Counter 1 to register A.
6. **OUT 81** is used to assign the value of A to port 81 which is Counter 1.
7. **HLT** is used end the program.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Synchronous Data Transfer in Computer Organization
*** Introduction of Input-Output Processor
The **DMA mode**
 of data transfer reduces CPU’s overhead in handling I/O operations. It 
also allows parallelism in CPU and I/O operations. Such parallelism is 
necessary to avoid wastage of valuable CPU time while handling I/O 
devices whose speeds are much slower as compared to CPU. The concept of 
DMA operation can be extended to relieve the CPU further from getting 
involved with the execution of I/O operations. This gives rises to the 
development of special purpose processor called **Input-Output Processor (IOP) or IO channel**.

The
 Input Output Processor (IOP) is just like a CPU that handles the 
details of I/O operations. It is more equipped with facilities than 
those are available in typical DMA controller. The IOP can fetch and 
execute its own instructions that are specifically designed to 
characterize I/O transfers. In addition to the I/O – related tasks, it 
can perform other processing tasks like arithmetic, logic, branching and
 code translation. The main memory unit takes the pivotal role. It 
communicates with processor by the means of DMA.

**The block diagram –**

!https://media.geeksforgeeks.org/wp-content/uploads/ip1.png

The
 Input Output Processor is a specialized processor which loads and 
stores data into memory along with the execution of I/O instructions. It
 acts as an interface between system and devices. It involves a sequence
 of events to executing I/O operations and then store the results into 
the memory.

**Advantages –**

- The I/O devices can directly access the main memory without the intervention by the processor in I/O processor based systems.
- It is used to address the problems that are arises in Direct memory access method.
***  MPU Communication in Computer Organization
**MPU communicates**
 with the outside world with the help of some external devices which are
 known as Input/Output devices. The MPU accepts the binary data from 
input devices such as keyboard and analog/digital converters and sends 
data to output devices such as printers and LEDs. For performing this 
task, MPU first need to identify the input/output devices.

There
 are two different methods by which I/O devices can be identified: Using
 8-bit address, and Using 16-bit address. These methods are described 
briefly in the following sections:

1. **I/Os with 8-bit addresses –**This is also known as **peripheral-mapped I/O or I/O-mapped-I/O**. In this type of I/O, MPU uses eight address lines to identify an input
or an output devices. This is an 8-bit numbering system for I/Os used in conjunction with the input and output instructions. This is also known
as I/O space that is separate from the memory space which is 16-bit
numbering system. The eight address lines have 2^8 combinations which is total 256 addresses; therefore MPU can identify 256 input devices and
256 output devices with addresses ranging from 00H to FFH.
    
    The input 
    and output devices can be differentiated by using the control lines I/O 
    Read and I/O Write. MPU uses I/O Read control signal for input devices 
    and I/O Write control signal for output devices. The individual 
    addresses of I/O Map is known as I/O port numbers. These I/O devices 
    cannot be connected directly to the data bus or the address bus; all 
    connections must be made through tri-state interfacing devices so they 
    will enabled and connected to the buses only when the MPU chooses to 
    communicate with them.
    
2. **I/Os with 16-bit addresses –**This is also known as **Memory-mapped I/O**. In this type of I/O, MPU uses sixteen address lines to identify an
input or an output devices; an I/O is connected as if it is a memory
register. The MPU uses same control signal (Memory Read and Memory
Write) and instructions as those of memory. In some microprocessors,
such as the Motorola 6800, all I/Os have 16-bit addresses; I/Os and
memory share the same memory map (64K). The steps in communicating with
an I/O device are similar for both 8-bit and 16-bit addresses. The steps are summarized below:
    1. The MPU places an 8-bit address ( or 16-bit address) on the address bus, which is decoded by external decode logic.
    2. The MPU sends a control signal (I/O Read or I/O Write) and enables the I/O device.
    3. Data are transferred using the data bus.
    
*** Memory mapped I|O and Isolated I|O
As a
 CPU needs to communicate with the various memory and input-output 
devices (I/O) as we know data between the processor and these devices 
flow with the help of the system bus. There are three ways in which 
system bus can be allotted to them :

1. Separate set of address, control and data bus to I/O and memory.
2. Have common bus (data and address) for I/O and memory but separate control lines.
3. Have common bus (data, address, and control) for I/O and memory.

In first case it is simple because both have different set of address space and instruction but require more buses.

### Isolated I/O –

Then
 we have Isolated I/O in which we Have common bus(data and address) for 
I/O and memory but separate read and write control lines for I/O. So 
when CPU decode instruction then if data is for I/O then it places the 
address on the address line and set I/O read or write control line on 
due to which data transfer occurs between CPU and I/O. As the address 
space of memory and I/O is isolated and the name is so. The address for 
I/O here is called ports. Here we have different read-write instruction 
for both I/O and memory.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-3-1.png

### Memory Mapped I/O –

In
 this case every bus in common due to which the same set of instructions
 work for memory and I/O. Hence we manipulate I/O same as memory and 
both have same address space, due to which addressing capability of 
memory become less because some part is occupied by the I/O.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-2-1.png

**Differences between memory mapped I/O and isolated I/O –**

[Untitled Database](https://www.notion.so/4813af4bba6948e28dce21dfe3ce2aea?pvs=21)

# Memory mapped I/O and Isolated I/O

- Difficulty Level :
[Basic](https://www.geeksforgeeks.org/basic/)
- Last Updated :
21 Nov, 2019

As a
 CPU needs to communicate with the various memory and input-output 
devices (I/O) as we know data between the processor and these devices 
flow with the help of the system bus. There are three ways in which 
system bus can be allotted to them :

1. Separate set of address, control and data bus to I/O and memory.
2. Have common bus (data and address) for I/O and memory but separate control lines.
3. Have common bus (data, address, and control) for I/O and memory.

In first case it is simple because both have different set of address space and instruction but require more buses.

### Isolated I/O –

Then
 we have Isolated I/O in which we Have common bus(data and address) for 
I/O and memory but separate read and write control lines for I/O. So 
when CPU decode instruction then if data is for I/O then it places the 
address on the address line and set I/O read or write control line on 
due to which data transfer occurs between CPU and I/O. As the address 
space of memory and I/O is isolated and the name is so. The address for 
I/O here is called ports. Here we have different read-write instruction 
for both I/O and memory.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-1-3-1.png

### Memory Mapped I/O –

In
 this case every bus in common due to which the same set of instructions
 work for memory and I/O. Hence we manipulate I/O same as memory and 
both have same address space, due to which addressing capability of 
memory become less because some part is occupied by the I/O.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-2-1.png

**Differences between memory mapped I/O and isolated I/O –**

[Untitled Database](https://www.notion.so/f0711d79ab7f433790f073465269adfe?pvs=21)

*** BUS Arbitration in Computer Organization
**Bus Arbitration**
 refers to the process by which the current bus master accesses and then
 leaves the control of the bus and passes it to another bus requesting 
processor unit. The controller that has access to a bus at an instance 
is known as a **Bus master**.

A
 conflict may arise if the number of DMA controllers or other 
controllers or processors try to access the common bus at the same time,
 but access can be given to only one of those. Only one processor or 
controller can be Bus master at the same point in time. To resolve these
 conflicts, the Bus Arbitration procedure is implemented to coordinate 
the activities of all devices requesting memory transfers. The selection
 of the bus master must take into account the needs of various devices 
by establishing a priority system for gaining access to the bus. The **Bus Arbiter** decides who would become the current bus master.

There are two approaches to bus arbitration: 

1. **Centralized bus arbitration –** A single bus arbiter performs the required arbitration. 
2. **Distributed bus arbitration –** All devices participating in the selection of the next bus master. 

**Methods of Centralized BUS Arbitration –** There are three bus arbitration methods:

**(i) Daisy Chaining method –** It
 is a simple and cheaper method where all the bus masters use the same 
line for making bus requests. The bus grant signal serially propagates 
through each master until it encounters the first one that is requesting
 access to the bus. This master blocks the propagation of the bus grant 
signal, therefore any other requesting module will not receive the grant
 signal and hence cannot access the bus.During any bus cycle, the bus master may be any device – the processor or any DMA controller unit, connected to the bus.

!https://media.geeksforgeeks.org/wp-content/uploads/bus1.png

**Advantages –**

- Simplicity and Scalability.
- The user can add more devices anywhere along the chain, up to a certain maximum value.

**Disadvantages –**

- The value of priority assigned to a device depends on the position of the master bus.
- Propagation delay arises in this method.
- If one device fails then the entire system will stop working.

**(ii) Polling or Rotating Priority method –** In
 this, the controller is used to generate the address for the 
master(unique priority), the number of address lines required depends on
 the number of masters connected in the system. The controller generates
 a sequence of master addresses. When the requesting master recognizes 
its address, it activates the busy line and begins to use the bus.

!https://media.geeksforgeeks.org/wp-content/uploads/bus2.png

**Advantages –**

- This method does not favor any particular device and processor.
- The method is also quite simple.
- If one device fails then the entire system will not stop working.

**Disadvantages –**

- Adding bus masters is difficult as increases the number of address lines of the circuit.

**(iii) Fixed priority or Independent Request method –** In this, each master has a separate pair of bus request and bus grant lines and each pair has a priority assigned to it.

The
 built-in priority decoder within the controller selects the highest 
priority request and asserts the corresponding bus grant signal.

!https://media.geeksforgeeks.org/wp-content/uploads/bus3.png

**Advantages –**

- This method generates a fast response.

**Disadvantages –**

- Hardware cost is high as a large no. of control lines is required.

**Distributed BUS Arbitration :**In
 this, all devices participate in the selection of the next bus master. 
Each device on the bus is assigned a 4bit identification number. The 
priority of the device will be determined by the generated ID.

** Computer Organization | Pipelining
*** Instruction Level Parallelism
**Prerequisite –** [Introduction to Parallel Computing](https://www.geeksforgeeks.org/introduction-to-parallel-computing/)

**Instruction Level Parallelism (ILP)**
 is used to refer to the architecture in which multiple operations can 
be performed parallelly in a particular process, with its own set of 
resources – address space, registers, identifiers, state, program 
counters. It refers to the compiler design techniques and processors 
designed to execute operations, like memory load and store, integer 
addition, float multiplication, in parallel to improve the performance 
of the processors. Examples of architectures that exploit ILP are VLIWs,
 Superscalar Architecture.

ILP processors have the same execution hardware as [RISC processors](https://www.geeksforgeeks.org/computer-organization-risc-and-cisc/).
 The machines without ILP have complex hardware which is hard to 
implement. A typical ILP allows multiple-cycle operations to be 
pipelined.

**Example :**Suppose, 4 operations can
 be carried out in single clock cycle. So there will be 4 functional 
units, each attached to one of the operations, branch unit, and common 
register file in the ILP execution hardware. The sub-operations that can
 be performed by the functional units are Integer ALU, Integer 
Multiplication, Floating Point Operations, Load, Store. Let the 
respective latencies be 1, 2, 3, 2, 1.

Let the sequence of instructions be –

1. y1 = x1*1010
2. y2 = x2*1100
3. z1 = y1+0010
4. z2 = y2+0101
5. t1 = t1+1
6. p = q*1000
7. clr = clr+0010
8. r = r+0001

**Sequential record of execution vs. Instruction-level Parallel record of execution –**

!https://media.geeksforgeeks.org/wp-content/uploads/20201101005203/SequentialvsILP.jpg

Fig. a shows sequential execution of operations.Fig. b shows use of ILP in improving performance of the processor.

The
 ‘nop’s or the ‘no operations’ in the above diagram are used to show 
idle time of processor. Since latency of floating-point operations is 3,
 hence multiplications take 3 cycles and processor has to remain idle 
for that time period. However, in Fig. b processor can utilize those 
nop’s to execute other operations while previous ones are still being 
executed.

While in sequential execution, each cycle has only one 
operation being executed, in processor with ILP, cycle 1 has 4 
operations, cycle 2 has 2 operations. In cycle 3 there is ‘nop’ as the 
next two operations are dependent on first two multiplication 
operations. The sequential processor takes 12 cycles to execute 8 
operations whereas processor with ILP takes only 4 cycles.**Architecture :**Instruction
 Level Parallelism is achieved when multiple operations are performed in
 single cycle, that is done by either executing them simultaneously or 
by utilizing gaps between two successive operations that is created due 
to the latencies.

Now, the decision of when to execute an 
operation depends largely on the compiler rather than hardware. However,
 extent of compiler’s control depends on type of ILP architecture where 
information regarding parallelism given by compiler to hardware via 
program varies. The classification of ILP architectures can be done in 
the following ways –

1. **Sequential Architecture :**Here, program is not expected to explicitly convey any information regarding
parallelism to hardware, like superscalar architecture.
2. **Dependence Architectures :**Here, program explicitly mentions information regarding dependencies between operations like dataflow architecture.
3. **Independence Architecture :**Here, program gives information regarding which operations are independent of each other so that they can be executed instead of the ‘nop’s.

In
 order to apply ILP, compiler and hardware must determine data 
dependencies, independent operations, and scheduling of these 
independent operations, assignment of functional unit, and register to 
store data.
*** Computer Organization and Architecture | Pipelining | Set 1 (Execution, Stages and Throughput)
To 
improve the performance of a CPU we have two options: 1) Improve the 
hardware by introducing faster circuits. 2) Arrange the hardware such 
that more than one operation can be performed at the same time. Since 
there is a limit on the speed of hardware and the cost of faster 
circuits is quite high, we have to adopt the 2nd option.

**Pipelining** is
 a process of arrangement of hardware elements of the CPU such that its 
overall performance is increased. Simultaneous execution of more than 
one instruction takes place in a pipelined processor. Let us see a 
real-life example that works on the concept of pipelined operation. 
Consider a water bottle packaging plant. Let there be 3 stages that a 
bottle should pass through, Inserting the bottle(**I**), Filling water in the bottle(**F**), and Sealing the bottle(**S**).
 Let us consider these stages as stage 1, stage 2 and stage 3 
respectively. Let each stage take 1 minute to complete its operation. 
Now, in a non-pipelined operation, a bottle is first inserted in the 
plant, after 1 minute it is moved to stage 2 where water is filled. Now,
 in stage 1 nothing is happening. Similarly, when the bottle moves to 
stage 3, both stage 1 and stage 2 are idle. But in pipelined operation, 
when the bottle is in stage 2, another bottle can be loaded at stage 1. 
Similarly, when the bottle is in stage 3, there can be one bottle each 
in stage 1 and stage 2. So, after each minute, we get a new bottle at 
the end of stage 3. Hence, the average time taken to manufacture 1 
bottle is:

**Without pipelining** = 9/3 minutes = 3m

```
I F S | | | | | |
| | | I F S | | |
| | | | | | I F S (9 minutes)
```

**With pipelining** = 5/3 minutes = 1.67m

```
I F S | |
| I F S |
| | I F S (5 minutes)
```

Thus, pipelined operation increases the efficiency of a system.

**Design of a basic pipeline**

- In a pipelined processor, a pipeline has two ends, the input end and the
output end. Between these ends, there are multiple stages/segments such
that the output of one stage is connected to the input of the next stage and each stage performs a specific operation.
- Interface
registers are used to hold the intermediate output between two stages.
These interface registers are also called latch or buffer.
- All the stages in the pipeline along with the interface registers are controlled by a common clock.

**Execution in a pipelined processor**
 Execution sequence of instructions in a pipelined processor can be 
visualized using a space-time diagram. For example, consider a processor
 having 4 stages and let there be 2 instructions to be executed. We can 
visualize the execution sequence through the following space-time 
diagrams:

**Non-overlapped execution:**

[Untitled Database](https://www.notion.so/2928c95f71ba4911a9e18cc69b4e6d8c?pvs=21)

Total time = 8 Cycle

**Overlapped execution:**

[Untitled Database](https://www.notion.so/4400b3d3b9514dc5a8fa9d20d2beee7f?pvs=21)

Total time = 5 Cycle **Pipeline Stages** RISC
 processor has 5 stage instruction pipeline to execute all the 
instructions in the RISC instruction set. Following are the 5 stages of 
the RISC pipeline with their respective operations:

- **Stage 1 (Instruction Fetch)** In this stage the CPU reads instructions from the address in the memory whose value is present in the program counter.
- **Stage 2 (Instruction Decode)** In this stage, instruction is decoded and the register file is accessed to get the values from the registers used in the instruction.
- **Stage 3 (Instruction Execute)** In this stage, ALU operations are performed.
- **Stage 4 (Memory Access)** In this stage, memory operands are read and written from/to the memory that is present in the instruction.
- **Stage 5 (Write Back)** In this stage, computed/fetched value is written back to the register present in the instructions.

**Performance of a pipelined processor**
 Consider a ‘k’ segment pipeline with clock cycle time as ‘Tp’. Let 
there be ‘n’ tasks to be completed in the pipelined processor. Now, the 
first instruction is going to take ‘k’ cycles to come out of the 
pipeline but the other ‘n – 1’ instructions will take only ‘1’ cycle 
each, i.e, a total of ‘n – 1’ cycles. So, time taken to execute ‘n’ 
instructions in a pipelined processor:

```
                     ETpipeline = k + n – 1 cycles
                              = (k + n – 1) Tp
```

In the same case, for a non-pipelined processor, the execution time of ‘n’ instructions will be:

```
                    ETnon-pipeline = n * k * Tp
```

So,
 speedup (S) of the pipelined processor over the non-pipelined 
processor, when ‘n’ tasks are executed on the same processor is:

```
    S = Performance of Non-pipelined processor /
        Performance of pipelined processor
```

As the performance of a processor is inversely proportional to the execution time, we have,

```
   S = ETnon-pipeline / ETpipeline
    => S =  [n * k * Tp] / [(k + n – 1) * Tp]
       S = [n * k] / [k + n – 1]
```

When the number of tasks ‘n’ is significantly larger than k, that is, n >> k

```
    S = n * k / n
    S = k
```

where ‘k’ are the number of stages in the pipeline. Also, **Efficiency** = Given speed up / Max speed up = S / Smax We know that Smax = k So, **Efficiency** = S / k **Throughput** = Number of instructions / Total time to complete the instructions So, **Throughput** = n / (k + n – 1) * Tp Note: The cycles per instruction (CPI) value of an ideal pipelined processor is 1 Please see **[Set 2](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-2-dependencies-and-data-hazard/)** for Dependencies and Data Hazard and **[Set 3](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-3-types-and-stalling/)** for Types of pipeline and Stalling.

This article has been contributed by **Saurabh Sharma**.
 Please write comments if you find anything incorrect, or if you want to
 share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Computer Organization and Architecture | Pipelining | Set 2 (Dependencies and Data Hazard)
Please see **[Set 1](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-1-execution-stages-and-throughput/)** for Execution, Stages and Performance (Throughput) and **[Set 3](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-3-types-and-stalling/)** for Types of Pipeline and Stalling. **Dependencies in a pipelined processor**

There are mainly three types of dependencies possible in a pipelined processor. These are :1) Structural Dependency2) Control Dependency3) Data Dependency

These dependencies may introduce stalls in the pipeline.

**Stall :** A stall is a cycle in the pipeline without new input. **Structural dependency**

This
 dependency arises due to the resource conflict in the pipeline. A 
resource conflict is a situation when more than one instruction tries to
 access the same resource in the same cycle. A resource can be a 
register, memory, or ALU.

Example:

[Untitled Database](https://www.notion.so/4873d3ca52ed4e10b1a7f450fc900aef?pvs=21)

In the above scenario, in cycle 4, instructions I1 and I4 are trying to access same resource (Memory) which introduces a resource conflict.To
 avoid this problem, we have to keep the instruction on wait until the 
required resource (memory in our case) becomes available. This wait will
 introduce stalls in the pipeline as shown below:

[Untitled Database](https://www.notion.so/5e051113f12448fea5f271f3c956bfd1?pvs=21)

**Solution for structural dependency**To minimize structural dependency stalls in the pipeline, we use a hardware mechanism called Renaming.**Renaming :**
 According to renaming, we divide the memory into two independent 
modules used to store the instruction and data separately called Code 
memory(CM) and Data memory(DM) respectively. CM will contain all the 
instructions and DM will contain all the operands that are required for 
the instructions.

[Untitled Database](https://www.notion.so/f7bb8d9ce9714f60b5c9586a78eeeff3?pvs=21)

 **Control Dependency (Branch Hazards)**This
 type of dependency occurs during the transfer of control instructions 
such as BRANCH, CALL, JMP, etc. On many instruction architectures, the 
processor will not know the target address of these instructions when it
 needs to insert the new instruction into the pipeline. Due to this, 
unwanted instructions are fed to the pipeline.

Consider the following sequence of instructions in the program:100: I1101: I2 (JMP 250)102: I3..250: BI1

Expected output: I1 -> I2 -> BI1

NOTE: Generally, the target address of the JMP instruction is known after ID stage only.

[Untitled Database](https://www.notion.so/18da402d9261497a90502b8c718c50fd?pvs=21)

Output Sequence: I1 -> I2 -> I3 -> BI1

So, the output sequence is not equal to the expected output, that means the pipeline is not implemented correctly.

To
 correct the above problem we need to stop the Instruction fetch until 
we get target address of branch instruction. This can be implemented by 
introducing delay slot until we get the target address.

[Untitled Database](https://www.notion.so/08566653692b4b258c988a91bbf471cd?pvs=21)

Output Sequence: I1 -> I2 -> Delay (Stall) -> BI1

As
 the delay slot performs no operation, this output sequence is equal to 
the expected output sequence. But this slot introduces stall in the 
pipeline.

**Solution for Control dependency** Branch 
Prediction is the method through which stalls due to control dependency 
can be eliminated. In this at 1st stage prediction is done about which 
branch will be taken.For branch prediction Branch penalty is zero.

**Branch penalty :** The number of stalls introduced during the branch operations in the pipelined processor is known as branch penalty.

**NOTE :**
 As we see that the target address is available after the ID stage, so 
the number of stalls introduced in the pipeline is 1. Suppose, the 
branch target address would have been present after the ALU stage, there
 would have been 2 stalls. Generally, if the target address is present 
after the kth stage, then there will be (k – 1) stalls in the pipeline.

Total number of stalls introduced in the pipeline due to branch instructions = **Branch frequency * Branch Penalty**

**Data Dependency (Data Hazard)**Let us consider an ADD instruction S, such thatS : ADD R1, R2, R3Addresses read by S = I(S) = {R2, R3}Addresses written by S = O(S) = {R1}

Now, we say that instruction S2 depends in instruction S1, when

This condition is called Bernstein condition.

!https://media.geeksforgeeks.org/wp-content/uploads/formula-41.jpg

Three cases exist:

- Flow (data) dependence: O(S1) ∩ I (S2), S1 → S2 and S1 writes after something read by S2
- Anti-dependence: I(S1) ∩ O(S2), S1 → S2 and S1 reads something before S2 overwrites it
- Output dependence: O(S1) ∩ O(S2), S1 → S2 and both write the same memory location.

Example: Let there be two instructions I1 and I2 such that:I1 : ADD R1, R2, R3I2 : SUB R4, R1, R2

When the above instructions are executed in a pipelined processor, then data dependency condition will occur, which means that I2 tries to read the data before I1 writes it, therefore, I2 incorrectly gets the old value from I1.

[Untitled Database](https://www.notion.so/f741f295e286440cb0f757da30f46c95?pvs=21)

To minimize data dependency stalls in the pipeline, **operand forwarding** is used.

**Operand Forwarding :**
 In operand forwarding, we use the interface registers present between 
the stages to hold intermediate output so that dependent instruction can
 access new value from the interface register directly.

Considering the same example:I1 : ADD R1, R2, R3I2 : SUB R4, R1, R2

[Untitled Database](https://www.notion.so/831be1b1ae8e4bc6ac338832ed65cb42?pvs=21)

 **Data Hazards**

Data
 hazards occur when instructions that exhibit data dependence, modify 
data in different stages of a pipeline. Hazard cause delays in the 
pipeline. There are mainly three types of data hazards:

1) RAW (Read after Write) [Flow/True data dependency]2) WAR (Write after Read) [Anti-Data dependency]3) WAW (Write after Write) [Output data dependency]

Let there be two instructions I and J, such that J follow I. Then,

- RAW hazard occurs when instruction J tries to read data before instruction I writes it.Eg:I: R2 <- R1 + R3J: R4 <- R2 + R3
- WAR hazard occurs when instruction J tries to write data before instruction I reads it.Eg:I: R2 <- R1 + R3J: R3 <- R4 + R5
- WAW hazard occurs when instruction J tries to write output before instruction I writes it.Eg:I: R2 <- R1 + R3J: R2 <- R4 + R5

WAR and WAW hazards occur during the out-of-order execution of the instructions.

Sources : [goo.gl/J9KVNt](http://goo.gl/J9KVNt)[https://en.wikipedia.org/wiki/Hazard_(computer_architecture)](https://en.wikipedia.org/wiki/Hazard_(computer_architecture))https://en.wikipedia.org/wiki/Data_dependency
*** Computer Organization and Architecture | Pipelining | Set 3 (Types and Stalling)
Please see **[Set 1](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-1-execution-stages-and-throughput/)** for Execution, Stages and Performance (Throughput) and **[Set 2](https://www.geeksforgeeks.org/computer-organization-and-architecture-pipelining-set-2-dependencies-and-data-hazard/)** for Dependencies and Data Hazard. **Types of pipeline**

- Uniform delay pipelineIn this type of pipeline, all the stages will take same time to complete an operation.In uniform delay pipeline, **Cycle Time (Tp) = Stage Delay**
    
    If buffers are included between the stages then, **Cycle Time (Tp) = Stage Delay + Buffer Delay**
    
- Non-Uniform delay pipelineIn this type of pipeline, different stages take different time to complete an operation.In this type of pipeline, Cycle Time (Tp) = Maximum(Stage Delay)
    
    For example, if there are 4 stages with delays, 1 ns, 2 ns, 3 ns, and 4 ns, then
    
    Tp = Maximum(1 ns, 2 ns, 3 ns, 4 ns) = 4 ns
    
    If buffers are included between the stages,
    
    Tp = Maximum(Stage delay + Buffer delay)
    
    **Example :**
     Consider a 4 segment pipeline with stage delays (2 ns, 8 ns, 3 ns, 10 
    ns). Find the time taken to execute 100 tasks in the above pipeline.**Solution :** As the above pipeline is a non-linear pipeline,Tp = max(2, 8, 3, 10) = 10 nsWe know that ETpipeline = (k + n – 1) Tp = (4 + 100 – 1) 10 ns = 1030 ns
    
    NOTE: MIPS = Million instructions per second
    

**Performance of pipeline with stalls**

```
Speed Up (S) = Performancepipeline / Performancenon-pipeline
=> S = Average Execution Timenon-pipeline / Average Execution Timepipeline
=> S = CPInon-pipeline * Cycle Timenon-pipeline / CPIpipeline * Cycle Timepipeline
```

Ideal CPI of the pipelined processor is ‘1’. But due to stalls, it becomes greater than ‘1’.=>

```
S = CPInon-pipeline * Cycle Timenon-pipeline  / (1 + Number of stalls per Instruction) * Cycle Timepipeline

As Cycle Timenon-pipeline =   Cycle Timepipeline,

Speed Up (S) = CPInon-pipeline / (1 + Number of stalls per instruction)
```

Sources : [goo.gl/J9KVNt](http://goo.gl/J9KVNt)[https://en.wikipedia.org/wiki/Hazard_(computer_architecture)](https://en.wikipedia.org/wiki/Hazard_(computer_architecture))https://en.wikipedia.org/wiki/Data_dependency This article has been contributed by Saurabh Sharma. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
** foot notes
