#+title:      DBMS
#+date:       [2024-12-26 Jo 03:09]
#+filetags:   :dbms:
#+identifier: 20241226T030951


* DBMS
** Basics of DBMS
*** Introduction of DBMS
A database is a collection of inter-related data which helps in the 
efficient retrieval, insertion, and deletion of data from the database 
and organizes the data in the form of tables, views, schemas, reports, 
etc. For Example, a university database organizes the data about 
students, faculty, admin staff, etc. which helps in the efficient 
retrieval, insertion, and deletion of data from it. **DDL**
 is the short name for Data Definition Language, which deals with 
database schemas and descriptions, of how the data should reside in the 
database.

- CREATE: to create a database and its objects like (table, index, views, store procedure, function, and triggers)
- ALTER: alters the structure of the existing database
- DROP: delete objects from the database
- TRUNCATE: remove all records from a table, including all spaces allocated for the records are removed
- COMMENT: add comments to the data dictionary
- RENAME: rename an object

**DML**
 is the short name for Data Manipulation Language which deals with data 
manipulation and includes most common SQL statements such SELECT, 
INSERT, UPDATE, DELETE, etc., and it is used to store, modify, retrieve,
 delete and update data in a database.

- SELECT: retrieve data from a database
- INSERT: insert data into a table
- UPDATE: updates existing data within a table
- DELETE: Delete all records from a database table
- MERGE: UPSERT operation (insert or update)
- CALL: call a PL/SQL or Java subprogram
- EXPLAIN PLAN: interpretation of the data access path
- LOCK TABLE: concurrency Control

**DCL** is
 short for Data Control Language which acts as an access specifier to 
the database.(basically to grant and revoke permissions to users in the 
database

- GRANT: grant permissions to the user for running DML(SELECT, INSERT, DELETE,…) commands on the table
- REVOKE: revoke permissions to the user for running DML(SELECT, INSERT, DELETE,…) command on the specified table

**Database Management System:** The
 software which is used to manage databases is called Database 
Management System (DBMS). For Example, MySQL, Oracle, etc. are popular 
commercial DBMS used in different applications. DBMS allows users the 
following tasks:

- **Data Definition:** It helps in the creation, modification, and removal of definitions that define the organization of data in the database.
- **Data Updation:** It helps in the insertion, modification, and deletion of the actual data in the database.
- **Data Retrieval:** It helps in the retrieval of data from the database which can be used by applications for various purposes.
- **User Administration:** It helps in registering and monitoring users, enforcing data security,
monitoring performance, maintaining data integrity, dealing with
concurrency control, and recovering information corrupted by unexpected
failure.

**Paradigm Shift from File System to DBMS**

****File
 System manages data using files on a hard disk. Users are allowed to 
create, delete, and update the files according to their requirements. 
Let us consider the example of file-based University Management System. 
Data of students is available to their respective Departments, Academics
 Section, Result Section, Accounts Section, Hostel Office, etc. Some of 
the data is common for all sections like Roll No, Name, Father Name, 
Address, and Phone number of students but some data is available to a 
particular section only like Hostel allotment number which is a part of 
the hostel office. Let us discuss the issues with this system:

- **Redundancy of data:** Data is said to be redundant if the same data is copied at many places. If a student wants to change their Phone number, he or has to get it
updated in various sections. Similarly, old records must be deleted from all sections representing that student.
- **Inconsistency of Data:** Data is said to be inconsistent if multiple copies of the same data do not
match each other. If the Phone number is different in Accounts Section
and Academics Section, it will be inconsistent. Inconsistency may be
because of typing errors or not updating all copies of the same data.
- **Difficult Data Access:** A user should know the exact location of the file to access data, so
the process is very cumbersome and tedious. If the user wants to search
the student hostel allotment number of a student from 10000 unsorted
students’ records, how difficult it can be.
- **Unauthorized Access:** File Systems may lead to unauthorized access to data. If a student gets access to a file having his marks, he can change it in an unauthorized
way.
- **No Concurrent Access:** The access of the
same data by multiple users at the same time is known as concurrency.
The file system does not allow concurrency as data can be accessed by
only one user at a time.
- **No Backup and Recovery:** The file system does not incorporate any backup and recovery of data if a file is lost or corrupted.
*** Introduction of 3-Tier Architecture in DBMS
[Database Management System – Introduction | Set 1](https://www.geeksforgeeks.org/introduction-of-dbms-database-management-system-set-1/)

**DBMS 3-tier Architecture**

DBMS 3-tier architecture divides the complete system into three inter-related but independent modules as shown below:

!https://media.geeksforgeeks.org/wp-content/uploads/dbms-3tier.jpg

1. **Physical Level:** At the physical level, the information about the location of database
objects in the data store is kept. Various users of DBMS are unaware of
the locations of these objects.In simple terms,physical level of a
database describes how the data is being stored in secondary storage
devices like disks and tapes and also gives insights on additional
storage details.
2. **Conceptual Level:** At
conceptual level, data is represented in the form of various database
tables. For Example, STUDENT database may contain STUDENT and COURSE
tables which will be visible to users but users are unaware of their
storage.Also referred as logical schema,it describes what kind of data
is to be stored in the database.
3. **External Level:**  An external level specifies a view of the data in terms of conceptual
level tables. Each external level view is used to cater to the needs of a particular category of users. For Example, FACULTY of a university is interested in looking course details of students, STUDENTS are
interested in looking at all details related to academics, accounts,
courses and hostel details as well. So, different views can be generated for different users. The main focus of external level is data
abstraction.

**Data Independence**

Data
 independence means a change of data at one level should not affect 
another level. Two types of data independence are present in this 
architecture:

1. **Physical Data Independence:**
Any change in the physical location of tables and indexes should not
affect the conceptual level or external view of data. This data
independence is easy to achieve and implemented by most of the DBMS.
2. **Conceptual Data Independence:** The data at conceptual level schema and external level schema must be
independent. This means a change in conceptual schema should not affect
external schema. e.g.; Adding or deleting attributes of a table should
not affect the user’s view of the table. But this type of independence
is difficult to achieve as compared to physical data independence
because the changes in conceptual schema are reflected in the user’s
view.

**Phases of database design**

Database
 designing for a real-world application starts from capturing the 
requirements to physical implementation using DBMS software which 
consists of following steps shown below:

!https://media.geeksforgeeks.org/wp-content/uploads/20190506212511/dbms-phases.jpg

**Conceptual Design:** The
 requirements of database are captured using high level conceptual data 
model. For Example, the ER model is used for the conceptual design of 
the database.

**Logical Design:** Logical Design 
represents data in the form of relational model. ER diagram produced in 
the conceptual design phase is used to convert the data into the 
Relational Model.

**Physical Design:** In physical design, data in relational model is implemented using commercial DBMS like Oracle, DB2.

**Advantages of DBMS**

****DBMS helps in efficient organization of data in database which has following advantages over typical file system:

- **Minimized redundancy and data inconsistency:** Data is normalized in DBMS to minimize the redundancy which helps in
keeping data consistent. For Example, student information can be kept at one place in DBMS and accessed by different users.This minimized
redundancy is due to primary key and foreign keys
- **Simplified Data Access:** A user need only name of the relation not exact location to access data, so the process is very simple.
- **Multiple data views:** Different views of same data can be created to cater the needs of
different users. For Example, faculty salary information can be hidden
from student view of data but shown in admin view.
- **Data Security:** Only authorized users are allowed to access the data in DBMS. Also, data can be encrypted by DBMS which makes it secure.
- **Concurrent access to data:** Data can be accessed concurrently by different users at same time in DBMS.
- **Backup and Recovery mechanism:** DBMS backup and recovery mechanism helps to avoid data loss and data inconsistency in case of catastrophic failures.
*** DBMS Architecture 1-level, 2-Level, 3-Level
A 
Database store a lots of critical information to access data quickly and
 securely. Hence it is important to select a correct Architecture for 
efficient data management.

Types of DBMS Architecture:

- 1- Tier Architecture
- 2- Tier Architecture
- 3- Tier Architecture

**1- Tier Architecture:**

In
 One- Tier Architecture the database is directly available to the user, 
the user can directly sit on the DBMS and use it i.e.; the client, 
server and the Database all present on the same machine. For Example- To
 learn SQL we setup SQL server and the database on the local system. 
This enable us to directly interact with the relational database and 
execute operation. Industry won’t use this architecture they logically 
go for 2-Tier and 3-Tier Architecture.

**Two-tier architecture:** The two-tier architecture is similar to a basic **client-server**
 model. The application at the client end directly communicates with the
 database at the server-side. APIs like ODBC, JDBC are used for this 
interaction. The server side is responsible for providing query 
processing and transaction management functionalities. On the 
client-side, the user interfaces and application programs are run. The 
application on the client-side establishes a connection with the 
server-side in order to communicate with the DBMS. An advantage of 
this type is that maintenance and understanding are easier, compatible 
with existing systems. However, this model gives poor performance when 
there are a large number of users.

!https://media.geeksforgeeks.org/wp-content/uploads/33-3.png

**Three Tier architecture:** In
 this type, there is another layer between the client and the server. 
The client does not directly communicate with the server. Instead, it 
interacts with an application server which further communicates with the
 database system and then the query processing and transaction 
management takes place. This intermediate layer acts as a medium for the
 exchange of partially processed data between server and client. This 
type of architecture is used in the case of large web applications. **Advantages:** 

- **Enhanced scalability** due to distributed deployment of application servers. Now, individual connections need not be made between client and server.
- **Data Integrity** is maintained. Since there is a middle layer between client and server, data corruption can be avoided/removed.
- **Security** is improved. This type of model prevents direct interaction of the
client with the server thereby reducing access to unauthorized data.

**Disadvantages**: Increased
 complexity of implementation and communication. It becomes difficult 
for this sort of interaction to take place due to the presence of middle
 layers.

**Three-TierArchitecture**

!https://media.geeksforgeeks.org/wp-content/uploads/34-1.png
*** Need for DBMS
A **D**ata **B**ase **M**anagement **S**ystem is a system software for easy, efficient and reliable data processing and management. It can be used for:

- Creation of a database.
- Retrieval of information from the database.
- Updating the database.
- Managing a database.

It
 provides us with the many functionalities and is more advantageous than
 the traditional file system in many ways listed below:

**1) Processing Queries and Object Management**:In
 traditional file systems, we cannot store data in the form of objects. 
In practical-world applications, data is stored in objects and not 
files. So in a file system, some application software maps the data 
stored in files to objects so that can be used further.We can 
directly store data in the form of objects in a database management 
system. Application level code needs to be written to handle, store and 
scan through the data in a file system whereas a DBMS gives us the 
ability to query the database.

**2) Controlling redundancy and inconsistency:**Redundancy
 refers to repeated instances of the same data. A database system 
provides redundancy control whereas in a file system, same data may be 
stored multiple times. For example, if a student is studying two 
different educational programs in the same college, say ,Engineering and
 History, then his information such as the phone number and address may 
be stored multiple times, once in Engineering dept and the other in 
History dept. Therefore, it increases time taken to access and store 
data. This may also lead to inconsistent data states in both places. A 
DBMS uses **data normalization** to avoid redundancy and duplicates.

**3) Efficient memory management and indexing:**DBMS
 makes complex memory management easy to handle. In file systems, files 
are indexed in place of objects so query operations require entire file 
scans whereas in a DBMS , object indexing takes place efficiently 
through database schema based on any attribute of the data or a 
data-property. This helps in fast retrieval of data based on the indexed
 attribute.

**4) Concurrency control and transaction management:**Several
 applications allow user to simultaneously access data. This may lead to
 inconsistency in data in case files are used. Consider two withdrawal 
transactions X and Y in which an amount of 100 and 200 is withdrawn from
 an account A initially containing 1000. Now since these transactions 
are taking place simultaneously, different transactions may update the 
account differently. X reads 1000, debits 100, updates the account A to 
900, whereas Y also reads 1000, debits 200, updates A to 800. In both 
cases account A has wrong information. This results in data 
inconsistency. A DBMS provides mechanisms to deal with this kind of data
 inconsistency while allowing users to access data concurrently. A DBMS 
implements [ACID](https://www.geeksforgeeks.org/acid-properties-in-dbms/)(atomicity, durability, isolation,consistency) properties to ensure efficient transaction management without data corruption.

**5) Access Control and ease in accessing data:**A
 DBMS can grant access to various users and determine which part and how
 much of the data can they access from the database thus removing 
redundancy. Otherwise in file system, separate files have to be created 
for each user containing the amount of data that they can access. 
Moreover, if a user has to extract specific data, then he needs a 
code/application to process that task in case of file system, e.g. 
Suppose a manager needs a list of all employees having salary greater 
than X. Then we need to write business logic for the same in case data 
is stored in files. In case of DBMS, it provides easy access of data 
through queries, (e.g., **SELECT** queries) and whole logic need not be rewritten. Users can specify exactly what they want to extract out of the data.

**6) Integrity constraints:**
 Data stored in databases must satisfy integrity constraints. For 
example, Consider a database schema consisting of the various 
educational programs offered by a university such 
as(B.Tech/M.Tech/B.Sc/M.Sc/BCA/MCA) etc. Then we have a schema of 
students enrolled in these programs. A DBMS ensures that it is only out 
of one of the programs offered schema , that the student is enrolled in,
 i.e. Not anything out of the blue. Hence, database integrity is 
preserved.

Apart from the above mentioned features a database management also provides the following:

- **Multiple User Interface**
- **Data scalability, expandability and flexibility**: We can change schema of the database, all schema will be updated according to it.
- Overall the time for developing an application is reduced.
- **Security:** Simplifies data storage as it is possible to assign security permissions allowing restricted access to data.
*** Advantages of DBMS over File system
**File System:** A
 File Management system is a DBMS that allows access to single files or 
tables at a time. In a File System, data is directly stored in set of 
files. It contains flat files that have no relation to other files (when
 only one table is stored in single file, then this file is known as 
flat file).

**DBMS:** A
 Database Management System (DBMS) is a application software that allows
 users to efficiently define, create, maintain and share databases. 
Defining a database involves specifying the data types, structures and 
constraints of the data to be stored in the database. Creating a 
database involves storing the data on some storage medium that is 
controlled by DBMS. Maintaining a database involves updating the 
database whenever required to evolve and reflect changes in the 
miniworld and also generating reports for each change. Sharing a 
database involves allowing multiple users to access the database. DBMS 
also serves as an interface between the database and end users or 
application programs. It provides control access to the data and ensures
 that data is consistent and correct by defining rules on them.

An
 application program accesses the database by sending queries or 
requests for data to the DBMS. A query causes some data to be retrieved 
from database.

### Advantages of DBMS over File system:

- **Data redundancy and inconsistency:** Redundancy is the concept of repetition of data i.e. each data may have more than a single copy. The file system cannot control the redundancy of data as
each user defines and maintains the needed files for a specific
application to run. There may be a possibility that two users are
maintaining the data of the same file for different applications. Hence
changes made by one user do not reflect in files used by second users,
which leads to inconsistency of data. Whereas DBMS controls redundancy
by maintaining a single repository of data that is defined once and is
accessed by many users. As there is no or less redundancy, data remains
consistent.
- **Data sharing:** The file system does
not allow sharing of data or sharing is too complex. Whereas in DBMS,
data can be shared easily due to a centralized system.
- **Data concurrency:** Concurrent access to data means more than one user is accessing the same data at
the same time. Anomalies occur when changes made by one user get lost
because of changes made by another user. The file system does not
provide any procedure to stop anomalies. Whereas DBMS provides a locking system to stop anomalies to occur.
- **Data searching:** For every search operation performed on the file system, a different
application program has to be written. While DBMS provides inbuilt
searching operations. The user only has to write a small query to
retrieve data from the database.
- **Data integrity:** There may be cases when some constraints need to be applied to the data
before inserting it into the database. The file system does not provide
any procedure to check these constraints automatically. Whereas DBMS
maintains data integrity by enforcing user-defined constraints on data
by itself.
- **System crashing:** In some cases,
systems might have crashed due to various reasons. It is a bane in the
case of file systems because once the system crashes, there will be no
recovery of the data that’s been lost. A DBMS will have the recovery
manager which retrieves the data making it another advantage over file
systems.
- **Data security:** A file system provides a password mechanism to protect the database but how long can the
password be protected? No one can guarantee that. This doesn’t happen in the case of DBMS. DBMS has specialized features that help provide
shielding to its data.
- **Backup:** It creates a backup subsystem to restore the data if required.
- **Interfaces**: It provides different multiple user interfaces like graphical user interface and application program interface.
- **Easy Maintenance**: It is easily maintainable due to its centralized nature.

DBMS
 is continuously evolving from time to time. It is a power tool for data
 storage and protection. In the coming years, we will get to witness an 
AI-based DBMS to retrieve databases of ancient eras.
*** Economic factors (Choice of DBMS)
Basically, the choice of a DBMS is governed by a number of factors such as: 

1. Technical 
2. Economics 
3. Politics of Organization 

Here
 we will basically discuss Non-technical factors which include the 
financial status and the support organization of the vendor. In this 
article, we will concentrate on discussing the economic and 
organizational factors that affect the choice of DBMS. Following cost 
are considered while choosing a DBMS these are as follows:

- **Software acquisition cost –** This is basically the up-front cost of software or buying cost of software
including language options, different types of interfaces. The correct
DBMS version for a specific OS must be selected. Basically, the
Development tools, design tools, and additional language support are not included in basic pricing.
- **Maintenance cost –** This is basically the recurring cost of receiving standard maintenance
service from the vendor and to keep the DBMS version up-to-date.
- **Hardware acquisition cost –** We may need hardware components such as memory, disk drives, controllers,
archival storage etc cost of all these also we need to consider while
choosing a DBMS.
- **Personal cost –** Acquisition of DBMS software for the first time by an organization is often
accompanied by a reorganization of the data processing department. The
position of DBA and staff exist in most companies and that have adopted
DBMS.
- **Training cost –** DBMS are
often complex systems personal must often be trained to use and program
for DBMS. Training is required at all levels which include programming,
application development and database administration.
- **Operating cost –** Cost of operating database also needs to be considered while choosing DBMS.
*** Data Abstraction and Data Independence
Database
 systems comprise complex data-structures. In order to make the system 
efficient in terms of retrieval of data, and reduce complexity in terms 
of usability of users, developers use abstraction i.e. hide irrelevant 
details from the users. This approach simplifies database design.

There are mainly **3** levels of data abstraction:

**Physical**:
 This is the lowest level of data abstraction. It tells us how the data 
is actually stored in memory. The access methods like sequential or 
random access and file organization methods like B+ trees, hashing used 
for the same. Usability, size of memory, and the number of times the 
records are factors that we need to know while designing the database. Suppose
 we need to store the details of an employee. Blocks of storage and the 
amount of memory used for these purposes are kept hidden from the user.

**Logical**:
 This level comprises the information that is actually stored in the 
database in the form of tables. It also stores the relationship among 
the data entities in relatively simple structures. At this level, the 
information available to the user at the view level is unknown. We can store the various attributes of an employee and relationships, e.g. with the manager can also be stored.

**View**:
 This is the highest level of abstraction. Only a part of the actual 
database is viewed by the users. This level exists to ease the 
accessibility of the database by an individual user. Users view data in 
the form of rows and columns. Tables and relations are used to store 
data. Multiple views of the same database may exist. Users can just view
 the data and interact with the database, storage and implementation 
details are hidden from them.

!https://media.geeksforgeeks.org/wp-content/uploads/13-1.png

The
 main purpose of data abstraction is to achieve data independence in 
order to save time and cost required when the database is modified or 
altered. We have namely two levels of data independence arising from these levels of abstraction :

**Physical level data independence**:
 It refers to the characteristic of being able to modify the physical 
schema without any alterations to the conceptual or logical schema, done
 for optimization purposes, e.g., Conceptual structure of the database 
would not be affected by any change in storage size of the database 
system server. Changing from sequential to random access files is one 
such example. These alterations or modifications to the physical 
structure may include: 

- Utilizing new storage devices.
- Modifying data structures used for storage.
- Altering indexes or using alternative file organization techniques etc.

**Logical level data independence:**
 It refers characteristic of being able to modify the logical schema 
without affecting the external schema or application program. The user 
view of the data would not be affected by any changes to the conceptual 
view of the data. These changes may include insertion or deletion of 
attributes, altering table structures entities or relationships to the 
logical schema, etc.
*** Introduction of ER Model
ER Model is used to model the logical view of the system from data perspective which consists of these components:

**Entity, Entity Type, Entity Set –**

An
 Entity may be an object with a physical existence – a particular 
person, car, house, or employee – or it may be an object with a 
conceptual existence – a company, a job, or a university course.

An
 Entity is an object of Entity Type and set of all entities is called as
 entity set. e.g.; E1 is an entity having Entity Type Student and set of
 all students is called Entity Set. In ER diagram, Entity Type is 
represented as:

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model.png

**Attribute(s):** Attributes are the **properties which define the entity type**.
 For example, Roll_No, Name, DOB, Age, Address, Mobile_No are the 
attributes which defines entity type Student. In ER diagram, attribute 
is represented by an oval.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-2.png

**1. Key Attribute –** The attribute which **uniquely identifies each entity**
 in the entity set is called key attribute.For example, Roll_No will be 
unique for each student. In ER diagram, key attribute is represented by 
an oval with underlying lines.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-3.png

**2. Composite Attribute –** An attribute **composed of many other attribute**
 is called as composite attribute. For example, Address attribute of 
student Entity type consists of Street, City, State, and Country. In ER 
diagram, composite attribute is represented by an oval comprising of 
ovals.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-4.png

**3. Multivalued Attribute –** An attribute consisting **more than one value**
 for a given entity. For example, Phone_No (can be more than one for a 
given student). In ER diagram, multivalued attribute is represented by 
double oval.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-5.png

**4. Derived Attribute –** An attribute which can be **derived from other attributes**
 of the entity type is known as derived attribute. e.g.; Age (can be 
derived from DOB). In ER diagram, derived attribute is represented by 
dashed oval.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-6.png

The complete entity type **Student** with its attributes can be represented as:

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-7.png

**Relationship Type and Relationship Set:** A relationship type represents the **association between entity types**.
 For example,‘Enrolled in’ is a relationship type that exists between 
entity type Student and Course. In ER diagram, relationship type is 
represented by a diamond and connecting the entities with lines.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-8.png

A
 set of relationships of same type is known as relationship set. The 
following relationship set depicts S1 is enrolled in C2, S2 is enrolled 
in C1 and S3 is enrolled in C3.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-S-ystem-ER-Model-9.png

**Degree of a relationship set:** The number of different entity sets **participating in a relationship** set is called as degree of a relationship set.

**1. Unary Relationship –** When there is **only ONE entity set participating in a relation**, the relationship is called as unary relationship. For example, one person is married to only one person.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-10.png

**2. Binary Relationship –** When there are **TWO entities set participating in a relation**, the relationship is called as binary relationship.For example, Student is enrolled in Course.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-8.png

**3. n-ary Relationship –** When there are n entities set participating in a relation, the relationship is called as n-ary relationship.

**Cardinality:**

The **number of times an entity of an entity set participates in a relationship** set is known as cardinality. Cardinality can be of different types:

**1. One to one –** When each entity in each entity set can take part **only once in the relationship**,
 the cardinality is one to one. Let us assume that a male can marry to 
one female and a female can marry to one male. So the relationship will 
be one to one.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-12.png

Using Sets, it can be represented as:

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-13.png

**2. Many to one –** When entities in one entity set **can
 take part only once in the relationship set and entities in other 
entity set can take part more than once in the relationship set,**
 cardinality is many to one. Let us assume that a student can take only 
one course but one course can be taken by many students. So the 
cardinality will be n to 1. It means that for one course there can be n 
students but for one student, there will be only one course.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-14.png

Using Sets, it can be represented as:

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-15.png

In this case, each student is taking only 1 course but 1 course has been taken by many students.

**3. Many to many –** When entities in all entity sets can **take part more than once in the relationship**
 cardinality is many to many. Let us assume that a student can take more
 than one course and one course can be taken by many students. So the 
relationship will be many to many.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-16.png

Using sets, it can be represented as:

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-17.png

In
 this example, student S1 is enrolled in C1 and C3 and Course C3 is 
enrolled by S1, S3 and S4. So it is many to many relationships.

**Participation Constraint:** Participation Constraint is applied on the entity participating in the relationship set.

**1. Total Participation –** Each entity in the entity set **must participate**
 in the relationship. If each student must enroll in a course, the 
participation of student will be total. Total participation is shown by 
double line in ER diagram.

**2. Partial Participation –** The entity in the entity set **may or may NOT participat**e in the relationship. If some courses are not enrolled by any of the student, the participation of course will be partial.

The
 diagram depicts the ‘Enrolled in’ relationship set with Student Entity 
set having total participation and Course Entity set having partial 
participation.

!https://media.geeksforgeeks.org/wp-content/uploads/Database-Management-System-ER-Model-18.png

Using set, it can be represented as,

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/33333-1.png

Every
 student in Student Entity set is participating in relationship but 
there exists a course C4 which is not taking part in the relationship.

**Weak Entity Type and Identifying Relationship:** As
 discussed before, an entity type has a key attribute which uniquely 
identifies each entity in the entity set. But there exists **some entity type for which key attribute can’t be defined**. These are called Weak Entity type.

For
 example, A company may store the information of dependents (Parents, 
Children, Spouse) of an Employee. But the dependents don’t have 
existence without the employee. So Dependent will be weak entity type 
and Employee will be Identifying Entity type for Dependent.

A 
weak entity type is represented by a double rectangle. The participation
 of weak entity type is always total. The relationship between weak 
entity type and its identifying strong entity type is called identifying
 relationship and it is represented by double diamond.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20210219121849/12310.png
*** Recursive Relationships in ER diagrams
Prerequisite – [ER Model](https://www.geeksforgeeks.org/database-management-system-er-model/) A relationship between two entities of a similar entity type is called a **recursive**
 relationship. Here the same entity type participates more than once in a
 relationship type with a different role for each instance. In other 
words, a relationship has always been between occurrences in two 
different entities. However, the same entity can participate in the 
relationship. This is termed a **recursive** relationship.

!https://media.geeksforgeeks.org/wp-content/uploads/DBMS.jpg

**Example –** Let
 us suppose that we have an employee table. A manager supervises a 
subordinate. Every employee can have a supervisor except the CEO and 
there can be at most one boss for each employee. One employee may be the
 boss of more than one employee. Let’s suppose that REPORTS_TO is a 
recursive relationship on the Employee entity type where each Employee 
plays two roles.

1. Supervisor
2. Subordinate

!https://media.geeksforgeeks.org/wp-content/uploads/dbms2.jpg

Supervisors and subordinates are called **“Role Names”**. Here the degree of the REPORTS_TO relationship is 1 i.e. a unary relationship. 

- The minimum cardinality of the Supervisor entity is ZERO since the lowest level employee may not be a manager for anyone.
- The maximum cardinality of the Supervisor entity is N since an employee can manage many employees.
- Similarly, the Subordinate entity has a minimum cardinality of ZERO to account for the case where CEO can never be a subordinate.
- Its maximum cardinality is ONE since a subordinate employee can have at most one supervisor.

**Note –**
 Here none of the participants have total participation since both 
minimum cardinalities are Zero. Hence, the relationships are connected 
by a single line instead of a double line in the ER diagram.

To 
implement a recursive relationship, a foreign key of the employee’s 
manager number would be held in each employee record. A Sample table 
would look something like this:-

```
Emp_entity( Emp_no,Emp_Fname, Emp_Lname, Emp_DOB, Emp_NI_Number, Manager_no);

Manager no - (this is the employee no of the employee's manager)
```
*** Minimization of ER Diagrams
Entity-Relationship (ER) Diagram is a diagrammatic representation of data in databases, it shows how data is related.

Note: This article is for those who already know what is ER diagram and how to draw ER diagram.

**1) When there are Many to One** cardinalities **in** the **ER diagram.** For example, a student can be enrolled only in one course, but a course can be enrolled by many students.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/many2one-300x115.png

For Student(SID, Name), SID is the primary key.  For Course(CID, C_name ), CID is the primary key 

```
           Student                           Course
        (SID   Name)                    ( CID   C_name )
        --------------                  -----------------
          1      A                        c1      Z
          2      B                        c2      Y
          3      C                        c3      X
          4      D

              Enroll
            (SID   CID)
             ----------
             1       C1
             2       C1
             3       c3
             4       C2
```

Now the question is, what should be the 
primary key for Enroll? Should it be SID or CID or both combined into 
one. We can’t have CID as the primary key because a CID can have 
multiple SIDs. (SID, CID) can distinguish table uniquely, but it is not 
minimum.  So SID is the primary key for the relation enrollment.

For the above ER diagram, we considered three tables in the database 

```
Student
Enroll
Course
```

But we can combine Student and Enroll table renamed as Student_enroll. 

```
                 Student_Enroll
                ( SID   Name   CID )
                ---------------------
                  1      A      c1
                  2      B      c1
                  3      C      c3
                  4      D      c2
```

Student and enroll tables are merged now.

So require a minimum of two DBMS tables for Student_enroll and Course.

**Note:** In One to Many relationships we can have a minimum of two tables.

**2. When there** **are Many to Many cardinalities in ER Diagram.** Let us consider the above example with the change that now a student can enroll in more than 1 course.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/many2many-300x115.png

```
  Student                            Course
( SID   Name)                    ( CID   C_name )
--------------                  -----------------
   1      A                        c1      Z
   2      B                        c2      Y
   3      C                        c3      X
   4      D

              Enroll
           ( SID   CID )
             ----------
             1       C1
             1       C2
             2       C1
             2       C2
             3       c3
             4       C2
```

Now, the same question arises. What is 
the primary key of Enroll relation? If we carefully analyze, the primary
 key for Enroll table is ( SID, CID ).

But in this case, we can’t
 merge Enroll table with any one of the Student and Course. If we try to
 merge Enroll with any one of the Student and Course it will create 
redundant data.

**Note:** Minimum of three tables are required in the Many to Many relationships.

**3. One to One Relationship**

There are two possibilities **A) If we have One to One relationship and we have total participation at at-least one end.**

For example, consider the below ER diagram.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/one2oneT1-300x115.png

A1 and B1 are primary keys of E1 and E2 respectively.

In the above diagram, we have total participation at the E1 end.

Only a single table is required in this case having the primary key of E1 as its primary key.

Since
 E1 is in total participation, each entry in E1 is related to only one 
entry in E2, but not all entries in E2 are related to an entry in E1.

The
 primary key of E1 should be allowed as the primary key of the reduced 
table since if the primary key of E2 is used, it might have null values 
for many of its entries in the reduced table.

**Note:** Only 1 table required.

**B) One to One relationship with no total participation.**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/one2one-300x115.png

A1 and B1 are primary keys of E1 and E2 respectively.

The
 primary key of R can be A1 or B1, but we can’t still combine all three 
tables into one. if we do so, some entries in the combined table may 
have NULL entries. So the idea of merging all three tables into one is 
not good.

But we can merge R into E1 or E2.  So a minimum of 2 tables is required.
*** Enhanced ER Model
**Prerequisite –** [Introduction of ER Model](https://www.geeksforgeeks.org/database-management-system-er-model/) Today
 the complexity of the data is increasing so it becomes more and more 
difficult to use the traditional ER model for database modeling. To 
reduce this complexity of modeling we have to make improvements or 
enhancements to the existing ER model to make it able to handle the 
complex application in a better way. Enhanced entity-relationship 
diagrams are advanced database diagrams very similar to regular ER 
diagrams which represent requirements and complexities of complex 
databases. It is a diagrammatic technique for displaying the Sub 
Class and Super Class; Specialization and Generalization; Union or 
Category; Aggregation etc.

**Generalization and Specialization –** These
 are very common relationships found in real entities. However, this 
kind of relationship was added later as an enhanced extension to the 
classical ER model. **Specialized** classes are often called **subclass** while a **generalized class** is called a superclass, probably inspired by object-oriented programming. A sub-class is best understood by **“IS-A analysis”**. Following statements hopefully makes some sense to your mind “Technician IS-A Employee”, “Laptop IS-A Computer”.

An
 entity is a specialized type/class of another entity. For example, a 
Technician is a special Employee in a university system Faculty is a 
special class of Employees. We call this phenomenon 
generalization/specialization. In the example here Employee is a 
generalized entity class while the Technician and Faculty are 
specialized classes of Employee.

**Example –** This example instance of **“sub-class”**
 relationships. Here we have four sets of employees: Secretary, 
Technician, and Engineer. The employee is super-class of the rest three 
sets of individual sub-class is a subset of Employee set.

!https://media.geeksforgeeks.org/wp-content/uploads/Enhanced-ER-Model-Diagram.png

- An entity belonging to a sub-class is related to some super-class entity.
For instance emp, no 1001 is a secretary, and his typing speed is 68.
Emp no 1009 is an engineer (sub-class) and her trade is “Electrical”, so forth.
- Sub-class entity “inherits” all attributes of
super-class; for example, employee 1001 will have attributes eno, name,
salary, and typing speed.

**Enhanced ER model of above example –** 

!https://media.geeksforgeeks.org/wp-content/uploads/Enhanced-ER-Model-Diagram-1.png

**Constraints –** There are two types of constraints on the “Sub-class” relationship. 

1. **Total or Partial –** A sub-classing relationship is total if every super-class entity is to
be associated with some sub-class entity, otherwise partial. Sub-class
“job type based employee category” is partial sub-classing – not
necessary every employee is one of (secretary, engineer, and
technician), i.e. union of these three types is a proper subset of all
employees. Whereas other sub-classing “Salaried Employee AND Hourly
Employee” is total; the union of entities from sub-classes is equal to
the total employee set, i.e. every employee necessarily has to be one of them.
2. **Overlapped or Disjoint –** If an entity
from super-set can be related (can occur) in multiple sub-class sets,
then it is overlapped sub-classing, otherwise disjoint. Both the
examples: job-type based and salaries/hourly employee sub-classing are
disjoint.

**Note –** These constraints are 
independent of each other: can be “overlapped and total or partial” or 
“disjoint and total or partial”. Also, sub-classing has transitive 
properties.

**Multiple Inheritance (sub-class of multiple** superclasses**) –** An
 entity can be a sub-class of multiple entity types; such entities are 
sub-class of multiple entities and have multiple super-classes; Teaching
 Assistant can subclass of Employee and Student both. A faculty in a 
university system can be a subclass of Employee and Alumnus. In multiple
 inheritances, attributes of sub-class are the union of attributes of 
all super-classes.

**Union –** 

- Set of Library Members is **UNION** of Faculty, Student, and Staff. A union relationship indicates either
type; for example, a library member is either Faculty or Staff or
Student.
- Below are two examples that show how **UNION** can be depicted in ERD – Vehicle Owner is UNION of PERSON and Company, and RTO Registered Vehicle is UNION of Car and Truck.

!https://media.geeksforgeeks.org/wp-content/uploads/Enhanced-ER-Model-Diagram-2.png

You
 might see some confusion in Sub-class and UNION; consider an example in
 above figure Vehicle is super-class of CAR and Truck; this is very much
 the correct example of the subclass as well but here use it differently
 we are saying RTO Registered vehicle is UNION of Car and Vehicle, they 
do not inherit any attribute of Vehicle, attributes of car and truck are
 altogether independent set, where is in sub-classing situation car and 
truck would be inheriting the attribute of vehicle class.

**References –** [comet.lehman.cuny.edu](http://comet.lehman.cuny.edu/jung/cmp420758/chapter8.pdf) [cs.toronto.edu](http://www.cs.toronto.edu/~jm/2507S/Notes04/EER.pdf)
*** Mapping from ER Model to Relational Model
https://www.geeksforgeeks.org/mapping-from-er-model-to-relational-model/?ref=lbp
** Relational model (relational algebra, tuple calculus)
*** Relational Model in DBMS
Relational
 Model was proposed by E.F. Codd to model data in the form of relations 
or tables. After designing the conceptual model of Database using ER 
diagram, we need to convert the conceptual model in the relational model
 which can be implemented using any RDBMS languages like Oracle SQL, 
MySQL etc. So we will see what Relational Model is.

**What is Relational Model?**

Relational
 Model represents how data is stored in Relational Databases.  A 
relational database stores data in the form of relations (tables). 
Consider a relation STUDENT with attributes ROLL_NO, NAME, ADDRESS, 
PHONE and AGE shown in Table 1.

**STUDENT**

[Untitled Database](https://www.notion.so/b19275e1012441a6894a8c6c5e75a84d?pvs=21)

**IMPORTANT TERMINOLOGIES**

- **Attribute:** Attributes are the properties that define a relation. e.g.; **ROLL_NO**, **NAME**
- **Relation Schema:** A relation schema represents name of the relation with its attributes.
e.g.; STUDENT (ROLL_NO, NAME, ADDRESS, PHONE and AGE) is relation schema for STUDENT. If a schema has more than 1 relation, it is called
Relational Schema.
- **Tuple:** Each row in the relation is known as tuple. The above relation contains 4 tuples, one of which is shown as:
- **Relation Instance:** The set of tuples of a relation at a particular instance of time is
called as relation instance. Table 1 shows the relation instance of
STUDENT at a particular time. It can change whenever there is insertion, deletion or updation in the database.
- **Degree:** The number of attributes in the relation is known as degree of the relation. The **STUDENT** relation defined above has degree 5.
- **Cardinality:** The number of tuples in a relation is known as cardinality. The **STUDENT** relation defined above has cardinality 4.
- **Column:** Column represents the set of values for a particular attribute. The column **ROLL_NO** is extracted from relation STUDENT.

**ROLL_NO**

---

1

---

2

---

3

---

4

---

- **NULL Values:** The value which is not known or unavailable is called NULL value. It is represented by blank space. e.g.; PHONE of STUDENT having ROLL_NO 4 is
NULL.

**Constraints in Relational Model**

While
 designing Relational Model, we define some conditions which must hold 
for data present in database are called Constraints. These constraints 
are checked before performing any operation (insertion, deletion and 
updation) in database. If there is a violation in any of constrains, 
operation will fail.

**Domain Constraints:** These 
are attribute level constraints. An attribute can only take values which
 lie inside the domain range. e.g,; If a constrains AGE>0 is applied 
on STUDENT relation, inserting negative value of AGE will result in 
failure.

**Key Integrity:** Every relation in the 
database should have atleast one set of attributes which defines a tuple
 uniquely. Those set of attributes is called key. e.g.; ROLL_NO in 
STUDENT is a key. No two students can have same roll number. So a key 
has two properties:

- It should be unique for all tuples.
- It can’t have NULL values.

**Referential Integrity:**
 When one attribute of a relation can only take values from other 
attribute of same relation or any other relation, it is called 
referential integrity. Let us suppose we have 2 relations

**STUDENT**

[Untitled Database](https://www.notion.so/068b6315e13c43229ba18edc40aab9d1?pvs=21)

**BRANCH**

[Untitled Database](https://www.notion.so/24d1ccc5db744ef9a4101923624aed8d?pvs=21)

BRANCH_CODE
 of STUDENT can only take the values which are present in BRANCH_CODE of
 BRANCH which is called referential integrity constraint. The relation 
which is referencing to other relation is called REFERENCING RELATION 
(STUDENT in this case) and the relation to which other relations refer 
is called REFERENCED RELATION (BRANCH in this case). 

**ANOMALIES**

An
 anomaly is an irregularity, or something which deviates from the 
expected or normal state. When designing databases, we identify three 
types of anomalies: Insert, Update and Delete.

**Insertion Anomaly in Referencing Relation:**

We
 can’t insert a row in REFERENCING RELATION if referencing attribute’s 
value is not present in referenced attribute value. e.g.; Insertion of a
 student with BRANCH_CODE ‘ME’ in STUDENT relation will result in error 
because ‘ME’ is not present in BRANCH_CODE of BRANCH.

**Deletion/ Updation Anomaly in Referenced Relation:**

We
 can’t delete or update a row from REFERENCED RELATION if value of 
REFERENCED ATTRIBUTE is used in value of REFERENCING ATTRIBUTE. e.g; if 
we try to delete tuple from BRANCH having BRANCH_CODE ‘CS’, it will 
result in error because ‘CS’ is referenced by BRANCH_CODE of STUDENT, 
but if we try to delete the row from BRANCH with BRANCH_CODE CV, it will
 be deleted as the value is not been used by referencing relation. It 
can be handled by following method:

**ON DELETE CASCADE:**
 It will delete the tuples from REFERENCING RELATION if  value used by 
REFERENCING ATTRIBUTE is deleted from REFERENCED RELATION. e.g;, if we 
delete a row from BRANCH with BRANCH_CODE ‘CS’, the rows in STUDENT 
relation with BRANCH_CODE CS (ROLL_NO 1 and 2 in this case) will be 
deleted.

**ON UPDATE CASCADE:** It will update the 
REFERENCING ATTRIBUTE in REFERENCING RELATION if attribute value used by
 REFERENCING ATTRIBUTE is updated in REFERENCED RELATION. e.g;, if we 
update a row from BRANCH with BRANCH_CODE ‘CS’ to ‘CSE’, the rows in 
STUDENT relation with BRANCH_CODE CS (ROLL_NO 1 and 2 in this case) will
 be updated with BRANCH_CODE ‘CSE’.

**SUPER KEYS:** Any
 set of attributes that allows us to identify unique rows (tuples) in a 
given relation are known as super keys. Out of these super keys we can 
always choose a proper subset among these which can be used as a primary
 key. Such keys are known as Candidate keys. If there is a combination 
of two or more attributes which is being used as the primary key then we
 call it as a Composite key.

[Basic Operators in Relational Algebra](https://www.geeksforgeeks.org/basic-operators-in-relational-algebra-2/)
*** Introduction of Relational Algebra in DBMS
Relational
 Algebra is procedural query language, which takes Relation as input and
 generate relation as output. Relational algebra mainly provides 
theoretical foundation for relational databases and SQL.

**Operators in Relational Algebra**

!https://videocdn.geeksforgeeks.org/geeksforgeeks/OperatorsinRelationalAlgebraDBMS/348Relationalalgebraoperators20220621124752.jpg

**Projection (π)**Projection is used to project required column data from a relation.

Example :

```
     R
  (A  B  C)
  ----------
   1  2  4
   2  2  3
   3  2  3
   4  3  4
```

```
π (BC)
B  C
-----
2  4
2  3
3  4
```

**Note:** By Default projection removes duplicate data.

**Selection (σ)**Selection is used to select required tuples of the relations.

for the above relationσ (c>3)Rwill select the tuples which have c more than 3.

**Note:** selection operator only selects the required tuples but does not display them. For displaying, data projection operator is used.

For the above selected tuples, to display we need to use projection also.

```
 π (σ (c>3)R ) will show following tuples.

A  B  C
-------
1  2  4
4  3  4
```

**Union (U)**Union operation in
 relational algebra is same as union operation in set theory, only 
constraint is for union of two relation both relation must have same set
 of Attributes.

**Set Difference (-)**Set
 Difference in relational algebra is same set difference operation as in
 set theory with the constraint that both relation should have same set 
of attributes.

**Rename (ρ)**Rename is a unary operation used for renaming attributes of a relation.ρ (a/b)R will rename the attribute ‘b’ of relation by ‘a’.

**Cross Product (X)**

Cross
 product between two relations let say A and B, so cross product between
 A X B will results all the attributes of A followed by each attribute 
of B. Each record of A will pairs with every record of B.

below is the example

```
   A                                  B
    (Name   Age  Sex )                (Id   Course)
    ------------------                -------------
    Ram    14   M                      1     DS
    Sona   15   F                      2     DBMS
    kim    20   M

     A X B
  Name   Age   Sex   Id   Course
---------------------------------
  Ram    14    M      1    DS
  Ram    14    M      2    DBMS
  Sona   15    F      1    DS
  Sona   15    F      2    DBMS
  Kim    20    M      1    DS
  Kim    20    M      2    DBMS
```

**Note:** if A has ‘n’ tuples and B has ‘m’ tuples then A X B will have ‘n*m’ tuples.

**Natural Join (⋈)**

Natural
 join is a binary operator. Natural join between two or more relations 
will result set of all combination of tuples where they have equal 
common attribute.

Let us see below example

```

           Emp                              Dep
   (Name   Id   Dept_name )          (Dept_name   Manager)
   ------------------------          ---------------------
     A     120    IT                    Sale     Y
     B     125    HR                    Prod     Z
     C     110    Sale                  IT       A
     D     111    IT

Emp ⋈ Dep

Name   Id   Dept_name   Manager
-------------------------------
A     120   IT          A
C     110   Sale        Y
D     111   IT          A
```

**Conditional Join**

Conditional
 join works similar to natural join. In natural join, by default 
condition is equal between common attribute while in conditional join we
 can specify the any condition such as greater than, less than, not 
equal

Let us see below example

```
         R                           S
  (ID   Sex   Marks)          (ID   Sex   Marks)
  ------------------          --------------------
   1   F   45                   10   M   20
   2   F   55                   11   M   22
   3   F   60                   12   M   59

Join between R And S with condition R.marks >= S.marks

R.ID   R.Sex   R.Marks   S.ID   S.Sex   S.Marks
-----------------------------------------------
1       F       45        10     M        20
1       F       45        11     M        22
2       F       55        10     M        20
2       F       55        11     M        22
3       F       60        10     M        20
3       F       60        11     M        22
3       F       60        12     M        59
```

---

In depth articles:[Basic-operators-in-relational-algebra](https://www.geeksforgeeks.org/basic-operators-in-relational-algebra-2/)                                          [Extended Relational Algebra Operators](https://www.geeksforgeeks.org/extended-operators-in-relational-algebra/)Following are Previous Year Gate Questionhttps://www.geeksforgeeks.org/gate-gate-cs-2012-question-50/https://www.geeksforgeeks.org/gate-gate-cs-2012-question-43/
*** Anomalies in Relational Model
We strongly recommend to refer below posts as a prerequisite of this.[DBMS | Relational Model Introduction and Codd Rules](https://www.geeksforgeeks.org/introduction-of-relational-model-and-codd-rules-in-dbms/)[DBMS | Keys in Relational Model (Candidate, Super, Primary, Alternate and Foreign)](https://www.geeksforgeeks.org/types-of-keys-in-relational-model-candidate-super-primary-alternate-and-foreign/)

**Anomalies**There are different types of anomalies which can occur in referencing and referenced relation which can be discussed as:

!https://media.geeksforgeeks.org/wp-content/uploads/image7.png

**Insertion anomaly:**
 If a tuple is inserted in referencing relation and referencing 
attribute value is not present in referenced attribute, it will not 
allow inserting in referencing relation. For Example, If we try to 
insert a record in STUDENT_COURSE with STUD_NO =7, it will not allow.

**Deletion and Updation anomaly:**
 If a tuple is deleted or updated from referenced relation and 
referenced attribute value is used by referencing attribute in 
referencing relation, it will not allow deleting the tuple from 
referenced relation. For Example, If we try to delete a record from 
STUDENT with STUD_NO =1, it will not allow. To avoid this, following can
 be used in query:

- **ON DELETE/UPDATE SET NULL:** If a tuple is deleted or updated from referenced relation and
referenced attribute value is used by referencing attribute in
referencing relation, it will delete/update the tuple from referenced
relation and set the value of referencing attribute to NULL.
- **ON DELETE/UPDATE CASCADE:** If a tuple is deleted or updated from referenced relation and
referenced attribute value is used by referencing attribute in
referencing relation, it will delete/update the tuple from referenced
relation and referencing relation as well.
*** Introduction of Relational Model and Codd Rules in DBMS
**Terminology**

**Relational Model:**

of all, what are this course, rule.

Remaining Time -15:46

1x

**– * It was proposed by Dr. E.F.Codd. *It uses the concept of relations to represent each and every file. *Relations are Two-Dimensional Tables. * It is easy to implement and easy to simplification in the operations to manipulating the data. * This is most popular data model.* It is simple to implement.*It uses the primary key and secondary key to connect any two files.*Normalization Theory is used to design the object based data model.*Relational algebra and relational calculus are used to process the relations manually.*Many of the database languages are oriented handling relational data model.* A RDM consists of relations with to connect them by key fields. A relation has some attributes. They are*The relation is represented in rows and columns.* Each column of the relation is called as attribute.*Each row in the relation is called as tuple.*Each relation can have one unique columns i.e. primary key.* Each relation can have n-columns and n-tuple.Each relation is preceded by the name of that relation. The fields of the relations are separated by commas and placed within the parentheses of the relation.STUDENT (StudNo, Sname, Special)ENROLLMENT (StudNo, Subcode, marks)SUBJECT (Subcode, Subname, Maxmarks, Faccode)FACULTY (Faccode, Fname, Dept)**

****Relational model represents data in the form of relations or tables. **Relational Schema:**
 Schema represents structure of a relation. e.g.; Relational Schema of 
STUDENT relation can be represented as: STUDENT (STUD_NO, STUD_NAME, 
STUD_PHONE, STUD_STATE, STUD_COUNTRY, STUD_AGE) **Relational Instance:**
 The set of values present in a relation at a particular instance of 
time is known as relational instance as shown in Table 1 and Table 2. **Attribute:**
 Each relation is defined in terms of some properties, each of which is 
known as attribute. For Example, STUD_NO, STUD_NAME etc. are attributes 
of relation STUDENT. **Domain of an attribute:** The 
possible values an attribute can take in a relation is called its 
domain. For Example, domain of STUD_AGE can be from 18 to 40. **Tuple:** Each row of a relation is known as tuple. e.g.; STUDENT relation given below has 4 tuples. **NULL values:**
 Values of some attribute for some tuples may be unknown, missing or 
undefined which are represented by NULL. Two NULL values in a relation 
are considered different from each other. Table 1 and Table 2 represent 
relational model having two relations STUDENT and STUDENT_COURSE.

!https://media.geeksforgeeks.org/wp-content/uploads/image7.png

RDBMS Vendors:-* There are many different vendors that produce relational database management systems (RDBMS).DBMS Vendors DBMS Product Product.

RELATIONAL ALGEBRA:- *It is a procedural Language.* It consists of a set of operators that can be performed on relations. * It forms the basis for many other high level data sub-languages like SQL, QBE. * The relational algebra has mainly 9 types of operators. These are

- UNIONINTERSECTMINUSTIMESSELECTIONPROJECTIONJOINDIVISIONRENAME1. UNION (U):- A and B are two relations. It displays total values (Attributes) in both relations. It avoids the duplicate values in both relations.U symbol can be usedSyntax:- A UNION B (or) A U Bexample:- A = { clerk, manager, salesman}B = { president, clerk, manager}A UNION B = {clerk, manager, salesman, president}2. INTERSECTION (∩):-A and B are two relations. It displays common elements in both relations. “∩” symbol can be usedSyntax:- A INTERSECT B (or) A ∩ BExample:- A = { clerk, manager, salesman}B = { president, clerk, manager}A INTERSECT B = { clerk, manager}3. DIFFERENCE (─) :-  A and B are two relations. It displays elements in relation A not in relation B. Syntax:- A MINUS B (OR) A ─ BExample:- A = { clerk, manager, salesman} B = { president, clerk, manager}A MINUS B = { salesman}

4. CARTESIAN PRODUCT(X):-  A and B are two relations. It has a new relation consisting of all pair wises combinations of all elements in A and B.  The relation A has “m” elements and relation B has “n” elements, then the resultant relation will be “ m * n “.  Syntax:- A TIMES B (OR) A X BExample:- A = { clerk, manager, salesman}B = { president, clerk, manager}A TIMES B = { (clerk, president), (clerk, clerk), (clerk, manager), (manager, president), (manger, clerk), (manger, manger), (salesman, president), (salesman, clerk), (salesman, manager) }5. SELECTION ():- It displays all the attributes or columns of all the rows in a relation. “” operator can be used to select the attributes in a relation.Notation – σcondition(relation name)

6. PROJECTION (π) :-  It displays some specified columns in a relation.“π” operator can be used to select some specified columns in a relation. Syntax:- π(col1,col2…) Relation Name It displays some specified columns by using some conditionExample:- π(sno,sname,total) MARKS

It selects tuples that satisfy the given predicate from a relation.

7. JOIN ( ):-*It combines two or more relations. * Symbol can be used.* It can be mainly divided into mainly 4 types. These are mainlyo Inner Joino Outer Joino Left Outer Joino Right Outer Join

8. DIVIDE (÷): *It divides the tuple from one relation to another relationSyntax:- A DIVIDE B (OR) A ÷ BExample:- A = { clerk, manager, salesman} B = { clerk, manager}A DIVIDE B = {salesman}

9. RENAME(ρ):-* It gives another name to relation.Syntax:- ρ(OLD RELATION, NEW RELATION)Eg:- ρ(STUDENT, MARKS)It changes the “student” relation to “Marks” relation.* It also renames the specified column.* It changes the old-column name to new-column name.Syntax:- ρ(STUDNAME TO SNAME) RELATION NAME

Advantages of Relational Algebra*The relational algebra has solid mathematical background.* The mathematical background of relational algebra is the basis ofmany interesting developments and theorems.* If we have two expressions for the same operation and if theexpressions are proved to be equivalent, then a query optimizer canautomatically substitute the more efficient form.Disadvantages of Relational Algebra* The Relational algebra cannot run it on database server.*Relational algebra cannot do arithmetic. For example the price of petrol is 50/-, by increasing 10% price, which cannot be done using relational algebra.*It cannot sort or print results in various formats.*It cannot perform aggregate operations.

**Codd Rules**

Codd rules were proposed by E.F. Codd which should be satisfied by relational model.

1. **Foundation Rule:** For any system that is advertised as, or claimed to be, a relational
data base management system, that system must be able to manage data
bases entirely through its relational capabilities.
2. **Information Rule:** Data stored in Relational model must be a value of some cell of a table.
3. **Guaranteed Access Rule:** Every data element must be accessible by table name, its primary key and name of attribute whose value is to be determined.
4. **Systematic Treatment of NULL values:** NULL value in database must only correspond to missing, unknown or not applicable values.
5. **Active Online Catalog:** Structure of database must be stored in an online catalog which can be queried by authorized users.
6. **Comprehensive Data Sub-language Rule:** A database should be accessible by a language supported for definition, manipulation and transaction management operation.
7. **View Updating Rule:** Different views created for various purposes should be automatically updatable by the system.
8. **High level insert, update and delete rule:** Relational Model should support insert, delete, update etc. operations
at each level of relations. Also, set operations like Union,
Intersection and minus should be supported.
9. **Physical data independence:** Any modification in the physical location of a table should not enforce modification at application level.
10. **Logical data independence:** Any modification in logical or conceptual schema of a table should not
enforce modification at application level. For example, merging of two
tables into one should not affect application accessing it which is
difficult to achieve.
11. **Integrity Independence:** Integrity constraints modified at database level should not enforce modification at application level.
12. **Distribution Independence:** Distribution of data over various locations should not be visible to end-users.
13. **Non-Subversion Rule:** Low level access to data should not be able to bypass integrity rule to change data.
*** Types of Keys in Relational Model (Candidate, Super, Primary, Alternate and Foreign)
[DBMS | Relational Model Introduction and Codd Rules](https://www.geeksforgeeks.org/introduction-of-relational-model-and-codd-rules-in-dbms/)

**Different Types of Keys in Relational Model**

candidate, Keys primary Keys,

Remaining Time -18:31

1x

**Candidate Key:**
 The minimal set of attributes that can uniquely identify a tuple is 
known as a candidate key. For Example, STUD_NO in STUDENT relation.

- It is a minimal super key.
- It is a super key with no repeated data is called a candidate key.
- The minimal set of attributes that can uniquely identify a record.
- It must contain unique values.
- It can contain NULL values.
- Every table must have at least a single candidate key.
- A table can have multiple candidate keys but only one primary key (the
primary key cannot have a NULL value, so the candidate key with NULL
value can’t be the primary key).

Eg: Studid, Roll No, and email are candidate keys.

- The value of the Candidate Key is unique and non-null for every tuple.
- There can be more than one candidate key in a relation. For Example, STUD_NO is the candidate key for relation STUDENT.
- The candidate key can be simple (having only one attribute) or composite as well. For Example, {STUD_NO, COURSE_NO} is a composite candidate key
for relation STUDENT_COURSE.
- No, of candidate keys in a Relation are nC(floor(n/2)),for example if a Relation have 5 attributes i.e.
R(A,B,C,D,E) then total no of candidate keys are 5C(floor(5/2))=10.

> Note: In SQL Server a unique constraint that has a nullable column, allows the value ‘null‘ in that column only once. That’s why the STUD_PHONE attribute is a candidate here, but can not be ‘null’ value in the primary key attribute.
> 

**Super Key:** The
 set of attributes that can uniquely identify a tuple is known as Super 
Key. For Example, STUD_NO, (STUD_NO, STUD_NAME), etc.  Or  A super 
key is a group of single or multiple keys that identifies rows in a 
table. It supports NULL values. Example: SNO+PHONE is a super key.

- Adding zero or more attributes to the candidate key generates the super key.
- A candidate key is a super key but vice versa is not true.

**Primary Key:**
 There can be more than one candidate key in relation out of which one 
can be chosen as the primary key. For Example, STUD_NO, as well as 
STUD_PHONE, are candidate keys for relation STUDENT but STUD_NO can be 
chosen as the primary key (only one out of many candidate keys).

**Primary Key:**

- It is a unique key.
- It can identify only one tuple (a record) at a time.
- It has no duplicate values, it has unique values.
- It cannot be NULL.
- Primary keys are not necessarily to be a single column; more than one the column can also be a primary key for a table.
- Eg:- STUDENT table SNO is a primary keySNO SNAME ADDRESS PHONE

**Alternate Key:** The
 candidate key other than the primary key is called an alternate key. 
For Example, STUD_NO, as well as STUD_PHONE both, are candidate keys for
 relation STUDENT but STUD_PHONE will be an alternate key (only one out 
of many candidate keys). It is a secondary key

- All the keys which are not primary keys are called alternate keys.
- It contains two or more fields to identify two or more records.
- These values are repeated.Eg:- SNAME, ADDRESS is Alternate keys

**Foreign Key:**
 If an attribute can only take the values which are present as values of
 some other attribute, it will be a foreign key to the attribute to 
which it refers. The relation which is being referenced is called 
referenced relation and the corresponding attribute is called referenced
 attribute and the relation which refers to the referenced relation is 
called referencing relation and the corresponding attribute is called 
referencing attribute. The referenced attribute of the referenced 
relation should be the primary key to it. For Example, STUD_NO in 
STUDENT_COURSE is a foreign key to STUD_NO in STUDENT relation.

- It is a key it acts as a primary key in one table and it acts as secondary key in another table.
- It combines two or more relations (table) at a time.
- They act as a cross-reference between the tables.
- For example, DNO is a primary key in the DEPT table and a non-key in EMP

It
 may be worth noting that unlike the Primary Key of any given relation, 
Foreign Key can be NULL as well as may contain duplicate tuples i.e. it 
need not follow uniqueness constraint. For Example, STUD_NO in 
STUDENT_COURSE relation is not unique. It has been repeated for the 
first and third tuples. However, the STUD_NO in STUDENT relation is a 
primary key and it needs to be always unique, and it cannot be null.
*** Basic Operators in Relational Algebra
Basics of Relational model: [Relational Model](https://www.geeksforgeeks.org/relational-model-in-dbms/)

Relational
 Algebra is a procedural query language that takes relations as an input
 and returns relations as an output. There are some basic operators 
which can be applied on relations to producing the required results 
which we will discuss one by one. We will use STUDENT_SPORTS, EMPLOYEE, 
and STUDENT relations as given in Table 1, Table 2, and Table 3 
respectively to understand the various operators.

**Table 1: STUDENT_SPORTS**

[Untitled Database](https://www.notion.so/c918bcb00bc449c1acad65ff7996bf18?pvs=21)

**Table 2: EMPLOYEE**

[Untitled Database](https://www.notion.so/bcbadaef66224f8c9f28b20c2091196e?pvs=21)

**Table 3: STUDENT**

[Untitled Database](https://www.notion.so/fd5d645ccfd34c1c96614483668863c9?pvs=21)

***Selection operator (σ):*** Selection operator is used to select tuples from a relation based on some condition. Syntax:

```
σ (Cond)(Relation Name)
```

Extract students whose age is greater than 18 from STUDENT relation given in Table 3

```
σ (AGE>18)(STUDENT)
```

**[Note:** SELECT
 operator does not show any result, the projection operator must be 
called before the selection operator to generate or project the result. 
So, the correct syntax to generate the result is**:** ∏(σ (AGE>18)(STUDENT))]

**RESULT:**

[Untitled Database](https://www.notion.so/6c2f448fdc704120add38b899196f492?pvs=21)

***Projection Operator (∏):*** Projection operator is used to project particular columns from a relation. Syntax:

```
∏(Column 1,Column 2….Column n)(Relation Name)
```

Extract ROLL_NO and NAME from STUDENT relation given in Table 3

```
∏(ROLL_NO,NAME)(STUDENT)
```

**RESULT:**

[Untitled Database](https://www.notion.so/25bb987d10c743739bea036ae89d96bc?pvs=21)

**Note:** If the resultant relation after projection has duplicate rows, it will be removed. For Example  ∏(ADDRESS)(STUDENT) will remove one duplicate row with the value DELHI and return three rows.

***Cross Product(X):***
 Cross product is used to join two relations. For every row of 
Relation1, each row of Relation2 is concatenated. If Relation1 has m 
tuples and and Relation2 has n tuples, cross product of Relation1 and 
Relation2 will have m X n tuples. Syntax:

```
Relation1 X Relation2
```

To apply Cross Product on STUDENT relation given in Table 1 and STUDENT_SPORTS relation given in Table 2,

```
STUDENT X STUDENT_SPORTS
```

**RESULT:**

[Untitled Database](https://www.notion.so/706f54b044154930beeee69da011cbb0?pvs=21)

***Union (U):*** Union on two relations R1 and R2 can only be computed if R1 and R2 are **union compatible**
 (These two relations should have the same number of attributes and 
corresponding attributes in two relations have the same domain). Union 
operator when applied on two relations R1 and R2 will give a relation 
with tuples that are either in R1 or in R2. The tuples which are in both
 R1 and R2 will appear only once in result relation. Syntax:

```
 Relation1 U Relation2
```

Find person who is either students or employees, we can use Union operators like:

```
STUDENT U EMPLOYEE
```

**RESULT:**

[Untitled Database](https://www.notion.so/28a9e8b897484d2ea904542979cdef46?pvs=21)

***Minus (-):*** Minus on two relations R1 and R2 can only be computed if R1 and R2 are **union compatible**. Minus operator when applied on two relations as R1-R2 will give a relation with tuples that are in R1 but not in R2. Syntax:

```
 Relation1 - Relation2
```

Find person who is student but not employee, we can use minus operator like:

```
STUDENT - EMPLOYEE
```

**RESULT:**

[Untitled Database](https://www.notion.so/1e66bab18cb0408b8e0a83ecb6db22cf?pvs=21)

***Rename(ρ):*** Rename operator is used to give another name to a relation. Syntax:

```
ρ(Relation2, Relation1)
```

To rename STUDENT relation to STUDENT1, we can use rename operator like:

```
ρ(STUDENT1, STUDENT)
```

If you want to create a relation STUDENT_NAMES with ROLL_NO and NAME from STUDENT, it can be done using rename operator as:

```
ρ(STUDENT_NAMES, ∏(ROLL_NO, NAME)(STUDENT))
```

[Extended Relational Algebra Operators](https://www.geeksforgeeks.org/extended-operators-in-relational-algebra/)                                        [Overview of Relational Algebra Operators](https://www.geeksforgeeks.org/introduction-of-relational-algebra-in-dbms/)
*** Extended Operators in Relational Algebra
Basic idea about  relational model and basic operators in Relational Algebra:

[Relational Model](https://www.geeksforgeeks.org/relational-model-in-dbms/)

[Basic Operators in Relational Algebra](https://www.geeksforgeeks.org/basic-operators-in-relational-algebra-2/)

Extended operators are those operators which can be derived from basic operators.There are mainly three types of extended operators in Relational Algebra:

- **Join**
- **Intersection**
- **Divide**

The
 relations used to understand extended operators are STUDENT, 
STUDENT_SPORTS, ALL_SPORTS and EMPLOYEE which are shown in Table 1, 
Table 2, Table 3 and Table 4 respectively.

**STUDENT**

[Untitled Database](https://www.notion.so/e0021af5729644cbae18449001fb9381?pvs=21)

**Table 1**

**STUDENT_SPORTS**

[Untitled Database](https://www.notion.so/7d989ba4c62e4131a9a3240ff020d656?pvs=21)

**Table 2**

**ALL_SPORTS**

**SPORTS**

---

Badminton

---

Cricket

---

**Table 3**

**EMPLOYEE**

[Untitled Database](https://www.notion.so/62d56aa6837a4b4cab7908da22b16b08?pvs=21)

**Table 4**

***Intersection (∩):*** Intersection on two relations R1 and R2 can only be computed if R1 and R2 are **union compatible** (These
 two relation should have same number of attributes and corresponding 
attributes in two relations have same domain). Intersection operator 
when applied on two relations as R1**∩**R2 will give a relation with tuples which are in R1 as well as R2. Syntax:

```
 Relation1 ∩ Relation2
```

```
Example: Find a person who is student as well as employee-STUDENT ∩ EMPLOYEE
```

In terms of basic operators (union and minus) :

```
STUDENT ∩ EMPLOYEE = STUDENT + EMPLOYEE - (STUDENT U EMPLOYEE)
```

**RESULT:**

[Untitled Database](https://www.notion.so/8898b1c53ca94df8a935e1b275217385?pvs=21)

***Conditional Join(⋈c):***
 Conditional Join is used when you want to join two or more relation 
based on some conditions. Example: Select students whose ROLL_NO is 
greater than EMP_NO of employees

```
STUDENT⋈cSTUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE
```

In terms of basic operators (cross product and selection) :

```
σ (STUDENT.ROLL_NO>EMPLOYEE.EMP_NO)(STUDENT×EMPLOYEE)
```

**RESULT:**

[Untitled Database](https://www.notion.so/063e09fb0cd94bc1a683d762274dd519?pvs=21)

***Equijoin(⋈):*** Equijoin is a **special case of conditional join**
 where only equality condition holds between a pair of attributes. As 
values of two attributes will be equal in result of equijoin, only one 
attribute will be appeared in result.

Example:Select students whose ROLL_NO is equal to EMP_NO of employees

```
STUDENT⋈STUDENT.ROLL_NO=EMPLOYEE.EMP_NOEMPLOYEE
```

In terms of basic operators (cross product, selection and projection) :

```
∏(STUDENT.ROLL_NO, STUDENT.NAME, STUDENT.ADDRESS, STUDENT.PHONE, STUDENT.AGE EMPLOYEE.NAME, EMPLOYEE.ADDRESS, EMPLOYEE.PHONE, EMPLOYEE>AGE)(σ (STUDENT.ROLL_NO=EMPLOYEE.EMP_NO)(STUDENT×EMPLOYEE))
```

RESULT:

[Untitled Database](https://www.notion.so/28016d0ba30f469d8cb8cd52a2edf5eb?pvs=21)

***Natural Join(⋈):*** It
 is a special case of equijoin in which equality condition hold on all 
attributes which have same name in relations R and S (relations on which
 join operation is applied). While applying natural join on two 
relations, there is no need to write equality condition explicitly. 
Natural Join will also return the similar attributes only once as their 
value will be same in resulting relation.

Example: Select students whose ROLL_NO is equal to ROLL_NO of STUDENT_SPORTS as:

```
STUDENT⋈STUDENT_SPORTS
```

In terms of basic operators (cross product, selection and projection) :

```
∏(STUDENT.ROLL_NO, STUDENT.NAME, STUDENT.ADDRESS, STUDENT.PHONE, STUDENT.AGE STUDENT_SPORTS.SPORTS)(σ (STUDENT.ROLL_NO=STUDENT_SPORTS.ROLL_NO)(STUDENT×STUDENT_SPORTS))
```

**RESULT:**

[Untitled Database](https://www.notion.so/1efcf1ea19c74416931e154c0860f4d1?pvs=21)

Natural
 Join is by default inner join because the tuples which does not satisfy
 the conditions of join does not appear in result set. e.g.; The tuple 
having ROLL_NO 3 in STUDENT does not match with any tuple in 
STUDENT_SPORTS, so it has not been a part of result set.

***Left Outer Join(⟕):***
 When applying join on two relations R and S, some tuples of R or S does
 not appear in result set which does not satisfy the join conditions. 
But Left Outer Joins gives all tuples of R in the result set. The tuples
 of R which do not satisfy join condition will have values as NULL for 
attributes of S.

Example:Select students whose ROLL_NO is greater than EMP_NO of employees and details of other students as well

```
STUDENT&#10197STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE
```

**RESULT**

[Untitled Database](https://www.notion.so/7d9bc948eec342248867898933db0566?pvs=21)

***Right Outer Join(⟖):***
 When applying join on two relations R and S, some tuples of R or S does
 not appear in result set which does not satisfy the join conditions. 
But Right Outer Joins gives all tuples of S in the result set. The 
tuples of S which do not satisfy join condition will have values as NULL
 for attributes of R.

Example: Select students whose ROLL_NO is greater than EMP_NO of employees and details of other Employees as well

```
STUDENT⟖STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE
```

**RESULT:**

[Untitled Database](https://www.notion.so/2ae5a8d1386849f590b291faf14d2b8b?pvs=21)

***Full Outer Join(⟗):***
 When applying join on two relations R and S, some tuples of R or S does
 not appear in result set which does not satisfy the join conditions. 
But Full Outer Joins gives all tuples of S and all tuples of R in the 
result set. The tuples of S which do not satisfy join condition will 
have values as NULL for attributes of R and vice versa.

Example:Select
 students whose ROLL_NO is greater than EMP_NO of employees and details 
of other Employees as well and other Students as well

```
STUDENT⟗STUDENT.ROLL_NO>EMPLOYEE.EMP_NOEMPLOYEE
```

**RESULT:**

[Untitled Database](https://www.notion.so/d8baa4b489314b8aafd35b5d6d65d26b?pvs=21)

***Division Operator (÷):*** Division operator A**÷**B can be applied if and only if:

- Attributes of B is proper subset of Attributes of A.
- The relation returned by division operator will have attributes = (All attributes of A – All Attributes of B)
- The relation returned by division operator will return those tuples from relation A which are associated to every B’s tuple.

Consider the relation STUDENT_SPORTS and ALL_SPORTS given in Table 2 and Table 3 above.

To apply division operator as

```
STUDENT_SPORTS÷ ALL_SPORTS
```

- The operation is valid as attributes in ALL_SPORTS is a proper subset of attributes in STUDENT_SPORTS.
- The attributes in resulting relation will have attributes {ROLL_NO,SPORTS}-{SPORTS}=ROLL_NO
- The tuples in resulting relation will have those ROLL_NO which are
associated with all B’s tuple {Badminton, Cricket}. ROLL_NO 1 and 4 are
associated to Badminton only. ROLL_NO 2 is associated to all tuples of
B. So the resulting relation will be:

**ROLL_NO**

---

2

---

---

[Overview of Relational Algebra Operators](https://www.geeksforgeeks.org/introduction-of-relational-algebra-in-dbms/)
*** Tuple Relational Calculus (TRC) in DBMS
Tuple Relational Calculus is a **non-procedural query language**
 unlike relational algebra. Tuple Calculus provides only the description
 of the query but it does not provide the methods to solve it. Thus, it 
explains what to do but not how to do. In Tuple Calculus, a query is expressed as

```
{t| P(t)}
```

where t = resulting tuples, P(t) = known as Predicate and these are the conditions that are used to fetch t

Thus, it generates set of all tuples t, such that Predicate P(t) is true for t.

P(t) may have various conditions logically combined with OR (∨), AND (∧), NOT(¬). It also uses quantifiers: ∃ t ∈ r (Q(t)) = ”there exists” a tuple in t in relation r such that predicate Q(t) is true. ∀ t ∈ r (Q(t)) = Q(t) is true “for all” tuples in relation r.

**Example:** **Table-1: Customer**

[Untitled Database](https://www.notion.so/074c3e7e9602402caef084a61ee2bc4a?pvs=21)

**Table-2: Branch**

[Untitled Database](https://www.notion.so/fcc202ba5c9a4cdc9285256e39f8a7b5?pvs=21)

**Table-3: Account**

[Untitled Database](https://www.notion.so/5c26bd95722743a287f8e4291d7a6c22?pvs=21)

**Table-4: Loan**

[Untitled Database](https://www.notion.so/a49bb35bfc8d4ad28ec1f67ef0181056?pvs=21)

**Table-5: Borrower**

[Untitled Database](https://www.notion.so/bb9c208e76df45b688ef9814179ea830?pvs=21)

**Table-6: Depositor**

[Untitled Database](https://www.notion.so/9b1aac95386a4a27b051237e15b8cd86?pvs=21)

**Queries-1:** Find the loan number, branch, amount of loans of greater than or equal to 10000 amount.

```
{t| t ∈ loan  ∧ t[amount]>=10000}
```

Resulting relation:

[Untitled Database](https://www.notion.so/ac78d621562643b8aa166b72f28138ea?pvs=21)

In the above query, t[amount] is known as tuple variable.

**Queries-2:** Find the loan number for each loan of an amount greater or equal to 10000.

```
{t| ∃ s ∈ loan(t[loan number] = s[loan number]
                   ∧ s[amount]>=10000)}
```

Resulting relation:

Loan number

---

L33

---

L35

---

L98

---

**Queries-3:** Find the names of all customers who have a loan and an account at the bank.

```
{t | ∃ s ∈ borrower( t[customer-name] = s[customer-name])
     ∧  ∃ u ∈ depositor( t[customer-name] = u[customer-name])}
```

Resulting relation:

Customer name

---

Saurabh

---

Mehak

---

**Queries-4:** Find the names of all customers having a loan at the “ABC” branch.

```
{t | ∃ s ∈ borrower(t[customer-name] = s[customer-name]
   ∧ ∃ u ∈  loan(u[branch-name] = “ABC” ∧ u[loan-number] = s[loan-number]))}
```

Resulting relation:

Customer name

---

Saurabh

---

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** How to solve Relational Algebra problems for GATE
In 
this article, Let’s discuss common types of questions in relational 
algebra which are asked in GATE. Before reading this article, you should
 have an idea about [Basic Operators](https://www.geeksforgeeks.org/basic-operators-in-relational-algebra-2/) and [Extended Operators](https://www.geeksforgeeks.org/extended-operators-in-relational-algebra/) in relational algebra.

**Type-1: Given a relational algebra expression, find the result.** Suppose
 you have a relation Order(Prod_Id, Agent_Id, Order_Month) and you have 
to find out what will the following algebra expression return.

```
∏Order1.Prod_Id (ρ(Order1,Order)Order1.Prod_Id=Order2.Prod_Id                                          and Order1.Agent_Id≠Order2.Agent_Id                                          and Order1.Order_Month=Order2.Order_Monthρ(Order2,Order))
```

Process the expression starting from innermost brackets.

In
 this example, we have renamed order to Order1 and Order2 (Both 
represent the same relation order). Then we have applied the conditional
 join between Order1 and Order2. It will return those rows where 
Product_Id and Order_Month of Order1 and Order2 are the same but 
Agent_Id of Order1 and Order2 is different. It implies the rows where 
the same product is ordered by two different agents in the same month. 
Then we are projecting the Prod_Id.

So the final output will 
return the Prod_Id of products that are ordered by different agents in 
the same month. We can do this by taking sample data. Let Order relation
 consists of the following data.

**Table –** Order

[Untitled Database](https://www.notion.so/bfbfb7abb94b40b4a5dc577dadd4a53b?pvs=21)

When we apply the following expression, the rows which are highlighted in blue will be selected.

```
(ρ(Order1,Order)Order1.Prod_Id=Order2.Prod_Id                   and Order1.Agent_Id≠Order2.Agent_Id                   and Order1.Order_Month=Order2.Order_Monthρ(Order2,Order))
```

[Untitled Database](https://www.notion.so/f0b14a8a06cc48e3b07fa8edceb1dc3a?pvs=21)

After projecting Order1.Prod_Id, the output will be **P002** which is Prod_Id of products that are ordered by at least two different agents in the same month.

**Note –** If we want to find Prod_Id which are ordered by at least three different agents in same month, it can be done as: 

```
∏Order1.Prod_Id (σOrder1.Prod_Id=Order2.Prod_Id                                 and Order1.Prod_Id=Order3.Prod_Id                                 and  Order1.Agent_Id≠Order2.Agent_Id                                 and Order1.Agent_Id≠Order3.Agent_Id                                 and Order2.Agent_Id≠Order3.Agent_Id                                 and Order1.Order_Month=Order2.Order_Month                                 and Order1.Order_Month=Order3.Order_Month(ρ(Order1,Order)X ρ(Order2,Order)X ρ(Order3,Order)))
```

**Type-2: Given two relations, what will be the maximum and minimum number of tuples after natural join?**

Consider the following relation R(A,B,C) and S(B,D,E)
 with underlined primary key. The relation R contains 200 tuples and the
 relation S contains 100 tuples. What is the maximum number of tuples 
possible in the natural Join R and S?

To solve this type of 
question, first, we will see on which attribute natural join will take 
place. Natural join selects those rows which have equal values for 
common attribute. In this case, the expression would be like:

```
σR.B=S.B(RX S)
```

In
 relation R, attribute B is the primary key. So Relation R will have 200
 distinct values of B. On the other hand, Relation S has BD as the 
primary key. So attribute B can have 100 distinct values or 1 value for 
all rows.

**Case-1:** S.B has 100 distinct values and each of these values match to R.B

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/11111111.png

In this case, every value of B in S will match to a value of B in R. So natural join will have 100 tuples.

**Case-2:** S.B has 1 values and this values match to R.B

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/22222222.png

In this case, every value of B in S will match to a value of B in R. So natural join will have 100 tuples.

**Case-3:** S.B has 100 distinct values and none of these values matches to R.B

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/33333333.png

In this case, no value of B in S will match to a value of B in R. So natural join will have 0 tuple.

**Case-4:** S.B has 1 value and it does not match with R.B

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/4444444.png

In this case, no value of B in S will match to a value of B in R. So natural join will have 0 tuples.

So the maximum number of tuples will be 100 and min will be 0.

**Note –**
 If it is explicitly mentioned that S.B is a foreign key to R.B, then 
Case-3 and Case-4 discussed above are not possible because the value of 
S.B will be from the values of R.B. So, the minimum and maximum number 
of tuples in natural join will be 100.
*** Difference between Row oriented and Column oriented data stores in DBMS
A **data store**
 is basically a place for storing collections of data, such as a 
database, a file system or a directory. In Database system they can be 
stored in two ways. These are as follows:

1. Row Oriented Data Stores
2. Column-Oriented Data Stores

Comparisons between Row oriented data stores and Column oriented data stores are as following:

[Untitled Database](https://www.notion.so/75e8ae91c2f24afdbd8694f4b1ec97ab?pvs=21)

Best Example of Row-oriented data stores is **Relational Database**,
 which is a structured data storage and also a sophisticated query 
engine. It incurs a big penalty to improve performance as the data size 
increases.

The best example of a Column-Oriented datastores is **HBase Database**,
 which is basically designed from the ground up to provide scalability 
and partitioning to enable efficient data structure serialization, 
storage, and retrieval.

Features of Relational Database and HBase are as following:

[Untitled Database](https://www.notion.so/330c08493c224f0d962931b3295f86b4?pvs=21)
** Database design (integrity constraints, normal forms)
*** Introduction of Database Normalization
Database normalization is the process of organizing the attributes of the database to reduce or eliminate **data redundancy (having the same data but at different places)** .

**Problems because of data redundancy** Data
 redundancy unnecessarily increases the size of the database as the same
 data is repeated in many places. Inconsistency problems also arise 
during insert, delete and update operations.

**Functional Dependency** Functional
 Dependency is a constraint between two sets of attributes in relation 
to a database. A functional dependency is denoted by an arrow (→). If an
 attribute A functionally determines B, then it is written as A → B.

For
 example, employee_id → name means employee_id functionally determines 
the name of the employee. As another example in a timetable database, 
{student_id, time} → {lecture_room}, student ID and time determine the 
lecture room where the student should be.

**What does functionally dependent mean?** A function dependency A → B means for all instances of a particular value of A, there is the same value of B.

For example in the below table A → B is true, but B → A is not true as there are different values of A for B = 3.

```
A   B
------
1   3
2   3
4   0
1   3
4   0
```

**Trivial Functional Dependency** X → Y is trivial only when Y is subset of X. Examples 

```
ABC → AB
ABC → A
ABC → ABC
```

**Non Trivial Functional Dependencies** X → Y is a non trivial functional dependency when Y is not a subset of X.

X → Y is called completely non-trivial when X intersect Y is NULL. 

**Example:**

```
Id → Name,
Name → DOB
```

**Semi Non Trivial Functional Dependencies** X → Y is called semi non-trivial when X intersect Y is not NULL. Examples: 

```
AB → BC,
AD → DC
```
*** Normal Forms in DBMS
**Prerequisite –** [Database normalization and functional dependency concept](https://www.geeksforgeeks.org/introduction-of-database-normalization/).

**Normalization** is the process of minimizing **redundancy**
 from a relation or set of relations. Redundancy in relation may cause 
insertion, deletion, and update anomalies. So, it helps to minimize the 
redundancy in relations. **Normal forms** are used to eliminate or reduce redundancy in database tables.

### 1. First Normal Form –

If
 a relation contain composite or multi-valued attribute, it violates 
first normal form or a relation is in first normal form if it does not 
contain any composite or multi-valued attribute. A relation is in first 
normal form if every attribute in that relation is **singled valued attribute**.

- **Example 1 –** Relation STUDENT in table 1 is not in 1NF because of multi-valued
attribute STUD_PHONE. Its decomposition into 1NF has been shown in table 2.
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/20210115105301/1239.png
    
- **Example 2 –**
    
    ```
    
    ID   Name   Courses
    ------------------
    1    A      c1, c2
    2    E      c3
    3    M      C2, c3
    
    ```
    
    In the above table Course is a multi-valued attribute so it is not in 1NF.
    
    Below Table is in 1NF as there is no multi-valued attribute
    
    ```
    ID   Name   Course
    ------------------
    1    A       c1
    1    A       c2
    2    E       c3
    3    M       c2
    3    M       c3
    
    ```
    

### 2. Second Normal Form –

To be in 
second normal form, a relation must be in first normal form and relation
 must not contain any partial dependency. A relation is in 2NF if it has
 **No Partial Dependency,** i.e.**,** no 
non-prime attribute (attributes which are not part of any candidate key)
 is dependent on any proper subset of any candidate key of the table.

**Partial Dependency –** If the proper subset of candidate key determines non-prime attribute, it is called partial dependency.

- **Example 1 –** Consider table-3 as following below.
    
    ```
    STUD_NO            COURSE_NO        COURSE_FEE
    1                     C1                  1000
    2                     C2                  1500
    1                     C4                  2000
    4                     C3                  1000
    4                     C1                  1000
    2                     C5                  2000
    
    ```
    
    {Note that, there are many courses having the same course fee. }
    
    Here,COURSE_FEE cannot alone decide the value of COURSE_NO or STUD_NO;COURSE_FEE together with STUD_NO cannot decide the value of COURSE_NO;COURSE_FEE together with COURSE_NO cannot decide the value of STUD_NO;Hence,COURSE_FEE would be a non-prime attribute, as it does not belong to the one only candidate key {STUD_NO, COURSE_NO} ;But,
     COURSE_NO -> COURSE_FEE, i.e., COURSE_FEE is dependent on COURSE_NO,
     which is a proper subset of the candidate key. Non-prime attribute 
    COURSE_FEE is dependent on a proper subset of the candidate key, which 
    is a partial dependency and so this relation is not in 2NF.
    
    To convert the above relation to 2NF,we need to split the table into two tables such as :Table 1: STUD_NO, COURSE_NOTable 2: COURSE_NO, COURSE_FEE
    
    ```
    Table 1Table 2
    STUD_NO            COURSE_NO          COURSE_NO                COURSE_FEE
    1                 C1                  C1                        1000
    2                 C2                  C2                        1500
    1                 C4                  C3                        1000
    4                 C3                  C4                        2000
    4                 C1                  C5                        2000
    ```
    
    2 C5
    
    **NOTE:**
     2NF tries to reduce the redundant data getting stored in memory. For 
    instance, if there are 100 students taking C1 course, we don’t need to 
    store its Fee as 1000 for all the 100 records, instead, once we can 
    store it in the second table as the course fee for C1 is 1000.
    
- **Example 2 –** Consider following functional dependencies in relation R (A, B , C, D )
    
    ```
    AB -> C  [A and B together determine C]
    BC -> D  [B and C together determine D]
    ```
    
    In the above 
    relation, AB is the only candidate key and there is no partial 
    dependency, i.e., any proper subset of AB doesn’t determine any 
    non-prime attribute.
    

### 3. Third Normal Form –

A relation is in third normal form, if there is **no transitive dependency** for non-prime attributes as well as it is in second normal form.A relation is in 3NF if **at least one of the following condition holds** in every non-trivial function dependency X –> Y

1. X is a super key.
2. Y is a prime attribute (each element of Y is part of some candidate key).

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Normalisation_normalforms_3.png

**Transitive dependency –** If A->B and B->C are two FDs then A->C is called transitive dependency.

- **Example 1 –** In relation STUDENT given in Table 4,
    
    FD set: {STUD_NO -> STUD_NAME, STUD_NO -> STUD_STATE, STUD_STATE -> STUD_COUNTRY, STUD_NO -> STUD_AGE}Candidate Key: {STUD_NO}
    
    For
     this relation in table 4, STUD_NO -> STUD_STATE and STUD_STATE ->
     STUD_COUNTRY are true. So STUD_COUNTRY is transitively dependent on 
    STUD_NO. It violates the third normal form. To convert it in third 
    normal form, we will decompose the relation STUDENT (STUD_NO, STUD_NAME,
     STUD_PHONE, STUD_STATE, STUD_COUNTRY_STUD_AGE) as:STUDENT (STUD_NO, STUD_NAME, STUD_PHONE, STUD_STATE, STUD_AGE)STATE_COUNTRY (STATE, COUNTRY)
    
- **Example 2 –** Consider relation R(A, B, C, D, E)A -> BC,CD -> E,B -> D,E -> AAll possible candidate keys in above relation are {A, E, CD, BC} All
attributes are on right sides of all functional dependencies are prime.

### 4. Boyce-Codd Normal Form (BCNF) –

A
 relation R is in BCNF if R is in Third Normal Form and for every FD, 
LHS is super key. A relation is in BCNF iff in every non-trivial 
functional dependency X –> Y, X is a super key.

- **Example 1 –** Find the highest normal form of a relation R(A,B,C,D,E) with FD set as {BC->D, AC->BE, B->E}Step 1. As we can see, (AC)+ ={A,C,B,E,D} but none of its subset can
determine all attribute of relation, So AC will be candidate key. A or C can’t be derived from any other attribute of the relation, so there
will be only 1 candidate key {AC}.Step 2. Prime attributes are those attributes that are part of candidate key {A, C} in this example and
others will be non-prime {B, D, E} in this example.Step 3. The relation R is in 1st normal form as a relational DBMS does not allow multi-valued or composite attribute.The relation is in 2nd normal form because BC->D is in 2nd normal form
(BC is not a proper subset of candidate key AC) and AC->BE is in 2nd
normal form (AC is candidate key) and B->E is in 2nd normal form (B
is not a proper subset of candidate key AC).The relation is not in
3rd normal form because in BC->D (neither BC is a super key nor D is a prime attribute) and in B->E (neither B is a super key nor E is a
prime attribute) but to satisfy 3rd normal for, either LHS of an FD
should be super key or RHS should be prime attribute.So the highest normal form of relation will be 2nd Normal form.
- **Example 2 –**For example consider relation R(A, B, C)A -> BC,B ->A and B both are super keys so above relation is in BCNF.

**Key Points –**

1. BCNF is free from redundancy.
2. If a relation is in BCNF, then 3NF is also satisfied.
3. If all attributes of relation are prime attribute, then the relation is always in 3NF.
4. A relation in a Relational Database is always and at least in 1NF form.
5. Every Binary Relation ( a Relation with only 2 attributes ) is always in BCNF.
6. If a Relation has only singleton candidate keys( i.e. every candidate key
consists of only 1 attribute), then the Relation is always in 2NF(
because no Partial functional dependency possible).
7. Sometimes
going for BCNF form may not preserve functional dependency. In that case go for BCNF only if the lost FD(s) is not required, else normalize till 3NF only.
8. There are many more Normal forms that exist after
BCNF, like 4NF and more. But in real world database systems it’s
generally not required to go beyond BCNF.

**Exercise 1**: Find the highest normal form in R (A, B, C, D, E) under following functional dependencies.

```
  ABC --> D
  CD --> AE
```

**Important Points** for solving above type of question.**1)** It is always a good idea to start checking from BCNF, then 3 NF, and so on.**2)** If
 any functional dependency satisfied a normal form then there is no need
 to check for lower normal form. For example, ABC –> D is in BCNF 
(Note that ABC is a superkey), so no need to check this dependency for 
lower normal forms.

Candidate keys in the given relation are {ABC, BCD}

**BCNF:**
 ABC -> D is in BCNF. Let us check CD -> AE, CD is not a super key
 so this dependency is not in BCNF. So, R is not in BCNF.

**3NF:**
 ABC -> D we don’t need to check for this dependency as it already 
satisfied BCNF. Let us consider CD -> AE. Since E is not a prime 
attribute, so the relation is not in 3NF.

**2NF:** In
 2NF, we need to check for partial dependency. CD is a proper subset of a
 candidate key and it determines E, which is non-prime attribute. So, 
given relation is also not in 2 NF. So, the highest normal form is 1 NF.
*** Functional Dependency and Attribute Closure
**Functional Dependency**

A
 functional dependency A->B in a relation holds if two tuples having 
same value of attribute A also have same value for attribute B. For 
Example, in relation STUDENT shown in table 1, Functional Dependencies 

```
STUD_NO->STUD_NAME, STUD_NO->STUD_PHONEhold
```

but 

```
STUD_NAME->STUD_STATEdo not hold
```

!https://media.geeksforgeeks.org/wp-content/uploads/image23.png

**How to find functional dependencies for a relation?**

Functional
 Dependencies in a relation are dependent on the domain of the relation.
 Consider the STUDENT relation given in Table 1. 

- We
know that STUD_NO is unique for each student. So STUD_NO->STUD_NAME,
STUD_NO->STUD_PHONE, STUD_NO->STUD_STATE, STUD_NO->STUD_COUNTRY and STUD_NO -> STUD_AGE all will be true.
- Similarly,
STUD_STATE->STUD_COUNTRY will be true as if two records have same
STUD_STATE, they will have same STUD_COUNTRY as well.
- For
relation STUDENT_COURSE, COURSE_NO->COURSE_NAME will be true as two
records with same COURSE_NO will have same COURSE_NAME.

**Functional Dependency Set:**  Functional
 Dependency set or FD set of a relation is the set of all FDs present in
 the relation. For Example, FD set for relation STUDENT shown in table 1
 is: 

```
 { STUD_NO->STUD_NAME, STUD_NO->STUD_PHONE, STUD_NO->STUD_STATE, STUD_NO->STUD_COUNTRY,
  STUD_NO -> STUD_AGE, STUD_STATE->STUD_COUNTRY }
```

**Attribute Closure:** Attribute closure of an attribute set can be defined as set of attributes which can be functionally determined from it.

**How to find attribute closure of an attribute set?** To find attribute closure of an attribute set: 

- Add elements of attribute set to the result set.
- Recursively add elements to the result set which can be functionally determined from the elements of the result set.

Using FD set of table 1, attribute closure can be determined as: 

```
(STUD_NO)+ = {STUD_NO, STUD_NAME, STUD_PHONE, STUD_STATE, STUD_COUNTRY, STUD_AGE}
(STUD_STATE)+ = {STUD_STATE, STUD_COUNTRY}
```

**How to find Candidate Keys and Super Keys using Attribute Closure?**

- If attribute closure of an attribute set contains all attributes of
relation, the attribute set will be super key of the relation.
- If no subset of this attribute set can functionally determine all
attributes of the relation, the set will be candidate key as well. For
Example, using FD set of table 1,

(STUD_NO, STUD_NAME)+ = {STUD_NO, STUD_NAME, STUD_PHONE, STUD_STATE, STUD_COUNTRY, STUD_AGE}

(STUD_NO)+ = {STUD_NO, STUD_NAME, STUD_PHONE, STUD_STATE, STUD_COUNTRY, STUD_AGE}

(STUD_NO,
 STUD_NAME) will be super key but not candidate key because its subset 
(STUD_NO)+ is equal to all attributes of the relation. So, STUD_NO will 
be a candidate key.

**GATE Question: Consider the relation 
scheme R = {E, F, G, H, I, J, K, L, M, N} and the set of functional 
dependencies {{E, F} -> {G}, {F} -> {I, J}, {E, H} -> {K, L}, K
 -> {M}, L -> {N} on R. What is the key for R? (GATE-CS-2014)** A. {E, F} B. {E, F, H} C. {E, F, H, K, L} D. {E}

**Answer:** Finding attribute closure of all given options, we get: {E,F}+ = {EFGIJ} {E,F,H}+ = {EFHGIJKLMN} {E,F,H,K,L}+ = {{EFHGIJKLMN} {E}+ = {E} {EFH}+
 and {EFHKL}+ results in set of all attributes, but EFH is minimal. So 
it will be candidate key. So correct option is (B). 

**How to check whether an FD can be derived from a given FD set?**

To check whether an FD A->B can be derived from an FD set F, 

1. Find (A)+ using FD set F.
2. If B is subset of (A)+, then A->B is true else not true.

**GATE Question: In a schema with attributes A, B, C, D and E following set of functional dependencies are given** **{A -> B, A -> C, CD -> E, B -> D, E -> A}** **Which of the following functional dependencies is NOT implied by the above set? (GATE IT 2005)** A. CD -> AC B. BD -> CD C. BC -> CD D. AC -> BC

**Answer:** Using FD set given in question, (CD)+ = {CDEAB} which means CD -> AC also holds true. (BD)+ = {BD} which means BD -> CD can’t hold true. So this FD is no implied in FD set. So (B) is the required option. Others can be checked in the same way. 

**Prime and non-prime attributes**

Attributes
 which are parts of any candidate key of relation are called as prime 
attribute, others are non-prime attributes. For Example, STUD_NO in 
STUDENT relation is prime attribute, others are non-prime attribute.

**GATE
 Question:  Consider a relation scheme R = (A, B, C, D, E, H) on which 
the following functional dependencies hold: {A–>B, BC–> D, 
E–>C, D–>A}. What are the candidate keys of R? [GATE 2005]** (a) AE, BE (b) AE, BE, DE (c) AEH, BEH, BCH (d) AEH, BEH, DEH

**Answer:** (AE)+ = {ABECD} which is not set of all attributes. So AE is not a candidate key. Hence option A and B are wrong. (AEH)+ = {ABCDEH} (BEH)+ = {BEHCDA} (BCH)+ = {BCHDA} which is not set of all attributes. So BCH is not a candidate key. Hence option C is wrong. So correct answer is D.
*** Finding Attribute Closure and Candidate Keys using Functional Dependencies
### **What is Functional Dependency?**

A
 functional dependency X->Y in a relation holds if two tuples having 
same value for X also have same value for Y i.e  X uniquely determines 
Y.

In EMPLOYEE relation given in Table 1,

- FD **E-ID->E-NAME** holds because for each E-ID, there is a unique value of E-NAME.
- FD **E-ID->E-CITY** and **E-CITY->E-STATE** also holds.
- FD E-NAME->E-ID **does not hold** because E-NAME ‘John’ is not uniquely determining E-ID. There are 2 E-IDs corresponding to John (E001 and E003).

**EMPLOYEE**

[Untitled Database](https://www.notion.so/d8758d8a2be5454ba0bfd2d5b7cd758d?pvs=21)

**Table 1**

The FD set for EMPLOYEE relation given in Table 1 are:

```
{E-ID->E-NAME, E-ID->E-CITY, E-ID->E-STATE, E-CITY->E-STATE}
```

**Trivial versus Non-Trivial Functional Dependency:** A trivial functional dependency is the one which will always hold in a relation.

```
X->Y will always hold if X ⊇ Y
```

In the example given above, **E-ID, E-NAME->E-ID**
 is a trivial functional dependency and will always hold because 
{E-ID,E-NAME} ⊃ {E-ID}. You can also see from the table that for each 
value of {E-ID, E-NAME}, value of E-ID is unique, so {E-ID, E-NAME} 
functionally determines E-ID.

If a functional dependency is not trivial, it is called **Non-Trivial Functional Dependency**. Non-Trivial functional dependency may or may not hold in a relation. e.g; **E-ID->E-NAME** is a non-trivial functional dependency which holds in the above relation.

**Properties of Functional Dependencies**

Let *X*, *Y*, and *Z* are sets of attributes in a relation *R*. There are several properties of functional dependencies which always hold in R also known as Armstrong Axioms.

1. **Reflexivity**: If *Y* is a subset of *X*, then *X* → *Y.* e.g.; Let X represents {E-ID, E-NAME} and Y represents {E-ID}. {E-ID, E-NAME}->E-ID is true for the relation.
2. **Augmentation**: If *X* → *Y*, then *XZ* → *YZ.* e.g.; Let X represents {E-ID}, Y represents {E-NAME} and Z represents
{E-CITY}. As {E-ID}->E-NAME is true for the relation, so {
E-ID,E-CITY}->{E-NAME,E-CITY} will also be true.
3. **Transitivity**: If *X* → *Y* and *Y* → *Z*, then *X* → *Z.* e.g.; Let X represents {E-ID}, Y represents {E-CITY} and Z represents
{E-STATE}. As {E-ID} ->{E-CITY} and {E-CITY}->{E-STATE} is true
for the relation, so { E-ID }->{E-STATE} will also be true.
4. **Attribute Closure:** The set of attributes that are functionally dependent on the attribute A is called Attribute Closure of A and it can be represented as A.
    
    +
    

**Steps to Find the Attribute Closure of A**

Q. Given FD set of a Relation R, The attribute closure set S be the set of A

1. Add A to S.
2. Recursively add attributes which can be functionally determined from attributes of the set S until done.

From Table 1, FDs are

**Given R(E-ID, E-NAME, E-CITY, E-STATE)**

```
FDs = { E-ID->E-NAME, E-ID->E-CITY, E-ID->E-STATE, E-CITY->E-STATE }
```

The attribute closure of E-ID can be calculated as:

1. Add E-ID to the set {E-ID}
2. Add Attributes which can be derived from any attribute of set. In this
case, E-NAME and E-CITY, E-STATE can be derived from E-ID. So these are
also a part of closure.
3. As there is one other attribute remaining in relation to be derived from E-ID. So result is:

```
(E-ID)+ = {E-ID, E-NAME, E-CITY, E-STATE }
```

Similarly,

```
(E-NAME)+ = {E-NAME}(E-CITY)+ = {E-CITY, E_STATE}
```

---

**Q. Find the attribute closures of given FDs R(ABCDE) = {AB->C, B->D, C->E, D->A}** To find (B)+ ,we will add attribute in set using various FD which has been shown in table below. 

[Untitled Database](https://www.notion.so/2d28b14689644ea696c67700384113c5?pvs=21)

- We can find (C, D) by adding C and D into the set (triviality) and then E using(C->E) and then A using (D->A) and set becomes.
    
    +
    
    ```
      (C,D)+ = {C,D,E,A}
    ```
    
- Similarly we can find (B,C) by adding B and C into the set (triviality) and then D using (B->D)
and then E using (C->E) and then A using (D->A) and set becomes
    
    +
    
    ```
     (B,C)+ ={B,C,D,E,A}
    ```
    

---

**Candidate Key**

Candidate Key is **minimal set of attributes**
 of a relation which can be used to identify a tuple uniquely. For 
Example, each tuple of EMPLOYEE relation given in Table 1 can be 
uniquely identified by **E-ID** and it is minimal as well. So it will be Candidate key of the relation.

A candidate key may or may not be a primary key.

**Super Key**

Super Key is **set of attributes**
 of a relation which can be used to identify a tuple uniquely.For 
Example, each tuple of EMPLOYEE relation given in Table 1 can be 
uniquely identified by **E-ID or (E-ID, E-NAME) or (E-ID, E-CITY) or (E-ID, E-STATE) or (E_ID, E-NAME, E-STATE)** etc. So all of these are super keys of EMPLOYEE relation.

```
Note: A candidate key is always a super key but vice versa is not true.
```

---

```

```

**Q. Finding Candidate Keys and Super Keys of a Relation using FD set** The **set of attributes**
 whose attribute closure is set of all attributes of relation is called 
super key of relation. For Example, the EMPLOYEE relation shown in Table
 1 has following FD set. **{E-ID->E-NAME, E-ID->E-CITY, E-ID->E-STATE, E-CITY->E-STATE}** Let us calculate attribute closure of different set of attributes:

```
(E-ID)+ = {E-ID, E-NAME,E-CITY,E-STATE}
(E-ID,E-NAME)+ = {E-ID, E-NAME,E-CITY,E-STATE}
(E-ID,E-CITY)+ = {E-ID, E-NAME,E-CITY,E-STATE}
(E-ID,E-STATE)+ = {E-ID, E-NAME,E-CITY,E-STATE}
(E-ID,E-CITY,E-STATE)+ = {E-ID, E-NAME,E-CITY,E-STATE}
(E-NAME)+ = {E-NAME}
(E-CITY)+ = {E-CITY,E-STATE}
```

As (E-ID)+, (E-ID, E-NAME)+, (E-ID, E-CITY)+, (E-ID, E-STATE)+, (E-ID, E-CITY, E-STATE)+ give set of all attributes of relation EMPLOYEE. So all of these are super keys of relation.

The **minimal set of attributes** whose attribute closure is set of all attributes of relation is called candidate key of relation. As shown above, (E-ID)+ is set of all attributes of relation and it is minimal. So E-ID will be candidate key. On the other hand (E-ID, E-NAME)+ also is set of all attributes but it is not minimal because its subset (E-ID)+ is equal to set of all attributes. So (E-ID, E-NAME) is not a candidate key.
*** Number of Possible Super Keys in DBMS
Prerequisite – [Relational Model Introduction and Codd Rules](https://www.geeksforgeeks.org/dbms-relational-model-introduction-and-codd-rules/)

Any set of attributes of a table that can uniquely identify all the tuples of that table is known as a **Super key**.
 It’s different from the primary and candidate keys in the sense that 
only the minimal superkeys are the candidate/primary keys.

This means that from a super key when we **remove** all the attributes that are **unnecessary** for its uniqueness, only then it **becomes a primary/candidate**
 key. So, in essence, all primary/candidate keys are super keys but not 
all super keys are primary/candidate keys. By the formal definition of a
 Relation(Table), we know that the tuples of a relation are all unique. 
So, the set of all attributes itself is a super key.

Counting the possible number of super keys for a table is a common question for **GATE**. The examples below will demonstrate all possible types of questions on this topic.

- **Example-1 :** Let a Relation R have attributes {a1,a2,a3} and a1 is the candidate key. Then how many super keys are possible?
    
    Here, any superset of a1 is the super key. Super keys are = {a1, a1 a2, a1 a3, a1 a2 a3} Thus we see that 4 Super keys are possible in this case.
    
    In general, if we have ‘N’ attributes with one candidate key then the number of possible superkeys is 2(N – 1). 
    
- **Example-2 :** Let a Relation R have attributes {a1, a2, a3,…,an}. Find Super key of R. Maximum Super keys = 2 – 1. If each attribute of relation is candidate key.
    
    n
    
- **Example-3:** Let a Relation R have attributes {a1, a2, a3,…, an} and the candidate
key is “a1 a2 a3” then the possible number of super keys?
    
    Following the previous formula, we have 3 attributes instead of one. So, here the number of possible super keys is 2(N-3). 
    
- **Example-4:** Let a Relation R have attributes {a1, a2, a3,…, an} and the candidate
keys are “a1”, “a2” then the possible number of super keys?
    
    This 
    problem now is slightly different since we now have two different 
    candidate keys instead of only one. Tackling problems like these is 
    shown in the diagram below:
    

!https://media.geeksforgeeks.org/wp-content/uploads/1-2-1.png

- → |A1 ∪ A2| = |A1| + |A2| – |A1 ∩ A2|
    
    =
     (super keys possible with candidate key A1) + (super keys possible with
     candidate key A2) – (common superkeys from both A1 and A2)
    
    = 2(n-1) + 2(n-1) – 2(n-2)
    
- **Example-5:** Let a Relation R have attributes {a1, a2, a3,…, an} and the candidate
keys are “a1”, “a2 a3” then the possible number of super keys?
    
    Super keys of (a1) + Super keys of (a2 a3) – Super keys of (a1 a2 a3) ⇒ 2(n – 1) + 2(n – 2) – 2(n – 3)
    
- **Example-6:** Let a Relation R have attributes {a1, a2, a3,…, an} and the candidate
keys are “a1 a2”, “a3 a4” then the possible number of super keys?
    
    Super keys of(a1 a2) + Super keys of(a3 a4) – Super keys of(a1 a2 a3 a4) ⇒ 2(n – 2) + 2(n – 2) – 2(n – 4)
    
- **Example-7:** Let a Relation R have attributes {a1, a2, a3,…, an} and the candidate
keys are “a1 a2”, “a1 a3” then the possible number of super keys?
    
    Super keys of (a1 a2) + Super keys of (a1 a3) – Super keys of(a1 a2 a3) ⇒ 2(n – 2) + 2(n – 2) – 2(n – 3)
    
- **Example-8 :** Let a Relation R have attributes {a1, a2, a3,…,an} and the candidate
keys are “a1”, “a2”, “a3” then the possible number of super keys?
    
    In this question, we have 3 different candidate keys. Tackling problems like these are shown in the diagram below.
    

!https://media.geeksforgeeks.org/wp-content/uploads/2-42.png

- → |A1 ∪ A2 ∪ A3| = |A1| + |A2| + |A3| – |A1 ∩ A2| – |A1 ∩ A3| – |A2 ∩ A3| + |A1 ∩ A2 ∩ A3|
    
    =
     (super keys possible with candidate key A1) + (super keys possible with
     candidate key A2) + (super keys possible with candidate key A3) – 
    (common super keys from both A1 and A2) – (common super keys from both 
    A1 and A3) – (common super keys from both A2 and A3) + (common super 
    keys from both       A1, A2, and A3)
    
    = 2(n-1) + 2(n-1) + 2(n-1) – 2(n-2) – 2(n-2) – 2(n-2) + 2(n-3)
    
- **Example-9:** A relation R (A, B, C, D, E, F, G, H)and set of functional dependencies are CH → G, A → BC, B → CFH, E → A, F → EG Then how many possible super keys are present?
    
    **Step 1:-** First of all, we have to find what the candidate keys are:- as
     we can see in the given functional dependency D is missing but in 
    relation, D is given so D must be a prime attribute of the Candidate 
    key.
    
    A+ = E+ = B+ = F+ = all attributes of a relation except D So, Candidate keys are = AD, BD, ED, FD
    
    **Step 2:-**Find super keys due to a single candidate key there is a two possibility of attribute either we select or not hence there will be 2 chances so, A_ _D_ _ _ _ = _ B_ D_ _ _ _ = _ _ _ DE _ _ _ = _ _ _ D_F_ _ = 26
    
    **Step 3:-**Find superkeys due to a combination of two Candidate Keys.  So,   n(AD ∩ BD) = n(AD ∩ ED) = n(AD ∩ FD) = n(BD ∩ ED) = n(BD ∩ FD) = n(ED ∩ FD) = 25
    
    **Step 4:-**Find super keys due to combination of 3 Candidate Keys So, n(AD ∩ BD ∩ ED) = n(AD ∩ ED ∩ FD) = n(ED ∩ BD ∩ FD) = n(BD ∩ FD ∩ AD) = 24
    
    **Step 5:-**Find super keys due to all, So,   n(AD ∩ BD ∩ ED ∩ FD) = AB_DEF_ _ = 23
    
    So, According to inclusion- exclusion principle :- |W
     ∪ X ∪ Y ∪ Z| = |W| + |X| + |Y| + |Z| – |W ∩ X| – |W ∩ Y| – |W ∩ Z| – |X
     ∩Y| – |X ∩ Z| – |Y ∩ Z| + |W ∩ X ∩ Y| + |W ∩ X ∩ Z| + |W ∩ Y ? Z| + |X ∩
     Y ∩ Z|                   – |W ∩ X ∩ Y ∩ Z|
    
    # Super keys = 4 * 26 – 6 * 25 + 4 * 24 – 23 = 120
    
    So the number of super keys is 120. This explanation is contributed by YaduvanshiRishi.
    
- **Example 10 :** Let a Relation R have attributes {a1,a2,a3______ an} and {a1a2a3____ak} as
the candidate key where k<=n. Then how many super keys are possible?The possible number of superkeys is 2.
    
    (n-k)
    
*** Lossless Decomposition in DBMS
Lossless
 join decomposition is a decomposition of a relation R into relations 
R1, R2 such that if we perform a natural join of relation R1 and R2, it 
will return the original relation R. This is effective in removing 
redundancy from databases while preserving the original data…

In
 other words by lossless decomposition, it becomes feasible to 
reconstruct the relation R from decomposed tables R1 and R2  by using 
Joins.

In Lossless Decomposition, we select the common attribute 
and the criteria for selecting a common attribute is that the common 
attribute must be a candidate key or super key in either relation R1, 
R2, or both.

Decomposition of a relation R into R1 and R2 is a 
lossless-join decomposition if at least one of the following functional 
dependencies are in F+ (Closure of functional dependencies) 

```

R1 ∩ R2 → R1
   OR
R1 ∩ R2 → R2
```

**Question 1:** Let R (A, B, C, D) be a relational schema with the following functional dependencies: 

```
A → B, B → C,
C → D and D → B.

The decomposition of R into
(A, B), (B, C), (B, D)
```

**(A)** gives a lossless join, and is dependency preserving **(B)** gives a lossless join, but is not dependency preserving **(C)** does not give a lossless join, but is dependency preserving **(D)** does not give a lossless join and is not dependency preserving

Refer to [this](https://www.geeksforgeeks.org/gate-gate-it-2008-question-59/) for a solution.

**Question 2** R(A,B,C,D) is a relation. Which of the following does not have a lossless join, dependency preserving BCNF decomposition? (A) A->B, B->CD (B) A->B, B->C, C->D (C) AB->C, C->AD (D) A ->BCD
*** Database Management System | Dependency Preserving Decomposition
**Dependency Preservation**

A Decomposition D = { R1, R2, R3….Rn } of R is dependency preserving wrt a set F of Functional dependency if

```
(F1 ∪ F2 ∪ … ∪ Fm)+ = F+.
Consider a relation R
R ---> F{...with some functional dependency(FD)....}

R is decomposed or divided into R1 with FD { f1 } and R2 with { f2 }, then
there can be three cases:

f1 U f2 = F -----> Decomposition is dependency preserving.
f1 U f2 is a subset of F -----> Not Dependency preserving.
f1 U f2is a super set of F -----> This case is not possible.
```

**Problem:**
 Let a relation R (A, B, C, D ) and functional dependency {AB –> C, C
 –> D, D –> A}. Relation R is decomposed into R1( A, B, C) and 
R2(C, D). Check whether decomposition is dependency preserving or not.**Solution:**

```
R1(A, B, C) and R2(C, D)

Let us find closure of F1 and F2
To find closure of F1, consider all combination of
ABC. i.e., find closure of A, B, C, AB, BC and AC
Note ABC is not considered as it is always ABC

closure(A) = { A }  // Trivial
closure(B) = { B }  // Trivial
closure(C) = {C, A, D} but D can't be in closure as D is not present R1.
           = {C, A}
C--> A   // Removing C from right side as it is trivial attribute

closure(AB) = {A, B, C, D}
            = {A, B, C}
AB --> C  // Removing AB from right side as these are trivial attributes

closure(BC) = {B, C, D, A}
            = {A, B, C}
BC --> A  // Removing BC from right side as these are trivial attributes

closure(AC) = {A, C, D}
AC --> D  // Removing AC from right side as these are trivial attributes

F1 {C--> A, AB --> C, BC --> A}.
Similarly F2 { C--> D }

In the original Relation Dependency { AB --> C , C --> D , D --> A}.
AB --> C is present in F1.
C --> D is present in F2.
D --> A is not preserved.

F1 U F2 is a subset of F. Sogiven decomposition is not dependency preservingg.
```

**Question 1:**Let R (A, B, C, D) be a relational schema with the following functional dependencies:

```
A → B, B → C,
C → D and D → B.

The decomposition of R into
(A, B), (B, C), (B, D)

```

**(A)** gives a lossless join, and is dependency preserving**(B)** gives a lossless join, but is not dependency preserving**(C)** does not give a lossless join, but is dependency preserving**(D)** does not give a lossless join and is not dependency preserving

Refer [this](https://www.geeksforgeeks.org/gate-gate-it-2008-question-59/) for solution.

**Question 2**R(A,B,C,D) is a relation. Which of the following does not have a lossless join, dependency preserving BCNF decomposition?(A) A->B, B->CD(B) A->B, B->C, C->D(C) AB->C, C->AD(D) A ->BCD
*** Lossless Join and Dependency Preserving Decomposition
Decomposition
 of a relation is done when a relation in relational model is not in 
appropriate normal form. Relation R is decomposed into two or more 
relations if decomposition is lossless join as well as dependency 
preserving.

**Lossless Join Decomposition**

If we decompose a relation R into relations R1 and R2,

- Decomposition is lossy if R1 ⋈ R2 ⊃ R
- Decomposition is lossless if R1 ⋈ R2 = R

**To check for lossless join decomposition using FD set, following conditions must hold:**

1. Union of Attributes of R1 and R2 must be equal to attribute of R. Each attribute of R must be either in R1 or in R2.
    
    ```
     Att(R1) U Att(R2) = Att(R)
    ```
    
2. Intersection of Attributes of R1 and R2 must not be NULL.
    
    ```
     Att(R1) ∩ Att(R2) ≠ Φ
    ```
    
3. Common attribute must be a key for at least one relation (R1 or R2)
    
    ```
     Att(R1) ∩ Att(R2) -> Att(R1) or Att(R1) ∩ Att(R2) -> Att(R2)
    ```
    

For
 Example, A relation R (A, B, C, D) with FD set{A->BC} is decomposed 
into R1(ABC) and R2(AD) which is a lossless join decomposition as:

1. First condition holds true as Att(R1) U Att(R2) = (ABC) U (AD) = (ABCD) = Att(R).
2. Second condition holds true as Att(R1) ∩ Att(R2) = (ABC) ∩ (AD) ≠ Φ
3. Third condition holds true as Att(R1) ∩ Att(R2) = A is a key of R1(ABC) because A->BC is given.

**Dependency Preserving Decomposition**

If
 we decompose a relation R into relations R1 and R2, All dependencies of
 R either must be a part of R1 or R2 or must be derivable from 
combination of FD’s of R1 and R2.For Example, A relation R (A, B, C,
 D) with FD set{A->BC} is decomposed into R1(ABC) and R2(AD) which is
 dependency preserving because FD A->BC is a part of R1(ABC).

**GATE
 Question: Consider a schema R(A,B,C,D) and functional dependencies 
A->B and C->D. Then the decomposition of R into R1(AB) and R2(CD) 
is [GATE-CS-2001]**A. dependency preserving and lossless joinB. lossless join but not dependency preservingC. dependency preserving but not lossless joinD. not dependency preserving and not lossless join

**Answer:** For lossless join decomposition, these three conditions must hold true:

1. Att(R1) U Att(R2) = ABCD = Att(R)
2. Att(R1) ∩ Att(R2) = Φ, which violates the condition of lossless join decomposition. Hence the decomposition is not lossless.

For dependency preserving decomposition,A->B can be ensured in R1(AB) and C->D can be ensured in R2(CD). Hence it is dependency preserving decomposition.So, the correct option is C.
*** How to find the highest normal form of a relation
To understand this topic, you should have a basic idea about [Functional Dependency & Candidate keys](https://www.geeksforgeeks.org/finding-attribute-closure-and-candidate-keys-using-functional-dependencies/) and [Normal forms](https://www.geeksforgeeks.org/normal-forms-in-dbms/) .

**Steps to find the highest normal form of relation:**

Hi. In this video, we are going

Remaining Time -6:59

1x

1. Find all possible candidate keys of the relation.
2. Divide all attributes into two categories: prime attributes and non-prime attributes.
3. Check for 1 normal form then 2 and so on. If it fails to satisfy the nnormal form condition, the highest normal form will be n-1.
    
    st
    
    nd
    
    th
    

**Example 1. Find the highest normal form of a relation** R(A,B,C,D,E) with FD set {A->D, B->A, BC->D, AC->BE} 

**Step 1.**   As we can see, (AC)+
 ={A, C, B, E, D}  but none of its subsets can determine all attributes 
of relation, So AC will be the candidate key. A can be derived from B, 
so we can replace A in AC with B. So BC will also be a candidate key. So
 there will be two candidate keys {AC, BC}.

**Step 2.** 
 The prime attribute is those attribute which is part of candidate key 
{A, B, C} in this example and others will be non-prime {D, E} in this 
example.

**Step 3.**  The relation R is in 1st normal form as a relational DBMS does not allow multi-valued or composite attributes.

The relation is not in the 2nd
 Normal form because A->D is partial dependency (A which is a subset 
of candidate key AC is determining non-prime attribute D) and the 2nd normal form does not allow partial dependency.

So the highest normal form will be the 1st Normal Form.

**Example 2.** Find the highest normal form of a relation **R(A,B,C,D,E) with FD set as {BC->D, AC->BE, B->E}**

**Step 1.**   As we can see, (AC)+
 ={A,C,B,E,D}  but none of its subsets can determine all attributes of 
relation, So AC will be the candidate key. A or C can’t be derived from 
any other attribute of the relation, so there will be only 1 candidate 
key {AC}.

**Step 2.**  The prime attribute is those 
attribute which is part of candidate key {A,C} in this example and 
others will be non-prime {B,D,E} in this example.

**Step 3.**  The relation R is in 1st normal form as a relational DBMS does not allow multi-valued or composite attributes.

The relation is in 2nd normal form because BC->D is in 2nd normal form (BC is not a proper subset of candidate key AC) and AC->BE is in 2nd normal form (AC is candidate key) and B->E is in 2nd normal form (B is not a proper subset of candidate key AC).

The relation is not in 3rd
 normal form because in BC->D (neither BC is a super key nor D is a 
prime attribute) and in B->E (neither B is a super key nor E is a 
prime attribute) but to satisfy 3rd normal for, either LHS of an FD should be super key or RHS should be a prime attribute.

So the highest normal form of relation will be the 2nd Normal form.

**Example 3.** Find the highest normal form of a relation **R(A,B,C,D,E)** with **FD set {B->A, A->C, BC->D, AC->BE}**

**Step 1.**   As we can see, (B)+
 ={B,A,C,D,E}, so B will be candidate key. B can be derived from AC 
using AC->B (Decomposing AC->BE to AC->B and AC->E). So AC 
will be super key but (C)+ ={C} and (A)+ ={A,C,B,E,D}. So A (subset of AC) will be candidate key. So there will be two candidate keys {A,B}.

**Step 2.** 
 The prime attribute is those attribute which is part of candidate key 
{A,B} in this example and others will be non-prime {C,D,E} in this 
example.

**Step 3**.  The relation R is in 1st normal form as a relational DBMS does not allow multi-valued or composite attributes.

The relation is in 2nd normal form because B->A is in 2nd normal form (B is a superkey) and A->C is in 2nd normal form (A is super key) and BC->D is in 2nd normal form (BC is a super key) and AC->BE is in 2nd normal form (AC is a super key).

The relation is in 3rd
 normal form because the LHS of all FD’s is super keys. The relation is 
in BCNF as all LHS of all FD’s are super keys. So the highest normal 
form is BCNF.
*** Minimum relations satisfying First Normal Form (1NF)
Prerequisite – [Normal Forms](https://www.geeksforgeeks.org/database-normalization-normal-forms/)The design of database proceeds in a following way:

1. Talk to the stakeholder for which we are designing the database. Get all the requirements, what attributes need to be stored and establish
functional dependencies over the given set of attributes.
2. Draw an Entity-Relationship diagram on the basis of requirements analysis.
3. Convert the ER-diagram into relational model and finally create these relations into our database with appropriate constraints.

We
 pretty much know, how to draw an ER-diagram. But, when we ask questions
 like how many minimum relations would be required satisfying first 
normal form (1NF), it is little confusing sometimes. We establish 
certain simple rules which are formed after deep analysis of each case 
and hence, could be used directly.

**Note –** It is not 
encouraged to mug up the rules, rather understanding the logic behind 
each case would help you remember them easily and for long time.

**Notes –**

1. If there is total participation on both sides;Merge the two entities involved and the relationship into 1 table.
2. Else if, one side is total participation and one side is partial;
    - **M:N** Merge the relationship on total participation side.
    - **1:N** Merge the relationship on total participation side.
    - **1:1** Merge the two entities involved and the relationship into 1 table.
3. else if, both sides are partial participation;
    - **M:N** Separate table for each entity as well as relationship. Hence, 3 tables.
    - **1:N** Merge the relationship on N-side using foreign key referencing 1-side.
    - **1:1** Merge the relationship and one entity into 1 table using foreign key and 1 table for other entity.

Now,
 you would definitely have a question in your mind, how we form such 
rules? This is very easy and logical. Let’s understand the logic behind 
it for one case and you can similarly establish the results for other 
cases too.

We have been given a scenario of 1:N relationship with 
two entities E1(ABC) and E2(DEF), where A and D are primary keys, 
respectively. E1 has a partial participation while E2 has total 
participation in the relationship R.

Based on above scenario, we create certain tuples in E1:

[Untitled Database](https://www.notion.so/7c3d26d0b55a413893e360876c47260a?pvs=21)

Similarly, create certain tuples for E2:

[Untitled Database](https://www.notion.so/8153165fb435434c889e5528bd714f59?pvs=21)

Now,
 create a relationship R satisfying above conditions, i.e. E1 is partial
 participation and E2 is total participation and E1 to E2 is 1:N 
relationship.

[Untitled Database](https://www.notion.so/2292076b89924ab0a5e604278ed863f9?pvs=21)

Think about possibilities, how can we merge?

- **Way-1:** Merge the two entities and relationship into a single table. This is
not correct as (AD) will become primary key for this table, but primary
key can never have a NULL value.
    
    [Untitled Database](https://www.notion.so/b037010f0c43477f95533a874bc3baad?pvs=21)
    
- **Way-2:** Merge relationship on 1-side. This is not correct as (AD) will become
primary key for this table, but primary key can never have a NULL value.
    
    [Untitled Database](https://www.notion.so/e6e3f852326d413db73ee9505047c2e3?pvs=21)
    
- **Way-3:** Merge relationship on N-side. This is correct.
    
    [Untitled Database](https://www.notion.so/0bc1d00d21a644779412643e06cf73ff?pvs=21)
    
    On
     the same grounds, could you think why we allow merging the two entities
     as well as relationship into 1 table, when it is a 1:1 relationship? 
    Simply, we would not have a composite primary key there, so we will 
    definitely have a primary key with no NULL values present in it. Stress 
    some more, why we allow merging the entities and relationship with both 
    sides total participation? The reason is even if we have a composite 
    primary key for such merged table, we are sure that it will never have 
    any NULL values for primary key.
    
    **Note –** You can follow the same procedure as stated above to establish all the results.
   
*** Equivalence of Functional Dependencies
For understanding the equivalence of Functional Dependencies Sets (FD sets), the basic idea about Attribute Closuresis given in [this](https://www.geeksforgeeks.org/finding-attribute-closure-and-candidate-keys-using-functional-dependencies/) article [](https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/12/fd1.png)Given
 a Relation with different FD sets for that relation, we have to find 
out whether one FD set is a subset of another or both are equal. **How do find the relationship between two FD sets?** Let FD1 and FD2 be two FD sets for a relation R.

1. If all FDs of FD1 can be derived from FDs present in FD2, we can say that FD2 ⊃ FD1.
2. If all FDs of FD2 can be derived from FDs present in FD1, we can say that FD1 ⊃ FD2.
3. If 1 and 2 both are true, FD1=FD2.

All these three cases can be shown using the Venn diagram:

!https://media.geeksforgeeks.org/wp-content/uploads/functional-dependencies.png

important topic from gate exam. Point of

Remaining Time -5:28

1x

**Why we need to compare functional Dependencies:**

Suppose
 in the designing process we convert the ER diagram to a relational 
model and this task is given to two different engineers. Now those two 
engineers give two different sets of functional dependencies. So, being 
an administrator we need to ensure that we must have a good set of 
Functional Dependencies. To ensure this we require to study the 
equivalence of Functional Dependencies.

**Q.
 Let us take an example to show the relationship between two FD sets. A 
relation R(A,B,C,D) having two FD sets FD1 = {A->B, B->C, 
AB->D} and FD2 = {A->B, B->C, A->C, A->D}**

**Step 1.**  Checking whether all FDs of FD1 are present in FD2

- A->B in set FD1 is present in set FD2.
- B->C in set FD1 is also present in set FD2.
- AB->D in present in set FD1 but not directly in FD2 but we will check whether we can derive it or not. For set FD2, (AB)= {A,B,C,D}. It means that AB can functionally determine A, B, C, and D. So AB->D will also hold in set FD2.
    
    +
    

As all FDs in set FD1 also hold in set FD2, FD2 ⊃ FD1 is true.

**Step 2.**  Checking whether all FDs of FD2 are present in FD1

- A->B in set FD2 is present in set FD1.
- B->C in set FD2 is also present in set FD1.
- A->C is present in FD2 but not directly in FD1 but we will check whether we can derive it or not. For set FD1, (A)= {A,B,C,D}. It means that A can functionally determine A, B, C, and D. SO A->C will also hold in set FD1.
    
    +
    
- A->D is present in FD2 but not directly in FD1 but we will check whether we can derive it or not. For set FD1, (A)= {A,B,C,D}. It means that A can functionally determine A, B, C, and D. SO A->D will also hold in set FD1.
    
    +
    

As all FDs in set FD2 also hold in set FD1, FD1 ⊃ FD2 is true.

**Step 3.** As FD2 ⊃ FD1 and FD1 ⊃ FD2 both are true FD2 =FD1 is true. These two FD sets are semantically equivalent.

**Q.
 Let us take another example to show the relationship between two FD 
sets. A relation R2(A,B,C,D) having two FD sets FD1 = {A->B, 
B->C,A->C} and FD2 = {A->B, B->C, A->D}**

**Step 1.**  Checking whether all FDs of FD1 are present in FD2

- A->B in set FD1 is present in set FD2.
- B->C in set FD1 is also present in set FD2.
- A->C is present in FD1 but not directly in FD2 but we will check whether we can derive it or not. For set FD2, (A)= {A,B,C,D}. It means that A can functionally determine A, B, C, and D. SO A->C will also hold in set FD2.
    
    +
    

As all FDs in set FD1 also hold in set FD2, FD2 ⊃ FD1 is true.

**Step 2.**  Checking whether all FDs of FD2 are present in FD1

- A->B in set FD2 is present in set FD1.,
- B->C in set FD2 is also present in set FD1.
- A->D is present in FD2 but not directly in FD1 but we will check whether we can derive it or not. For set FD1, (A)= {A,B,C}. It means that A can’t functionally determine D. SO A->D will not hold in FD1.
    
    +
    

As all FDs in set FD2 do not hold in set FD1, FD2 ⊄ FD1.

**Step 3.** In this case, FD2 ⊃ FD1 and FD2 ⊄ FD1, these two FD sets are not semantically equivalent.

https://youtu.be/WD4xfGDCTmQ
*** Armstrong’s Axioms in Functional Dependency in DBMS
**Prerequisite –** [Functional Dependencies](https://www.geeksforgeeks.org/database-normalization-introduction/)

The
 term Armstrong axioms refer to the sound and complete set of inference 
rules or axioms, introduced by William W. Armstrong, that is used to 
test the logical implication of

**functional dependencies**

. If F is a set of functional dependencies then the closure of F, denoted as

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-43bfd74314135633bcb7e6f043190e14_l3.svg

,
 is the set of all functional dependencies logically implied by F. 
Armstrong’s Axioms are a set of rules, that when applied repeatedly, 
generates a closure of functional dependencies.

### Axioms –

1. **Axiom of reflexivity –** If is a set of attributes and is subset of , then holds . If then This property is trivial property.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1e7d52a5c08cbc4970334ad68ef8e54e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-49b63c4da9f48b949d01f2ff9293abdf_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1e7d52a5c08cbc4970334ad68ef8e54e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1e7d52a5c08cbc4970334ad68ef8e54e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-49b63c4da9f48b949d01f2ff9293abdf_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-bcda4ecf532f3201ba21dfb7e137b162_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
2. **Axiom of augmentation –** If holds and is attribute set, then also holds. That is adding attributes in dependencies, does not change the basic dependencies. If , then for any .
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-16d6aa64a3f2142274fb5721b4ca88dc_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-99fcf4e80db68bd10c573dbac6f71cb6_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a19c7240eb66fee0232d3f6ce200d770_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d079fc721a5da6ef20f7a9d949fe0f0b_l3.svg
    
3. **Axiom of transitivity –** Same as the transitive rule in algebra, if holds and holds, then also holds. is called as functionally that determines . If and , then
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b487cdd008c75bccdde9fc571bb4421d_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7268c13c66e20af7f483fa85917162be_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1e7d52a5c08cbc4970334ad68ef8e54e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-49b63c4da9f48b949d01f2ff9293abdf_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-30337ce28f1f2981d17c3a9af4a23104_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2576474544d64405cdd5d7772c295934_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7d41c900231655f15988435343f0dcee_l3.svg
    

### Secondary Rules –

These rules can be derived from the above axioms.

1. **Union –** If holds and holds, then holds. If and then
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7268c13c66e20af7f483fa85917162be_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1b696fc0073bae9ec1ef56b38c93953e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-30337ce28f1f2981d17c3a9af4a23104_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ba05a56dccd08e1d39ac40ce746ea5d3_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-089c673065f45801b99e76c426fdcf3f_l3.svg
    
2. **Composition –** If and holds, then holds.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-30337ce28f1f2981d17c3a9af4a23104_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-0f5f8240b1e19fb312e84079dd25f515_l3.svg
    
3. **Decomposition –** If holds then and hold. If then and
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-1b696fc0073bae9ec1ef56b38c93953e_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-7268c13c66e20af7f483fa85917162be_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a2e9387caf928e9a5637ead77494fac4_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-30337ce28f1f2981d17c3a9af4a23104_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-26404e733f0118bd35052bd3a8d7c18b_l3.svg
    
4. **Pseudo Transitivity –** If holds and holds, then holds. If and then .
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e86f7dc10f63d367c95fbeee20488bac_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-067f7fbf1918f9b8ba66de3a3fb49636_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a44016c2f79beb3aa423d27c11af3317_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-30337ce28f1f2981d17c3a9af4a23104_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-e1ece3b1b8c5b431b0b9d35efdd13614_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-fdb8c2d24505bbb056f344cd72d42223_l3.svg
    

**Why armstrong axioms refer to the Sound and Complete ?** By
 sound, we mean that given a set of functional dependencies F specified 
on a relation schema R, any dependency that we can infer from F by using
 the primary rules of Armstrong axioms holds in every relation state r 
of R that satisfies the dependencies in F. By complete, we mean that
 using primary rules of Armstrong axioms repeatedly to infer 
dependencies until no more dependencies can be inferred results in the 
complete set of all possible dependencies that can be inferred from F.

**References –**

- Book – [Fundamentals of Database Systems](https://amzn.to/3j834dt)
- [http://tinman.cs.gsu.edu](http://tinman.cs.gsu.edu/~raj/4710/sp08/fd-theory.pdf)
*** Canonical Cover of Functional Dependencies in DBMS
Whenever
 a user updates the database, the system must check whether any of the 
functional dependencies are getting violated in this process. If there 
is a violation of dependencies in the new database state, the system 
must roll back. Working with a huge set of [functional dependencies](https://www.geeksforgeeks.org/functional-dependency-and-attribute-closure/) can cause unnecessary added computational time. This is where the canonical cover comes into play.A
 canonical cover of a set of functional dependencies F is a simplified 
set of functional dependencies that has the same closure as the original
 set F.

**Important definitions:**

**Extraneous attributes:**

An attribute of a functional dependency is said to be extraneous if we 
can remove it without changing the closure of the set of functional 
dependencies.

**Canonical cover:**

A canonical cover

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg

of a set of functional dependencies F such that ALL the following properties are satisfied:

- F logically implies all dependencies in .
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    
- logically implies all dependencies in F.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    
- No functional dependency in  contains an extraneous attribute.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    
- Each left side of a functional dependency in  is unique. That is, there are no two dependencies  and  in such that .
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-161a5786720d93cf51f382c13358e709_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a8f93a07a961100132da363173e41e5c_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-19ab787ab12d3ef0c03d102500eac25a_l3.svg
    

**Finding Canonical Cover**

**Algorithm to compute canonical cover of set F:**

```
repeat
    1. Use the union rule to replace any dependencies in
 and with.
    2. Find a functional dependency with an
       extraneous attribute either in or in.
    3. If an extraneous attribute is found, delete it from.
until F does not change
```

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-161a5786720d93cf51f382c13358e709_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b12e7f6783191c4459d96de9a3fd8681_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-b92bbe32de17f7a89bc5ce05d645a7a0_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-f59cd24599b7fd30932cba3702f8fe30_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-091e0ab996bb6a7f0e720b84e9aee36f_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-59d0659f4d0bd914215c6c4f36a5aed9_l3.svg

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-f59cd24599b7fd30932cba3702f8fe30_l3.svg

**Example1:**

Consider the following set *F* of functional dependencies:

F= {

A

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

BC

B

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

C

A

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

B

AB

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

C

}

Steps to find canonical cover:

1. There are two functional dependencies with the same set of attributes on the left:A  BCA  BThese two can be combined to getA  BC.F= {A  BCB  CAB  C}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    Now, the revised set F becomes:
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
2. There is an extraneous attribute in AB  C because even after removing AB  C from the set F, we get the same closures. This is because B  C is already a part of F.F= {A  BCB  C}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    Now, the revised set F becomes:
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
3. C is an extraneous attribute in A  BC, also A  B is logically implied by A  B and B  C (by transitivity).F= {A  BB  C}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
4. After this step, F does not change anymore. So,Hence the required canonical cover is,= {A  BB  C}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    

**Example2:**

Consider another set F of functional dependencies:

F={

A

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

BC

CD

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

E

B

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

D

E

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

A

}

1. The left side of each functional dependency in F is unique.
2. None of the attributes in the left or right side of any functional
dependency is extraneous (Checked by applying definition of extraneous
attributes on every functional dependency).
3. Hence, the canonical cover  is equal to F.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg
    

**Note:**

There can be more than one canonical cover

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a778e923b4cf4ef49092d739043f1c1d_l3.svg

of a set F of functional dependencies.

**How to check whether a set of f.d.’s F canonically cover another set of f.d.’s G?**

Consider the following two sets of functional dependencies:

F = {

A

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

B

AB

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

C

D

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

AC

D

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

E

}G = {

A

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

BC

D

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg

AB

}

Now,
 we are required to find out whether one of these f.d.’s canonically 
covers the other set of f.d.’s. This means, we need to find out whether F
 canonically covers G, G canonically covers F, or none of the two 
canonically cover the other.

To find out, we follow the following steps:

- Create a singleton right hand side. This means, the attributes to the right side of the f.d. arrow should all be singleton.The functional dependency D  AC gets broken down into two functional dependencies, D  A and D  C.F = {A  BAB  CD  AD  CD  E}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
- Remove all extraneous attributes.Consider any functional dependency XY  Z. If X in itself can determine Z, then the attribute Y is extraneous
and can be removed. As we can see, the occurrence of extraneous
attributes is possible only in those functional dependencies where there are more than one attributes in the LHS.So, consider the functional dependency AB  C.Now, we must find the closures of A and B to find whether any of these is extraneous.=AB=BAs we can see, B can be determined from A. This means we can remove B from the functional dependency AB  C.F = {A  BA  CD  AD  CD  E}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-f61bcdc6e7e0251136b31f80f5e220e1_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-72cae375578ee87fe234fa4f67b3c7e2_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
- Remove all redundant functional dependencies.Check all f.d.’s one by one, and see if by removing a f.d. X  Y, we can still find out Y from X by some other f.d. A more formal way to state this is find  without making use of the f.d. we are testing and check whether Y is a part of the closure. If yes, then the f.d. is redundant.Here, when checking for the f.d. D  C, we observe that even after hiding it, the closure of D contains C.
This is because we can obtain C from D by the combination of two other
f.d.’s D  A and A  C. So,  C is redundant.F = {A  BA  CD  AD  E}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-fb98dbac3a7d3685a95ea5ddfa5a7d13_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    

Now, do the same for G.

- Create a singleton right hand side. This means, the attributes to the right side of the f.d. arrow should all be singleton.G = {A  BA  CD  AD  B}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
- Remove all extraneous attributes.Since the RHS of all f.d.’s contains only 1 attribute, there is no extraneous attribute possible.
- Remove all redundant functional dependencies.By looping over all f.d.’s and checking the closure of the LHS in all cases, we observe that the f.d. D  B is redundant as it can be obtained through a combination of 2 other f.d.’s, D  A and A  B.G = {A  BA  CD  A}
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-d18f05af053fa3533a76ea36fcd085ef_l3.svg
    

Now, since all f.d.’s of G are already covered in F, we conclude that F covers G.
*** Denormalization in Databases
Denormalization
 is a database optimization technique in which we add redundant data to 
one or more tables. This can help us avoid costly joins in a relational 
database. Note that *denormalization* does not mean ‘reversing 
normalization’ or ‘not to normalize’. It is an optimization technique 
that is applied after normalization.

Basically,
 The process of taking a normalized schema and making it non-normalized 
is called denormalization, and designers use it to tune the performance 
of systems to support time-critical operations.

In a traditional 
normalized database, we store data in separate logical tables and 
attempt to minimize redundant data. We may strive to have only one copy 
of each piece of data in a database.

For example, in a normalized 
database, we might have a Courses table and a Teachers table. Each entry
 in Courses would store the teacherID for a Course but not the 
teacherName. When we need to retrieve a list of all Courses with the 
Teacher’s name, we would do a join between these two tables.

In some ways, this is great; if a teacher changes his or her name, we only have to update the name in one place. The drawback is that if tables are large, we may spend an unnecessarily long time doing joins on tables. Denormalization,
 then, strikes a different compromise. Under denormalization, we decide 
that we’re okay with some redundancy and some extra effort to update the
 database in order to get the efficiency advantages of fewer joins.

**Pros of Denormalization:**

1. Retrieving data is faster since we do fewer joins
2. Queries to retrieve can be simpler(and therefore less likely to have bugs), since we need to look at fewer tables.

**Cons of Denormalization:**

1. Updates and inserts are more expensive.
2. Denormalization can make *update* and *insert* code harder to write.
3. Data may be inconsistent.
4. Data redundancy necessitates more storage.

In
 a system that demands scalability, like that of any major tech company,
 we almost always use elements of both normalized and denormalized 
databases.
*** Introduction of 4th and 5th Normal form in DBMS
Prerequisite – [Functional Dependency](https://www.geeksforgeeks.org/functional-dependency-and-attribute-closure/), [Database Normalization](https://www.geeksforgeeks.org/database-normalization-introduction/), [Normal Forms](https://www.geeksforgeeks.org/database-normalization-normal-forms/) If two or more independent relation are kept in a single relation or we can say **multivalue dependency**
 occurs when the presence of one or more rows in a table implies the 
presence of one or more other rows in that same table. Put another way, 
two attributes (or columns) in a table are independent of one another, 
but both depend on a third attribute. A **multivalued dependency** always requires at least three attributes because it consists of at least two attributes that are dependent on a third.

For
 a dependency A -> B, if for a single value of A, multiple value of B
 exists, then the table may have multi-valued dependency. The table 
should have at least 3 attributes and B and C should be independent for A
 ->> B multivalued dependency. For example,

[Untitled Database](https://www.notion.so/394687d776214397af5ef2e2d7d075ed?pvs=21)

```
Person->-> mobile,
Person ->-> food_likes
```

This is read as “person multidetermines mobile” and “person multidetermines food_likes.”

Note
 that a functional dependency is a special case of multivalued 
dependency. In a functional dependency X -> Y, every x determines 
exactly one y, never more than one.

### Fourth normal form (4NF):

Fourth
 normal form (4NF) is a level of database normalization where there are 
no non-trivial multivalued dependencies other than a candidate key. It 
builds on the first three normal forms (1NF, 2NF and 3NF) and the 
Boyce-Codd Normal Form (BCNF). It states that, in addition to a database
 meeting the requirements of BCNF, it must not contain more than one 
multivalued dependency.

**Properties –** A relation R is in 4NF if and only if the following conditions are satisfied: 

1. It should be in the Boyce-Codd Normal Form (BCNF).
2. the table should not have any Multi-valued Dependency.

A
 table with a multivalued dependency violates the normalization standard
 of Fourth Normal Form (4NK) because it creates unnecessary redundancies
 and can contribute to inconsistent data. To bring this up to 4NF, it is
 necessary to break this information into two tables.

**Example –**
 Consider the database table of a class which has two relations R1 
contains student ID(SID) and student name (SNAME) and R2 contains course
 id(CID) and course name (CNAME).

**Table –** R1(SID, SNAME)

[Untitled Database](https://www.notion.so/91f12bd7e6da4614aa88c00b055fd341?pvs=21)

**Table –** R2(CID, CNAME) 

[Untitled Database](https://www.notion.so/cc24454df77548709779e3785b48af0a?pvs=21)

When there cross product is done it resulted in multivalued dependencies:

**Table –** R1 X R2

[Untitled Database](https://www.notion.so/7ec64d86b64f42eeadb58fe1ff29e653?pvs=21)

Multivalued dependencies (MVD) are: 

```
 SID->->CID; SID->->CNAME; SNAME->->CNAME
```

**Joint dependency –**
 Join decomposition is a further generalization of Multivalued 
dependencies. If the join of R1 and R2 over C is equal to relation R 
then we can say that a join dependency (JD) exists, where R1 and R2 
are the decomposition R1(A, B, C) and R2(C, D) of a given relations R 
(A, B, C, D). Alternatively, R1 and R2 are a lossless decomposition of 
R. A JD ⋈ {R1, R2, …, Rn} is said to hold over a relation R if R1, R2, 
….., Rn is a lossless-join decomposition. The *(A, B, C, D), (C, D) will
 be a JD of R if the join of join’s attribute is equal to the relation R. Here, *(R1, R2, R3) is used to indicate that relation R1, R2, R3 and so on are a JD of R.

Let R is a relation schema R1, R2, R3……..Rn be the decomposition of R. r( R ) is said to satisfy join dependency if and only if

!https://media.geeksforgeeks.org/wp-content/uploads/111-4.jpg

**Example –**

**Table –** R1

[Untitled Database](https://www.notion.so/33164de9bd0d4372bfef3d194a4b9ac6?pvs=21)

```
Company->->Product
```

**Table –** R2

[Untitled Database](https://www.notion.so/f31123aaf25e4abbb77b10f9ef3684e7?pvs=21)

```
Agent->->Company
```

**Table –** R3

[Untitled Database](https://www.notion.so/daeb39e020884076bda46bb8dc987c89?pvs=21)

```
Agent->->Product
```

**Table –** R1⋈R2⋈R3

[Untitled Database](https://www.notion.so/53a6f14777664ad5830d47cc81460181?pvs=21)

```
Agent->->Product
```

### Fifth Normal Form / Projected Normal Form (5NF):

A
 relation R is in 5NF if and only if every join dependency in R is 
implied by the candidate keys of R. A relation decomposed into two 
relations must have loss-less join Property, which ensures that no 
spurious or extra tuples are generated, when relations are reunited 
through a natural join.

**Properties –** A relation R is in 5NF if and only if it satisfies following conditions: 

1. R should be already in 4NF. 
2. It cannot be further non loss decomposed (join dependency)

**Example –**
 Consider the above schema, with a case as “if a company makes a product
 and an agent is an agent for that company, then he always sells that 
product for the company”. Under these circumstances, the ACP table is 
shown as:

**Table –** ACP

[Untitled Database](https://www.notion.so/50603f8bd8ce409b8a225923e357d309?pvs=21)

The relation ACP is again decompose into 3 relations. Now, the natural Join of all the three relations will be shown as:

**Table –** R1

[Untitled Database](https://www.notion.so/d5c69aeb3c174f6aa0cc23e13cbafba7?pvs=21)

**Table –** R2 

[Untitled Database](https://www.notion.so/f2035fe5af9f456d92725c5089950290?pvs=21)

**Table –** R3 

[Untitled Database](https://www.notion.so/1bd13225936f4a009156236127d71f2f?pvs=21)

Result of Natural Join of R1 and R3 over ‘Company’ and then Natural Join of R13 and R2 over ‘Agent’and ‘Product’ will be table **ACP**.

Hence,
 in this example, all the redundancies are eliminated, and the 
decomposition of ACP is a lossless join decomposition. Therefore, the 
relation is in 5NF as it does not violate the property of [lossless join](https://www.geeksforgeeks.org/database-management-system-lossless-decomposition/).
** Query languages (SQL)
*** Structured Query Language (SQL)
Structured
 Query Language is a standard Database language which is used to create,
 maintain and retrieve the relational database. Following are some 
interesting facts about SQL.

- SQL is case insensitive. But it is a recommended practice to use keywords
(like SELECT, UPDATE, CREATE, etc) in capital letters and use user
defined things (liked table name, column name, etc) in small letters.
- We can write comments in SQL using “–” (double hyphen) at the beginning of any line.
- SQL is the programming language for relational databases (explained below)
like MySQL, Oracle, Sybase, SQL Server, Postgre, etc. Other
non-relational databases (also called NoSQL) databases like MongoDB,
DynamoDB, etc do not use SQL
- Although there is an ISO standard
for SQL, most of the implementations slightly vary in syntax. So we may
encounter queries that work in SQL Server but do not work in MySQL.

.

**What is Relational Database?**

Relational
 database means the data is stored as well as retrieved in the form of 
relations (tables). Table 1 shows the relational database with only one 
relation called **STUDENT** which stores **ROLL_NO**, **NAME**, **ADDRESS**, **PHONE** and **AGE** of students.

**STUDENT**

[Untitled Database](https://www.notion.so/3753f99dfdeb4a7789172c2b86eece22?pvs=21)

**TABLE 1**

These are some important terminologies that are used in terms of relation.

**Attribute:** Attributes are the properties that define a relation. e.g.; **ROLL_NO**, **NAME** etc.

**Tuple:** Each row in the relation is known as tuple. The above relation contains 4 tuples, one of which is shown as:

**Degree:** The number of attributes in the relation is known as degree of the relation. The **STUDENT** relation defined above has degree 5.

**Cardinality:** The number of tuples in a relation is known as cardinality. The **STUDENT** relation defined above has cardinality 4.

**Column:** Column represents the set of values for a particular attribute. The column **ROLL_NO** is extracted from relation STUDENT.

**ROLL_NO**

---

---

---

---

---

The queries to deal with relational database can be categories as:

**Data Definition Language:** It is used to define the structure of the database. e.g; CREATE TABLE, ADD COLUMN, DROP COLUMN and so on.

**Data Manipulation Language:** It is used to manipulate data in the relations. e.g.; INSERT, DELETE, UPDATE and so on.

**Data Query Language:** It is used to extract the data from the relations. e.g.; SELECT

So first we will consider the Data Query Language. A generic query to retrieve from a relational database is:

1. **SELECT** [**DISTINCT**] Attribute_List **FROM** R1,R2….RM
2. [**WHERE** condition]
3. [**GROUP BY** (Attributes)[**HAVING** condition]]
4. [**ORDER BY**(Attributes)[**DESC**]];

Part
 of the query represented by statement 1 is compulsory if you want to 
retrieve from a relational database. The statements written inside [] 
are optional. We will look at the possible query combination on relation
 shown in Table 1.

**Case 1:** If we want to retrieve attributes **ROLL_NO** and **NAME** of all students, the query will be:

```
SELECT ROLL_NO, NAMEFROM STUDENT;
```

[Untitled Database](https://www.notion.so/8860fa37a82f462595e9fb7f7aebfbb5?pvs=21)

**Case 2:** If we want to retrieve **ROLL_NO** and **NAME** of the students whose **ROLL_NO** is greater than 2, the query will be:

```
SELECT ROLL_NO, NAMEFROM STUDENT
WHEREROLL_NO>2;
```

[Untitled Database](https://www.notion.so/2ba256b203514a1bb97a883f7881a625?pvs=21)

**CASE 3:** If we want to retrieve all attributes of students, we can write * in place of writing all attributes as:

```
SELECT* FROM STUDENT
WHERE ROLL_NO>2;
```

[Untitled Database](https://www.notion.so/e925dd102b6843d4b1d26dbd836406e9?pvs=21)

**CASE 4:** If we want to represent the relation in ascending order by **AGE**, we can use ORDER BY clause as:

```
SELECT * FROM STUDENTORDER BY AGE;
```

[Untitled Database](https://www.notion.so/34e9a72175cb4cd197bd83c613c2c340?pvs=21)

**Note:** ORDER BY **AGE** is equivalent to ORDER BY **AGE** ASC. If we want to retrieve the results in descending order of **AGE**, we can use ORDER BY **AGE** DESC.

**CASE 5:** If we want to retrieve distinct values of an attribute or group of attribute, DISTINCT is used as in:

```
SELECT DISTINCT ADDRESSFROMSTUDENT;
```

**ADDRESS**

---

---

---

---

If
 DISTINCT is not used, DELHI will be repeated twice in result set. 
Before understanding GROUP BY and HAVING, we need to understand 
aggregations functions in SQL.

**AGGRATION FUNCTIONS:** Aggregation
 functions are used to perform mathematical operations on data values of
 a relation. Some of the common aggregation functions used in SQL are:

- **COUNT:** Count function is used to count the number of rows in a relation. e.g;

```
SELECTCOUNT (PHONE)FROM STUDENT;
```

**COUNT(PHONE)**

---

---

- **SUM:** SUM function is used to add the values of an attribute in a relation. e.g;

**SELECT** **SUM** (AGE) **FROM** STUDENT;

**SUM(AGE)**

---

---

In the same way, MIN, MAX and AVG can be used.  As we have seen above, all aggregation functions return only 1 row.

AVERAGE: It gives the average values of the tupples. It is also defined as sum divided by count values.Syntax:AVG(attributename)ORSyntax:SUM(attributename)/COUNT(attributename)The above mentioned syntax also retrieves the average value of tupples.

MAXIMUM:It extracts the maximum value among the set of tupples.Syntax:MAX(attributename)

MINIMUM:It extracts the minimum value amongst the set of all the tupples.Syntax:MIN(attributename)

**GROUP BY:**
 Group by is used to group the tuples of a relation based on an 
attribute or group of attribute. It is always combined with aggregation 
function which is computed on group. e.g.;

```
SELECT ADDRESS,SUM(AGE)FROM STUDENT
GROUP BY (ADDRESS);
```

In this query, SUM(**AGE**)
 will be computed but not for entire table but for each address. i.e.; 
sum of AGE for address DELHI(18+18=36) and similarly for other address 
as well. The output is:

[Untitled Database](https://www.notion.so/f302d30480834cd0baa754269eeb5342?pvs=21)

If
 we try to execute the query given below, it will result in error 
because although we have computed SUM(AGE) for each address, there are 
more than 1 ROLL_NO for  each address we have grouped. So it can’t be 
displayed in result set. We need to use aggregate functions on columns 
after SELECT statement to make sense of the resulting set whenever we 
are using GROUP BY.

```
SELECT ROLL_NO, ADDRESS,SUM(AGE)FROM STUDENT
GROUP BY (ADDRESS);
```

**NOTE:** An 
attribute which is not a part of GROUP BY clause can’t be used for 
selection. Any attribute which is part of GROUP BY CLAUSE can be used 
for selection but it is not mandatory. But we could use attributes which
 are not a part of the GROUP BY clause in an aggregrate function.**[Quiz on SQL](https://www.geeksforgeeks.org/dbms-gq/sql-gq/)**
 
*** Inner Join vs Outer Join
An 
SQL Join is used to combine data from two or more tables based on a 
common field between them. For example, consider the following two 
tables.

**Student Table**

[Untitled Database](https://www.notion.so/0a517a6eaf234d938aca253542482908?pvs=21)

**StudentCourse Table**

[Untitled Database](https://www.notion.so/6bb824c00dd74d46aa31199369fa0dee?pvs=21)

### Inner Join / Simple join:

In an INNER join, it allows retrieving data from two tables with the same ID.

**Syntax**:

**SELECT COLUMN1, COLUMN2 FROM**

**[TABLE 1] INNER JOIN [TABLE 2]**

**ON Condition;**

The following is a join query that shows the names of students enrolled in different courseIDs.

```
SELECT StudentCourse.CourseID,Student.StudentName
FROM Student
INNER JOIN StudentCourse
ON StudentCourse.EnrollNo = Student.EnrollNo
ORDER BY StudentCourse.CourseID;
```

**Note**: INNER is optional above.  Simple JOIN is also considered as INNER JOIN The above query would produce following result.

[Untitled Database](https://www.notion.so/771a411a0de84b4e86f7a34ccb26fce9?pvs=21)

### What is the difference between inner join and outer join?

Outer Join is of three types:

1. Left outer join
2. Right outer join
3. Full Join

**1. Left outer join**
 returns all rows of a table on the left side of the join. For the rows 
for which there is no matching row on the right side, the result 
contains NULL on the right side.

**Syntax:**

**SELECT  T1.C1, T2.C2**

**FROM TABLE T1**

**LEFT JOIN TABLE T2**

**ON T1.C1= T2.C1;**

```
SELECT Student.StudentName,StudentCourse.CourseID
FROM Student
LEFT OUTER JOIN StudentCourse
ON StudentCourse.EnrollNo = Student.EnrollNo
ORDER BY StudentCourse.CourseID;
```

**Note**: OUTER is optional above. Simple LEFT JOIN is also considered as LEFT OUTER JOIN

[Untitled Database](https://www.notion.so/529d496d99e743108cc6137dcf9dbd81?pvs=21)

**2. Right Outer Join** is similar to Left Outer Join (Right replaces Left everywhere).

**Syntax:**

**SELECT T1.C1, T2.C2**

**FROM TABLE T1**

**RIGHT JOIN TABLE T2**

**ON T1.C1= T2.C1;**

**Example:**

SELECT Student.StudentName, StudentCourse.CourseID

FROM Student

RIGHT OUTER JOIN StudentCourse

ON StudentCourse.EnrollNo = Student.EnrollNo

ORDER BY StudentCourse.CourseID;

**3. Full Outer Join** contains
 the results of both the Left and Right outer joins. It is also known as
 cross join. It will provide a mixture of two tables.

**Syntax:**

**SELECT * FROM T1**

**CROSS JOIN T2;**

Please write comments if you find anything incorrect, or if you want to share more information about the topic discussed above
*** Having vs Where Clause in SQL
The difference between the having and where clause in SQL is that the where clause cann*ot* be used with aggregates, but the having clause can.

The **where** clause works on row’s data, not on aggregated data.  Let us consider below table ‘Marks’.

**Student       Course      Score**

a                c1             40

a                c2             50

b                c3             60

d                c1             70

e                c2             80

Consider the query

`SELECT Student, Score FROM Marks WHERE Score >=40`

This would select data row by row basis.

The **having** clause works on aggregated data.

For example,  output of below query

`SELECT Student, SUM(score) AS total FROM Marks GROUP BY Student`

**Student     Total**

a              90

b               60

d               70

e               80

When we apply having in above query, we get

`SELECT Student, SUM(score) AS total FROM Marks GROUP BY Student`

`HAVING total > 70`

**Student     Total**

a              90

e               80

Note: 
 It is not a predefined rule but  in a good number of the SQL queries, 
we use WHERE prior to GROUP BY and HAVING after GROUP BY. The Where 
clause acts as a **pre filter** where as Having as a **post filter.**

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Database Objects in DBMS
A **database object** is any defined object in a database that is used to store or reference data.Anything which we make from **create command** is
 known as Database Object.It can be used to hold and manipulate the 
data.Some of the examples of database objects are : view, sequence, 
indexes, etc.

- **Table –** Basic unit of storage; composed rows and columns
- **View –** Logically represents subsets of data from one or more tables
- **Sequence –** Generates primary key values
- **Index –** Improves the performance of some queries
- **Synonym –** Alternative name for an object

Different database Objects :

1. **Table –** This database object is used to create a table in database.
    
    **Syntax :**
    
    ```
    CREATE TABLE [schema.]table
                   (column datatype [DEFAULT expr][, ...]);
    ```
    
    **Example :**
    
    ```
    CREATE TABLE dept
               (deptno NUMBER(2),
                dname VARCHAR2(14),
                loc VARCHAR2(13));
    ```
    
    **Output :**
    
    ```
    DESCRIBE dept;
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/table-output.png
    
2. **View –** This database object is used to create a view in database.A view is a
logical table based on a table or another view. A view contains no data
of its own but is like a window through which data from tables can be
viewed or changed. The tables on which a view is based are called base
tables. The view is stored as a SELECT statement in the data dictionary.
    
    **Syntax :**
    
    ```
    CREATE [OR REPLACE] [FORCE|NOFORCE] VIEW view
                           [(alias[, alias]...)]
                           AS subquery
                           [WITH CHECK OPTION [CONSTRAINT constraint]]
                           [WITH READ ONLY [CONSTRAINT constraint]];
    ```
    
    **Example :**
    
    ```
    CREATE VIEW salvu50
                   AS SELECT employee_id ID_NUMBER, last_name NAME,
                   salary*12 ANN_SALARY
                   FROM employees
                   WHERE department_id = 50;
    ```
    
    **Output :**
    
    ```
    SELECT *
    FROM salvu50;
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/view-output.png
    
3. **Sequence –** This database object is used to create a sequence in database.A
sequence is a user created database object that can be shared by
multiple users to generate unique integers. A typical usage for
sequences is to create a primary key value, which must be unique for
each row.The sequence is generated and incremented (or decremented) by
an internal Oracle routine.
    
    **Syntax :**
    
    ```
    CREATE SEQUENCE sequence
                        [INCREMENT BY n]
                        [START WITH n]
                        [{MAXVALUE n | NOMAXVALUE}]
                        [{MINVALUE n | NOMINVALUE}]
                        [{CYCLE | NOCYCLE}]
                        [{CACHE n | NOCACHE}];
    ```
    
    **Example :**
    
    ```
    CREATE SEQUENCE dept_deptid_seq
                            INCREMENT BY 10
                            START WITH 120
                            MAXVALUE 9999
                            NOCACHE
                            NOCYCLE;
    ```
    
    **Check if sequence is created by :**
    
    ```
    SELECT sequence_name, min_value, max_value,
                           increment_by, last_number
                           FROM   user_sequences;
    ```
    
4. **Index –** This database object is used to create a indexes in database.An Oracle
server index is a schema object that can speed up the retrieval of rows
by using a pointer.Indexes can be created explicitly or automatically.
If you do not have an index on the column, then a full table scan
occurs.
    
    An index provides direct and fast access to rows in a table. 
    Its purpose is to reduce the necessity of disk I/O by using an indexed 
    path to locate data quickly. The index is used and maintained 
    automatically by the Oracle server. Once an index is created, no direct 
    activity is required by the user.Indexes are logically and physically 
    independent of the table they index. This means that they can be created
     or dropped at any time and have no effect on the base tables or other 
    indexes.
    
    **Syntax :**
    
    ```
    CREATE INDEX index
                ON table (column[, column]...);
    ```
    
    **Example :**
    
    ```
    CREATE INDEX emp_last_name_idx
                    ON  employees(last_name);
    ```
    
5. **Synonym –** This database object is used to create a indexes in database.It
simplify access to objects by creating a synonym(another name for an
object). With synonyms, you can Ease referring to a table owned by
another user and shorten lengthy object names.To refer to a table owned
by another user, you need to prefix the table name with the name of the
user who created it followed by a period. Creating a synonym eliminates
the need to qualify the object name with the schema and provides you
with an alternative name for a table, view, sequence,procedure, or other objects. This method can be especially useful with lengthy object
names, such as views.
    
    In the syntax:PUBLIC : creates a synonym accessible to all userssynonym : is the name of the synonym to be createdobject : identifies the object for which the synonym is created
    
    **Syntax :**
    
    ```
    CREATE [PUBLIC] SYNONYM synonym FOR  object;
    ```
    
    **Example :**
    
    ```
    CREATE SYNONYM d_sum FOR dept_sum_vu;
    ```
    

**References :**[Database objects – ibm](https://www.ibm.com/support/knowledgecenter/en/SSEPGG_9.5.0/com.ibm.db2.luw.admin.dbobj.doc/doc/c0023321.html)[Introduction to Oracle 9i](https://www.amazon.com/Introduction-Oracle-9i-Student-Guide/dp/B007U55B2A/ref=sr_1_9/135-0559177-4020104?s=books&ie=UTF8&qid=1513328691&sr=1-9&refinements=p_27%3AOracle): SQL Student Guide Volume 2
*** Nested Queries in SQL
Prerequisites : [Basics of SQL](https://www.geeksforgeeks.org/structured-query-language/)

In
 nested queries, a query is written inside a query. The result of inner 
query is used in execution of outer query. We will use **STUDENT, COURSE, STUDENT_COURSE** tables for understanding nested queries.

**STUDENT**

[Untitled Database](https://www.notion.so/e22bf594b35f4ee1b960123c6088306c?pvs=21)

**COURSE**

[Untitled Database](https://www.notion.so/db6fbb666a5f4d4b9e85cb8c3d7ab0ad?pvs=21)

**STUDENT_COURSE**

[Untitled Database](https://www.notion.so/b159ae2cf34a4bb1b7c293ef2c5998b5?pvs=21)

There are mainly two types of nested queries:

- **Independent Nested Queries:** In independent nested queries, query execution starts from innermost
query to outermost queries. The execution of inner query is independent
of outer query, but the result of inner query is used in execution of
outer query. Various operators like IN, NOT IN, ANY, ALL etc are used in writing independent nested queries.
    
    **IN:** If we want to find out **S_ID** who are enrolled in **C_NAME** ‘DSA’ or ‘DBMS’, we can write it with the help of independent nested query and IN operator. From **COURSE** table, we can find out **C_ID** for **C_NAME** ‘DSA’ or DBMS’ and we can use these **C_ID**s for finding **S_ID**s from **STUDENT_COURSE** TABLE.
    
    **STEP 1:** Finding **C_ID** for **C_NAME** =’DSA’ or ‘DBMS’
    
    Select **C_ID** from **COURSE** where **C_NAME** = ‘DSA’ or **C_NAME** = ‘DBMS’
    
    **STEP 2:** Using **C_ID** of step 1 for finding **S_ID**
    
    Select **S_ID** from **STUDENT_COURSE** where **C_ID** IN
    
    (SELECT **C_ID** from **COURSE** where **C_NAME** = ‘DSA’ or **C_NAME**=’DBMS’);
    
    The inner query will return a set with members C1 and C3 and outer query will return those **S_ID**s for which **C_ID** is equal to any member of set (C1 and C3 in this case). So, it will return S1, S2 and S4.
    
    **Note:** If we want to find out names of **STUDENT**s who have either enrolled in ‘DSA’ or ‘DBMS’, it can be done as:
    
    Select S_NAME from **STUDENT** where **S_ID** IN
    
    (Select **S_ID** from **STUDENT_COURSE** where **C_ID** IN
    
    (SELECT **C_ID** from **COURSE** where **C_NAME**=’DSA’ or **C_NAME**=’DBMS’));
    
    **NOT IN:** If we want to find out **S_ID**s of **STUDENT**s who have neither enrolled in ‘DSA’ nor in ‘DBMS’, it can be done as:
    
    Select **S_ID** from **STUDENT** where **S_ID** NOT IN
    
    (Select **S_ID** from **STUDENT_COURSE** where **C_ID** IN
    
    (SELECT **C_ID** from **COURSE** where **C_NAME**=’DSA’ or **C_NAME**=’DBMS’));
    
    The innermost query will return a set with members C1 and C3. Second inner query will return those **S_ID**s for which **C_ID** is equal to any member of set (C1 and C3 in this case) which are S1, S2 and S4. The outermost query will return those **S_ID**s where **S_ID** is not a member of set (S1, S2 and S4). So it will return S3.
    
- **Co-related Nested Queries:** In co-related nested queries, the output of inner query depends on the
row which is being currently executed in outer query. e.g.; If we want
to find out **S_NAME** of **STUDENT**s who are enrolled in **C_ID** ‘C1’, it can be done with the help of co-related nested query as:
    
    Select S_NAME from **STUDENT** S where EXISTS
    
    ( select * from **STUDENT_COURSE** SC where S.**S_ID**=SC.**S_ID** and SC.**C_ID**=’C1’);
    
    For each row of **STUDENT** S, it will find the rows from **STUDENT_COURSE** where S.**S_ID** = SC.**S_ID** and SC.**C_ID**=’C1’. If for a **S_ID** from **STUDENT** S, atleast a row exists in **STUDENT_COURSE** SC with **C_ID**=’C1’, then inner query will return true and corresponding **S_ID** will be returned as output.
    

This article has been contributed by Sonal Tuteja.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-24-14-39-28-SQL _.webp
*** Join operation Vs Nested query in DBMS
The 
growth of technology and automation coupled with exponential amounts of 
data has led to the importance and omnipresence of databases which, 
simply put, are organized collections of data. Considering a naive 
approach, one can theoretically keep all the data in one large table, 
however that increases the access time in searching for a record, 
security issues if the master table is destroyed, redundant storage of 
information and other issues. So tables are decomposed into multiple 
smaller tables.

For 
retrieving information from multiple tables, we need to extract selected
 data from different records, using operations called join(inner join, 
outer join and most importantly natural join). Consider 2 table schemas 
employee(employee_name, street, city)with n rows and 
works(employee_name, branch_name, salary) with m rows. A cartesian 
product of these 2 tables creates a table with n*m rows. A natural join 
selects from this n*m rows all rows with same values for employee_name. 
To avoid loss of information(some tuples in employee have no 
corresponding tuples in works) we use left outer join or right outer 
join.

A join or a nested query is better subject to conditions: 

- Suppose our 2 tables are stored on a local system. Performing a join or a
nested query will make little difference. Now let tables be stored
across a distributed databases. For a nested query, we only extract the
relevant information from each table, located on different computers,
then merge the tuples obtained to obtain the result. For a join, we
would be required to fetch the whole table from each site and create a
large table from which the filtering will occur, hence more time will be required. So for distributed databases, nested queries are better.
- RDBMS optimizer is concerned with performance related to the subquery or join written by the programmer. Joins are universally understood hence no
optimization issues can arise. If portability across multiple platforms
is called for, avoid subqueries as it may run into bugs(SQL server more
adept with joins as its usually used with Microsoft’s graphical query
editors that use joins).
- Implementation specific: Suppose we
have queries where a few of the nested queries are constant. In MySQL,
every constant subquery would be evaluated as many times as encountered, there being no cache facility. This is an obvious problem if the
constant subquery involves large tuples. Subqueries return a set of
data. Joins return a dataset which is necessarily indexed. Working on
indexed data is faster so if the dataset returned by subqueries is
large, joins are a better idea.
- Subqueries may take longer to
execute than joins depending on how the database optimizer treats
them(may be converted to joins). Subqueries are easier to read,
understand and evaluate than cryptic joins. They allow a bottom-up
approach, isolating and completing each task sequentially.

Refer – [Join operation Vs nested query](https://practice.geeksforgeeks.org/problems/which-one-is-more-efficient-a-join-operation-or-a-nested-query)
*** Indexing in Databases
Indexing
 is a way to optimize the performance of a database by minimizing the 
number of disk accesses required when a query is processed. It is a data
 structure technique which is used to quickly locate and access the data
 in a database.

Indexes are created using a few database columns.

- The first column is the **Search key** that contains a copy of the primary key or candidate key of the table.
These values are stored in sorted order so that the corresponding data
can be accessed quickly. *Note: The data may or may not be stored in sorted order.*
- The second column is the **Data Reference** or **Pointer** which contains a set of pointers holding the address of the disk block where that particular key value can be found.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190812183525/Structure-of-an-Index-in-Database.jpg

The indexing has various attributes:

- **Access Types**: This refers to the type of access such as value based search, range access, etc.
- **Access Time**: It refers to the time needed to find particular data element or set of elements.
- **Insertion Time**: It refers to the time taken to find the appropriate space and insert a new data.
- **Deletion Time**: Time taken to find an item and delete it as well as update the index structure.
- **Space Overhead**: It refers to the additional space required by the index.

In general, there are two types of file organization mechanism which are followed by the indexing methods to store the data:

**1. Sequential File Organization or Ordered Index File:**
 In this, the indices are based on a sorted ordering of the values. 
These are generally fast and a more traditional type of storing 
mechanism. These Ordered or Sequential file organization might store the
 data in a dense or sparse format:

**(i) Dense Index:**

- For every search key value in the data file, there is an index record.
- This record contains the search key and also a reference to the first data record with that search key value.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190812183521/Dense-Index.jpg

**(ii) Sparse Index:**

- The index record appears only for a few items in the data file. Each item points to a block as shown.
- To locate a record, we find the index record with the largest search key
value less than or equal to the search key value we are looking for.
- We start at that record pointed to by the index record, and proceed along
with the pointers in the file (that is, sequentially) until we find the
desired record.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190812183518/Sparse-Index.jpg

**2. Hash File organization:**
 Indices are based on the values being distributed uniformly across a 
range of buckets. The buckets to which a value is assigned is determined
 by a function called a hash function.

There are primarily three methods of indexing:

- Clustered Indexing
- Non-Clustered or Secondary Indexing
- Multilevel Indexing

**1. Clustered Indexing** When
 more than two records are stored in the same file these types of 
storing known as cluster indexing. By using the cluster indexing we can 
reduce the cost of searching reason being multiple records related to 
the same thing are stored at one place and it also gives the frequent 
joining of more than two tables (records). Clustering index is 
defined on an ordered data file. The data file is ordered on a non-key 
field. In some cases, the index is created on non-primary key columns 
which may not be unique for each record. In such cases, in order to 
identify the records faster, we will group two or more columns together 
to get the unique values and create index out of them. This method is 
known as the clustering index. Basically, records with similar 
characteristics are grouped together and indexes are created for these 
groups. For example, students studying in each semester are grouped together. i.e. 1st Semester students, 2nd semester students, 3rd semester students etc. are grouped.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/07/cluster_index.png

**Clustered index sorted according to first name (Search key)**

***Primary Indexing:*** This
 is a type of Clustered Indexing wherein the data is sorted according to
 the search key and the primary key of the database table is used to 
create the index. It is a default format of indexing where it induces 
sequential file organization. As primary keys are unique and are stored 
in a sorted manner, the performance of the searching operation is quite 
efficient.

**2. Non-clustered or Secondary Indexing** A
 non clustered index just tells us where the data lies, i.e. it gives us
 a list of virtual pointers or references to the location where the data
 is actually stored. Data is not physically stored in the order of the 
index. Instead, data is present in leaf nodes. For eg. the contents page
 of a book. Each entry gives us the page number or location of the 
information stored. The actual data here(information on each page of the
 book) is not organized but we have an ordered reference(contents page) 
to where the data points actually lie. We can have only dense ordering 
in the non-clustered index as sparse ordering is not possible because 
data is not physically organized accordingly. It requires more time 
as compared to the clustered index because some amount of extra work is 
done in order to extract the data by further following the pointer. In 
the case of a clustered index, data is directly present in front of the 
index.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/07/indexing3.png

**3. Multilevel Indexing**

With
 the growth of the size of the database, indices also grow. As the index
 is stored in the main memory, a single-level index might become too 
large a size to store with multiple disk accesses. The multilevel 
indexing segregates the main block into various smaller blocks so that 
the same can stored in a single block. The outer blocks are divided into
 inner blocks which in turn are pointed to the data blocks. This can be 
easily stored in the main memory with fewer overheads.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190812143045/Untitled-Diagram-41.png

This article is contributed by **Avneet Kaur**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

Indexing in Databases | Set 1
*** SQL queries on clustered and non-clustered Indexes
Prerequisite – [Indexing in Databases](https://www.geeksforgeeks.org/indexing-in-databases-set-1/) **Indexing**
 is a procedure that returns your requested data faster from the defined
 table. Without indexing, the SQL server has to scan the whole table for
 your data. By indexing, SQL server will do the exact same thing you do 
when searching for content in a book by checking the index page. In the 
same way, a table’s index allows us to locate the exact data without 
scanning the whole table. There are two types of indexing in SQL.

1. Clustered index
2. Non-clustered index

**1. Clustered –** Clustered index is the type of indexing that establishes a physical sorting order of rows. Suppose you have a table *Student_info* which contains *ROLL_NO* as a primary key, then Clustered index which is self created on that primary key will sort the *Student_info* table as per *ROLL_NO*. Clustered index is like Dictionary; in the dictionary, sorting order is alphabetical and there is no separate index page.

**Examples:**

```
Input:
CREATE TABLE Student_info
(
ROLL_NO int(10) primary key,
NAME varchar(20),
DEPARTMENT varchar(20),
);
insert into Student_info values(1410110405, 'H Agarwal', 'CSE')
insert into Student_info values(1410110404, 'S Samadder', 'CSE')
insert into Student_info values(1410110403, 'MD Irfan', 'CSE')

SELECT * FROM Student_info
```

**Output:**

[Untitled Database](https://www.notion.so/c29d897ef2da4db4b5745d5cc4e1b754?pvs=21)

If
 we want to create a Clustered index on another column, first we have to
 remove the primary key, and then we can remove the previous index. Note
 that defining a column as a primary key makes that column the Clustered
 Index of that table. To make any other column, the clustered index, 
first we have to remove the previous one as follows below.

**Syntax:**

```
//Drop index
drop index table_name.index_name
//Create Clustered index index
create Clustered index IX_table_name_column_name
             on table_name (column_name ASC)
```

**Note:** We can create only one clustered index in a table.

**2. Non-clustered:**
 Non-Clustered index is an index structure separate from the data stored
 in a table that reorders one or more selected columns. The 
non-clustered index is created to improve the performance of frequently 
used queries not covered by a clustered index. It’s like a textbook; the
 index page is created separately at the beginning of that book. **Examples:**

```
Input:
CREATE TABLE Student_info
(
ROLL_NO int(10),
NAME varchar(20),
DEPARTMENT varchar(20),
);
insert into Student_info values(1410110405, 'H Agarwal', 'CSE')
insert into Student_info values(1410110404, 'S Samadder', 'CSE')
insert into Student_info values(1410110403, 'MD Irfan', 'CSE')

SELECT * FROM Student_info
```

**Output:**

[Untitled Database](https://www.notion.so/4ef49163b12b4105b2a22cb2c87affc6?pvs=21)

**Note:** We can create one or more Non_Clustered index in a table. **Syntax:**

```
//Create Non-Clustered index
create NonClustered index IX_table_name_column_name
      on table_name (column_name ASC)
```

Table: Student_info

[Untitled Database](https://www.notion.so/6a2d944bcc1b4894abd5c6890db5c9e4?pvs=21)

**Input:** create NonClustered index IX_Student_info_NAME on Student_info (NAME ASC) **Output:**Index

[Untitled Database](https://www.notion.so/7f6a2bf3fd70459b82f51db7a5db1041?pvs=21)

**Clustered vs Non-Clustered index:**

- In a table there can be only one clustered index or one or more than one non_clustered index.
- In Clustered index there is no separate index storage but in Non_Clustered index there is separate index storage for the index.
- Clustered index is slower than Non_Clustered index.
** Transactions and concurrency control
*** Concurrency Control in DBMS
Concurrency Control deals with **interleaved execution**
 of more than one transaction. In the next article, we will see what is 
serializability and how to find whether a schedule is serializable or 
not.

**What is Transaction?**

A set of logically related operations is known as a transaction. The main operations of a transaction are:

**Read(A):** Read operations Read(A) or R(A) reads the value of A from the database and stores it in a buffer in the main memory.

**Write (A):** Write operation Write(A) or W(A) writes the value back to the database from the buffer.

(Note:
 It doesn’t always need to write it to a database back it just writes 
the changes to buffer this is the reason where dirty read comes into the
 picture)

Let us take a debit transaction from an account that consists of the following operations:

1. R(A);
2. A=A-1000;
3. W(A);

Assume A’s value before starting the transaction is 5000.

- The first operation reads the value of A from the database and stores it in a buffer.
- the Second operation will decrease its value by 1000. So buffer will contain 4000.
- the Third operation will write the value from the buffer to the database. So A’s final value will be 4000.

But it may also be possible that the transaction may fail after executing some of its operations. The failure can be because of **hardware, software or power,**
 etc. For example, if the debit transaction discussed above fails after 
executing operation 2, the value of A will remain 5000 in the database 
which is not acceptable by the bank. To avoid this, Database has two 
important operations:

**Commit:**
 After all instructions of a transaction are successfully executed, the 
changes made by a transaction are made permanent in the database.

**Rollback:** If a transaction is not able to execute all operations successfully, all the changes made by a transaction are undone.

### Properties of a transaction:

**Atomicity:** As a transaction is a set of logically related operations, **either all of them should be executed or none**.
 A debit transaction discussed above should either execute all three 
operations or none. If the debit transaction fails after executing 
operations 1 and 2 then its new value of 4000 will not be updated in the
 database which leads to inconsistency.

**Consistency:** If
 operations of debit and credit transactions on the same account are 
executed concurrently, it may leave the database in an inconsistent 
state.

- For Example, with T1 (debit of Rs. 1000 from A) and T2 (credit of 500 to A) executing concurrently, the database reaches an inconsistent state.
- Let us assume the Account balance of A is Rs. 5000. T1 reads A(5000) and
stores the value in its local buffer space. Then T2 reads A(5000) and
also stores the value in its local buffer space.
- T1 performs A=A-1000 (5000-1000=4000) and 4000 is stored in T1 buffer
space. Then T2 performs A=A+500 (5000+500=5500) and 5500 is stored in
the T2 buffer space. T1 writes the value from its buffer back to the
database.
- A’s value is updated to
4000 in the database and then T2 writes the value from its buffer back
to the database. A’s value is updated to 5500 which shows that the
effect of the debit transaction is lost and the database has become
inconsistent.
- To maintain consistency of the database, we need **concurrency control protocols** which will be discussed in the next article. The operations of T1 and
T2 with their buffers and database have been shown in Table 1.

[Untitled Database](https://www.notion.so/9e5f20798d2642d9bfb08eab4e30348e?pvs=21)

**Table 1**

**Isolation:**
 The result of a transaction should not be visible to others before the 
transaction is committed. For example, let us assume that A’s balance is
 Rs. 5000 and T1 debits Rs. 1000 from A. A’s new balance will be 4000. 
If T2 credits Rs. 500 to A’s new balance, A will become 4500, and after 
this T1 fails. Then we have to roll back T2 as well because it is using 
the value produced by T1. So transaction results are not made visible to
 other transactions before it commits.

**Durable:**
 Once the database has committed a transaction, the changes made by the 
transaction should be permanent. e.g.; If a person has credited $500000 
to his account, the bank can’t say that the update has been lost. To 
avoid this problem, multiple copies of the database are stored at 
different locations.

**What is a Schedule?**

A schedule is a series of operations from one or more transactions. A schedule can be of two types:

- **Serial Schedule:** When one transaction completely executes before starting another
transaction, the schedule is called a serial schedule. A serial schedule is always consistent. e.g.; If a schedule S has debit transaction T1
and credit transaction T2, possible serial schedules are T1 followed by
T2 (T1->T2) or T2 followed by T1 ((T2->T1). A serial schedule has
low throughput and less resource utilization.
- **Concurrent Schedule:** When operations of a transaction are interleaved with operations of
other transactions of a schedule, the schedule is called a Concurrent
schedule. e.g.; the Schedule of debit and credit transactions shown in
Table 1 is concurrent. But concurrency can lead to inconsistency in the
database. The above example of a concurrent schedule is also
inconsistent.

**Question: Consider the following transaction involving two bank accounts x and y:**

1. read(x);
2. x := x – 50;
3. write(x);
4. read(y);
5. y := y + 50;
6. write(y);

The constraint that the sum of the accounts x and y should remain constant is that of?

1. Atomicity
2. Consistency
3. Isolation
4. Durability **[GATE 2015]**

**Solution:**
 As discussed in properties of transactions, consistency properties say 
that sum of accounts x and y should remain constant before starting and 
after completion of a transaction. So, the correct answer is B.

### Advantages of Concurrency:

In general, concurrency means, that more than one transaction can work on a system.

The advantages of a concurrent system are:

- **Waiting Time:** It means if a process is in a ready state but still the process does
not get the system to get execute is called waiting time. So,
concurrency leads to .
    
    less waiting time
    
- **Response Time:** The time wasted in getting the response from the cpu for the first time, is called response time. So, concurrency leads to .
    
    less Response Time
    
- **Resource Utilization:** The amount of Resource utilization in a particular system is called
Resource Utilization. Multiple transactions can run parallel in a
system. So, concurrency leads to .
    
    more Resource Utilization
    
- **Efficiency:** The amount of output produced in comparison to given input is called efficiency. So, Concurrency leads to .
    
    more Efficiency
    

Next article- [Serializability of Schedules](https://www.geeksforgeeks.org/conflict-serializability-in-dbms/)

Article
 contributed by Sonal Tuteja. Please write comments if you find anything
 incorrect, or if you want to share more information about the topic 
discussed above.
*** Database Recovery Techniques in DBMS
**Database systems**,
 like any other computer system, are subject to failures but the data 
stored in it must be available as and when required. When a database 
fails it must possess the facilities for fast recovery. It must also 
have atomicity i.e. either transactions are completed successfully and 
committed (the effect is recorded permanently in the database) or the 
transaction should have no effect on the database.

There
 are both automatic and non-automatic ways for both, backing up of data 
and recovery from any failure situations. The techniques used to recover
 the lost data due to system crash, transaction errors, viruses, 
catastrophic failure, incorrect commands execution etc. are database 
recovery techniques. So to prevent data loss recovery techniques based 
on deferred update and immediate update or backing up data can be used.

Recovery techniques are heavily dependent upon the existence of a special file known as a **system log**. It contains information about the start and end of each transaction and any updates which occur in the **transaction**.
 The log keeps track of all transaction operations that affect the 
values of database items. This information is needed to recover from 
transaction failure.

- The log is kept on disk start_transaction(T): This log entry records that transaction T starts the execution.
- read_item(T, X): This log entry records that transaction T reads the value of database item X.
- write_item(T, X, old_value, new_value): This log entry records that transaction T
changes the value of the database item X from old_value to new_value.
The old value is sometimes known as a before an image of X, and the new
value is known as an afterimage of X.
- commit(T): This log entry
records that transaction T has completed all accesses to the database
successfully and its effect can be committed (recorded permanently) to
the database.
- abort(T): This records that transaction T has been aborted.
- checkpoint: Checkpoint is a mechanism where all the previous logs are removed from
the system and stored permanently in a storage disk. Checkpoint declares a point before which the DBMS was in consistent state, and all the
transactions were committed.

A transaction T reaches its **commit**
 point when all its operations that access the database have been 
executed successfully i.e. the transaction has reached the point at 
which it will not **abort** (terminate without completing). Once 
committed, the transaction is permanently recorded in the database. 
Commitment always involves writing a commit entry to the log and writing
 the log to disk. At the time of a system crash, item is searched back 
in the log for all transactions T that have written a 
start_transaction(T) entry into the log but have not written a commit(T)
 entry yet; these transactions may have to be rolled back to undo their 
effect on the database during the recovery process

- **Undoing –** If a transaction crashes, then the recovery manager may undo
transactions i.e. reverse the operations of a transaction. This involves examining a transaction for the log entry write_item(T, x, old_value,
new_value) and setting the value of item x in the database to
old-value.There are two major techniques for recovery from
non-catastrophic transaction failures: deferred updates and immediate
updates.
- **Deferred update –** This technique does not
physically update the database on disk until a transaction has reached
its commit point. Before reaching commit, all transaction updates are
recorded in the local transaction workspace. If a transaction fails
before reaching its commit point, it will not have changed the database
in any way so UNDO is not needed. It may be necessary to REDO the effect of the operations that are recorded in the local transaction workspace, because their effect may not yet have been written in the database.
Hence, a deferred update is also known as the **No-undo/redo algorithm**
- **Immediate update –** In the immediate update, the database may be updated by some operations of a transaction before the transaction reaches its commit point.
However, these operations are recorded in a log on disk before they are
applied to the database, making recovery still possible. If a
transaction fails to reach its commit point, the effect of its operation must be undone i.e. the transaction must be rolled back hence we
require both undo and redo. This technique is known as **undo/redo algorithm.**
- **Caching/Buffering –** In this one or more disk pages that include data items to be updated
are cached into main memory buffers and then updated in memory before
being written back to disk. A collection of in-memory buffers called the DBMS cache is kept under control of DBMS for holding these buffers. A
directory is used to keep track of which database items are in the
buffer. A dirty bit is associated with each buffer, which is 0 if the
buffer is not modified else 1 if modified.
- **Shadow paging –** It provides atomicity and durability. A directory with n entries is
constructed, where the ith entry points to the ith database page on the
link. When a transaction began executing the current directory is copied into a shadow directory. When a page is to be modified, a shadow page
is allocated in which changes are made and when it is ready to become
durable, all pages that refer to original are updated to refer new
replacement page.

Some of the backup techniques are as follows :

- **Full database backup –** In this full database including data and database, Meta information
needed to restore the whole database, including full-text catalogs are
backed up in a predefined time series.
- **Differential backup –** It stores only the data changes that have occurred since last full
database backup. When same data has changed many times since last full
database backup, a differential backup stores the most recent version of changed data. For this first, we need to restore a full database
backup.
- **Transaction log backup –** In this, all events that have occurred in the database, like a record of every single statement
executed is backed up. It is the backup of transaction log entries and
contains all transaction that had happened to the database. Through
this, the database can be recovered to a specific point in time. It is
even possible to perform a backup from a transaction log if the data
files are destroyed and not even a single committed transaction is lost.

**Reference –**[Backup and Recovery – cs.uct.ac.za/mit_notes](https://www.cs.uct.ac.za/mit_notes/database/htmls/chp14.html)

This article is contributed by **[Himanshi](https://auth.geeksforgeeks.org/profile.php?user=Himanshi_Singh)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** ACID Properties in DBMS
A **[transaction](https://www.geeksforgeeks.org/sql-transactions/)**
 is a single logical unit of work that accesses and possibly modifies 
the contents of a database. Transactions access data using read and 
write operations. In order to maintain consistency in a database, 
before and after the transaction, certain properties are followed. These
 are called **ACID** properties.

### 

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20191121102921/ACID-Properties.jpg

### **Atomicity:**

By
 this, we mean that either the entire transaction takes place at once or
 doesn’t happen at all. There is no midway i.e. transactions do not 
occur partially. Each transaction is considered as one unit and either 
runs to completion or is not executed at all. It involves the following 
two operations. —**Abort**: If a transaction aborts, changes made to the database are not visible. —**Commit**: If a transaction commits, changes made are visible. Atomicity is also known as the ‘All or nothing rule’.

Consider the following transaction **T** consisting of **T1** and **T2**: Transfer of 100 from account **X** to account **Y**.

!https://media.geeksforgeeks.org/wp-content/uploads/11-6.jpg

If the transaction fails after completion of **T1** but before completion of **T2**.( say, after **write(X)** but before **write(Y)**), then the amount has been deducted from **X** but not added to **Y**.
 This results in an inconsistent database state. Therefore, the 
transaction must be executed in its entirety in order to ensure the 
correctness of the database state.

### Consistency:

This 
means that integrity constraints must be maintained so that the database
 is consistent before and after the transaction. It refers to the 
correctness of a database. Referring to the example above, The total amount before and after the transaction must be maintained. Total **before T** occurs = **500 + 200 = 700**. Total **after T occurs** = **400 + 300 = 700**. Therefore, the database is **consistent**. Inconsistency occurs in case **T1** completes but **T2** fails. As a result, T is incomplete.

### Isolation:

This
 property ensures that multiple transactions can occur concurrently 
without leading to the inconsistency of the database state. Transactions
 occur independently without interference. Changes occurring in a 
particular transaction will not be visible to any other transaction 
until that particular change in that transaction is written to memory or
 has been committed. This property ensures that the execution of 
transactions concurrently will result in a state that is equivalent to a
 state achieved these were executed serially in some order. Let **X**= 500, **Y** = 500. Consider two transactions **T** and **T”.**

!https://media.geeksforgeeks.org/wp-content/uploads/20210402015259/isolation-300x137.jpg

Suppose **T** has been executed till **Read (Y)** and then **T’’** starts. As a result, interleaving of operations takes place due to which **T’’** reads the correct value of **X** but the incorrect value of **Y** and sum computed by **T’’: (X+Y = 50, 000+500=50, 500)** is thus not consistent with the sum at end of the transaction: **T: (X+Y = 50, 000 + 450 = 50, 450)**. This
 results in database inconsistency, due to a loss of 50 units. Hence, 
transactions must take place in isolation and changes should be visible 
only after they have been made to the main memory.

### Durability:

This
 property ensures that once the transaction has completed execution, the
 updates and modifications to the database are stored in and written to 
disk and they persist even if a system failure occurs. These updates now
 become permanent and are stored in non-volatile memory. The effects of 
the transaction, thus, are never lost.

**Some important points:**

[Untitled Database](https://www.notion.so/28abd755d1884029bd4e689ffe1b3177?pvs=21)

The **ACID**
 properties, in totality, provide a mechanism to ensure the correctness 
and consistency of a database in a way such that each transaction is a 
group of operations that acts as a single unit, produces consistent 
results, acts in isolation from other operations, and updates that it 
makes are durably stored.

This article is contributed by **Avneet Kaur**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Log based Recovery in DBMS
[Atomicity](https://www.geeksforgeeks.org/acid-properties-in-dbms/)
 property of DBMS states that either all the operations of transactions 
must be performed or none. The modifications done by an aborted 
transaction should not be visible to database and the modifications done
 by committed transaction should be visible.

To
 achieve our goal of atomicity, user must first output to stable storage
 information describing the modifications, without modifying the 
database itself. This information can help us ensure that all 
modifications performed by committed transactions are reflected in the 
database. This information can also help us ensure that no modifications
 made by an aborted transaction persist in the database.

**Log and log records –**The
 log is a sequence of log records, recording all the update activities 
in the database. In a stable storage, logs for each transaction are 
maintained. Any operation which is performed on the database is recorded
 is on the log. Prior to performing any modification to database, an 
update log record is created to reflect that modification.

An update log record represented as: <Ti, Xj, V1, V2> has these fields:

1. **Transaction identifier:** Unique Identifier of the transaction that performed the write operation.
2. **Data item:** Unique identifier of the data item written.
3. **Old value:** Value of data item prior to write.
4. **New value:** Value of data item after write operation.

Other type of log records are:

1. **<Ti start>**: It contains information about when a transaction Ti starts.
2. **<Ti commit>**: It contains information about when a transaction Ti commits.
3. **<Ti abort>**: It contains information about when a transaction Ti aborts.

**Undo and Redo Operations –**Because
 all database modifications must be preceded by creation of log record, 
the system has available both the old value prior to modification of 
data item and new value that is to be written for data item. This allows
 system to perform redo and undo operations as appropriate:

1. **Undo:** using a log record sets the data item specified in log record to old value.
2. **Redo:** using a log record sets the data item specified in log record to new value.

**The database can be modified using two approaches –**

1. **Deferred Modification Technique:** If the transaction does not modify the database until it has partially
committed, it is said to use deferred modification technique.
2. **Immediate Modification Technique:** If database modification occur while transaction is still active, it is said to use immediate modification technique.

**Recovery using Log records –**After
 a system crash has occurred, the system consults the log to determine 
which transactions need to be redone and which need to be undone.

1. Transaction Ti needs to be undone if the log contains the record <Ti start>
but does not contain either the record <Ti commit> or the record
<Ti abort>.
2. Transaction Ti needs to be redone if log
contains record <Ti start> and either the record <Ti commit> or the record <Ti abort>.

**Use of Checkpoints –**When
 a system crash occurs, user must consult the log. In principle, that 
need to search the entire log to determine this information. There are 
two major difficulties with this approach:

1. The search process is time-consuming.
2. Most of the transactions that, according to our algorithm, need to be redone have already written their updates into the database. Although redoing
them will cause no harm, it will cause recovery to take longer.

To
 reduce these types of overhead, user introduce checkpoints. A log 
record of the form <checkpoint L> is used to represent a 
checkpoint in log where L is a list of transactions active at the time 
of the checkpoint. When a checkpoint log record is added to log all the 
transactions that have committed before this checkpoint have <Ti 
commit> log record before the checkpoint record. Any database 
modifications made by Ti is written to the database either prior to the 
checkpoint or as part of the checkpoint itself. Thus, at recovery time, 
there is no need to perform a redo operation on Ti.

After a system
 crash has occurred, the system examines the log to find the last 
<checkpoint L> record. The redo or undo operations need to be 
applied only to transactions in L, and to all transactions that started 
execution after the record was written to the log. Let us denote this 
set of transactions as T. Same rules of undo and redo are applicable on T
 as mentioned in Recovery using Log records part.

Note that user 
need to only examine the part of the log starting with the last 
checkpoint log record to find the set of transactions T, and to find out
 whether a commit or abort record occurs in the log for each transaction
 in T. For example, consider the set of transactions {T0, T1, . . ., 
T100}. Suppose that the most recent checkpoint took place during the 
execution of transaction T67 and T69, while T68 and all transactions 
with subscripts lower than 67 completed before the checkpoint. Thus, 
only transactions T67, T69, . . ., T100 need to be considered during the
 recovery scheme. Each of them needs to be redone if it has completed 
(that is, either committed or aborted); otherwise, it was incomplete, 
and needs to be undone.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

# Log based Recovery in DBMS

- Difficulty Level :
[Medium](https://www.geeksforgeeks.org/medium/)
- Last Updated :
20 Aug, 2019

[Atomicity](https://www.geeksforgeeks.org/acid-properties-in-dbms/)
 property of DBMS states that either all the operations of transactions 
must be performed or none. The modifications done by an aborted 
transaction should not be visible to database and the modifications done
 by committed transaction should be visible.

To
 achieve our goal of atomicity, user must first output to stable storage
 information describing the modifications, without modifying the 
database itself. This information can help us ensure that all 
modifications performed by committed transactions are reflected in the 
database. This information can also help us ensure that no modifications
 made by an aborted transaction persist in the database.

**Log and log records –**The
 log is a sequence of log records, recording all the update activities 
in the database. In a stable storage, logs for each transaction are 
maintained. Any operation which is performed on the database is recorded
 is on the log. Prior to performing any modification to database, an 
update log record is created to reflect that modification.

An update log record represented as: <Ti, Xj, V1, V2> has these fields:

1. **Transaction identifier:** Unique Identifier of the transaction that performed the write operation.
2. **Data item:** Unique identifier of the data item written.
3. **Old value:** Value of data item prior to write.
4. **New value:** Value of data item after write operation.

Other type of log records are:

1. **<Ti start>**: It contains information about when a transaction Ti starts.
2. **<Ti commit>**: It contains information about when a transaction Ti commits.
3. **<Ti abort>**: It contains information about when a transaction Ti aborts.

**Undo and Redo Operations –**Because
 all database modifications must be preceded by creation of log record, 
the system has available both the old value prior to modification of 
data item and new value that is to be written for data item. This allows
 system to perform redo and undo operations as appropriate:

1. **Undo:** using a log record sets the data item specified in log record to old value.
2. **Redo:** using a log record sets the data item specified in log record to new value.

**The database can be modified using two approaches –**

1. **Deferred Modification Technique:** If the transaction does not modify the database until it has partially
committed, it is said to use deferred modification technique.
2. **Immediate Modification Technique:** If database modification occur while transaction is still active, it is said to use immediate modification technique.

**Recovery using Log records –**After
 a system crash has occurred, the system consults the log to determine 
which transactions need to be redone and which need to be undone.

1. Transaction Ti needs to be undone if the log contains the record <Ti start>
but does not contain either the record <Ti commit> or the record
<Ti abort>.
2. Transaction Ti needs to be redone if log
contains record <Ti start> and either the record <Ti commit> or the record <Ti abort>.

**Use of Checkpoints –**When
 a system crash occurs, user must consult the log. In principle, that 
need to search the entire log to determine this information. There are 
two major difficulties with this approach:

1. The search process is time-consuming.
2. Most of the transactions that, according to our algorithm, need to be redone have already written their updates into the database. Although redoing
them will cause no harm, it will cause recovery to take longer.

To
 reduce these types of overhead, user introduce checkpoints. A log 
record of the form <checkpoint L> is used to represent a 
checkpoint in log where L is a list of transactions active at the time 
of the checkpoint. When a checkpoint log record is added to log all the 
transactions that have committed before this checkpoint have <Ti 
commit> log record before the checkpoint record. Any database 
modifications made by Ti is written to the database either prior to the 
checkpoint or as part of the checkpoint itself. Thus, at recovery time, 
there is no need to perform a redo operation on Ti.

After a system
 crash has occurred, the system examines the log to find the last 
<checkpoint L> record. The redo or undo operations need to be 
applied only to transactions in L, and to all transactions that started 
execution after the record was written to the log. Let us denote this 
set of transactions as T. Same rules of undo and redo are applicable on T
 as mentioned in Recovery using Log records part.

Note that user 
need to only examine the part of the log starting with the last 
checkpoint log record to find the set of transactions T, and to find out
 whether a commit or abort record occurs in the log for each transaction
 in T. For example, consider the set of transactions {T0, T1, . . ., 
T100}. Suppose that the most recent checkpoint took place during the 
execution of transaction T67 and T69, while T68 and all transactions 
with subscripts lower than 67 completed before the checkpoint. Thus, 
only transactions T67, T69, . . ., T100 need to be considered during the
 recovery scheme. Each of them needs to be redone if it has completed 
(that is, either committed or aborted); otherwise, it was incomplete, 
and needs to be undone.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

Log based Recovery in DBMS
*** Why recovery is needed in DBMS
Basically,
 whenever a transaction is submitted to a DBMS for execution, the 
operating system is responsible for making sure or to be confirmed that 
all the operation which need to be in performed in the transaction have 
completed successfully and their effect is either recorded in the 
database or the transaction doesn’t affect the database or any other 
transactions.

The DBMS 
must not permit some operation of the transaction T to be applied to the
 database while other operations of T is not. This basically may happen 
if a transaction fails after executing some of its operations but before
 executing all of them.

**Types of failures –**There are basically following types of failures that may occur and leads to failure of the transaction such as:

1. Transaction failure
2. System failure
3. Media failure and so on.

Let us try to understand the different types of failures that may occur during the transaction.

1. **System crash –**A hardware, software or network error occurs comes under this category
this types of failures basically occurs during the execution of the
transaction. Hardware failures are basically considered as Hardware
failure.
2. **System error –**Some operation that is
performed during the transaction is the reason for this type of error to occur, such as integer or divide by zero. This type of failures is also known as the transaction which may also occur because of erroneous
parameter values or because of a logical programming error. In addition
to this user may also interrupt the execution during execution which may lead to failure in the transaction.
3. **Local error –**This basically happens when we are doing the transaction but certain
conditions may occur that may lead to cancellation of the transaction.
This type of error is basically coming under Local error. The simple
example of this is that data for the transaction may not found. When we
want to debit money from an insufficient balance account which leads to
the cancellation of our request or transaction. And this exception
should be programmed in the transaction itself so that it wouldn’t be
considered as a failure.
4. **Concurrency control enforcement –**The concurrency control method may decide to abort the transaction, to
start again because it basically violates serializability or we can say
that several processes are in a deadlock.
5. **Disk failure –**This type of failure basically occur when some disk loses their data because of a read or write malfunction or because of a disk read/write head
crash. This may happen during a read /write operation of the
transaction.
6. **Castropher –**These are also known as
physical problems it basically refers to the endless list of problems
that include power failure or air-conditioning failure, fire, theft
sabotage overwriting disk or tapes by mistake and mounting of the wrong
tape by the operator.
*** Transaction Isolation Levels in DBMS
**Prerequisite –** [Concurrency control in DBMS](https://www.geeksforgeeks.org/concurrency-control-introduction/), [ACID Properties in DBMS](https://www.geeksforgeeks.org/acid-properties-in-dbms/)

As
 we know that, in order to maintain consistency in a database, it 
follows ACID properties. Among these four properties (Atomicity, 
Consistency, Isolation, and Durability) Isolation determines how 
transaction integrity is visible to other users and systems. It means 
that a transaction should take place in a system in such a way that it 
is the only transaction that is accessing the resources in a database 
system. Isolation levels define the degree to which a transaction 
must be isolated from the data modifications made by any other 
transaction in the database system. A transaction isolation level is 
defined by the following phenomena –

- **Dirty Read –** A Dirty read is a situation when a transaction reads data that has not
yet been committed. For example, Let’s say transaction 1 updates a row
and leaves it uncommitted, meanwhile, Transaction 2 reads the updated
row. If transaction 1 rolls back the change, transaction 2 will have
read data that is considered never to have existed.
- **Non Repeatable read –** Non Repeatable read occurs when a transaction reads the same row twice and
gets a different value each time. For example, suppose transaction T1
reads data. Due to concurrency, another transaction T2 updates the same
data and commit, Now if transaction T1 rereads the same data, it will
retrieve a different value.
- **Phantom Read –** Phantom Read occurs when two same queries are executed, but the rows retrieved
by the two, are different. For example, suppose transaction T1 retrieves a set of rows that satisfy some search criteria. Now, Transaction T2
generates some new rows that match the search criteria for transaction
T1. If transaction T1 re-executes the statement that reads the rows, it
gets a different set of rows this time.

Based on these phenomena, The SQL standard defines four isolation levels :

1. **Read Uncommitted –** Read Uncommitted is the lowest isolation level. In this level, one
transaction may read not yet committed changes made by other
transactions, thereby allowing dirty reads. At this level, transactions
are not isolated from each other.
2. **Read Committed –** This isolation level guarantees that any data read is committed at the
moment it is read. Thus it does not allow dirty read. The transaction
holds a read or write lock on the current row, and thus prevents other
transactions from reading, updating, or deleting it.
3. **Repeatable Read –** This is the most restrictive isolation level. The transaction holds read
locks on all rows it references and writes locks on referenced rows for
update and delete actions. Since other transactions cannot read, update
or delete these rows, consequently it avoids non-repeatable read.
4. **Serializable –** This is the highest isolation level. A *serializable* execution is guaranteed to be serializable. Serializable execution is
defined to be an execution of operations in which concurrently executing transactions appears to be serially executing.

The Table is given below clearly depicts the relationship between isolation levels, read phenomena, and locks :

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/transactnLevel.png

Anomaly
 Serializable is not the same as Serializable. That is, it is necessary,
 but not sufficient that a Serializable schedule should be free of all 
three phenomena types.

**References –** [Isolation – Wikipedia](https://en.wikipedia.org/wiki/Isolation_(database_systems)) [Transaction Isolation Levels – docs.microsoft](https://docs.microsoft.com/en-us/sql/odbc/reference/develop-app/transaction-isolation-levels)
*** Types of Schedules in DBMS
Schedule,
 as the name suggests, is a process of lining the transactions and 
executing them one by one. When there are multiple transactions that are
 running in a concurrent manner and the order of operation is needed to 
be set so that the operations do not overlap each other, Scheduling is 
brought into play and the transactions are timed accordingly. The basics
 of Transactions and Schedules is discussed in [Concurrency Control (Introduction)](https://www.geeksforgeeks.org/concurrency-control-introduction/), and [Transaction Isolation Levels in DBMS](https://www.geeksforgeeks.org/transaction-isolation-levels-dbms/) articles. Here we will discuss various types of schedules.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190813142109/Types-of-schedules-in-DBMS-1.jpg

1. **Serial Schedules:**Schedules in which the transactions are executed non-interleaved, i.e., a serial
schedule is one in which no transaction starts until a running
transaction has ended are called serial schedules.
    
    **Example:** Consider the following schedule involving two transactions T1 and T2.
    
    [Untitled Database](https://www.notion.so/83d9a087f7114c0ab6139993ea6cb849?pvs=21)
    
    where R(A) denotes that a read operation is performed on some data item ‘A’This is a serial schedule since the transactions perform serially in the order T1 —> T2
    
2. **Non-Serial Schedule:**This is a type of Scheduling where the operations of multiple transactions
are interleaved. This might lead to a rise in the concurrency problem.
The transactions are executed in a non-serial manner, keeping the end
result correct and same as the serial schedule. Unlike the serial
schedule where one transaction must wait for another to complete all its operation, in the non-serial schedule, the other transaction proceeds
without waiting for the previous transaction to complete. This sort of
schedule does not provide any benefit of the concurrent transaction. It
can be of two types namely, Serializable and Non-Serializable Schedule.
    
    The Non-Serial Schedule can be divided further into Serializable and Non-Serializable.
    
    1. **Serializable:**This is used to maintain the consistency of the database. It is mainly used
    in the Non-Serial scheduling to verify whether the scheduling will lead
    to any inconsistency or not. On the other hand, a serial schedule does
    not need the serializability because it follows a transaction only when
    the previous transaction is complete. The non-serial schedule is said to be in a serializable schedule only when it is equivalent to the serial
    schedules, for an n number of transactions. Since concurrency is allowed in this case thus, multiple transactions can execute concurrently. A
    serializable schedule helps in improving both resource utilization and
    CPU throughput. These are of two types:
        1. **[Conflict Serializable:](https://www.geeksforgeeks.org/conflict-serializability/)**A schedule is called conflict serializable if it can be transformed into a serial schedule by swapping non-conflicting operations. Two operations
        are said to be conflicting if all conditions satisfy:
            - They belong to different transactions
            - They operate on the same data item
            - At Least one of them is a write operation
        2. **[View Serializable:](https://www.geeksforgeeks.org/dbms-how-to-test-two-schedule-are-view-equal-or-not-2/)**A Schedule is called view serializable if it is view equal to a serial
        schedule (no overlapping transactions). A conflict schedule is a view
        serializable but if the serializability contains blind writes, then the
        view serializable does not conflict serializable.
    2. **Non-Serializable:**The non-serializable schedule is divided into two types, Recoverable and Non-recoverable Schedule.
        1. **[Recoverable Schedule:](https://www.geeksforgeeks.org/recoverability-in-dbms/)**Schedules in which transactions commit only after all transactions whose changes
        they read commit are called recoverable schedules. In other words, if
        some transaction T is reading value updated or written by some other transaction T, then the commit of T must occur after the commit of T.
            
            j
            
            i
            
            j
            
            i
            
            **Example –** Consider the following schedule involving two transactions T1 and T2.
            
            [Untitled Database](https://www.notion.so/dbd0a831153046a2ba42b03cf03d8b16?pvs=21)
            
            This is a recoverable schedule since T1 commits before T2, that makes the value read by T2 correct.
            
            There can be three types of recoverable schedule:
            
            1. **Cascading Schedule:**Also called Avoids cascading aborts/rollbacks (ACA). When there is a failure in one transaction and this leads to the rolling back or aborting other dependent transactions, then such scheduling is referred to as
            Cascading rollback or cascading abort. Example:
                
                !https://media.geeksforgeeks.org/wp-content/uploads/6-54.png
                
            2. **[Cascadeless Schedule:](https://www.geeksforgeeks.org/cascadeless-in-dbms/)**Schedules in which transactions read values only after all transactions whose
            changes they are going to read commit are called cascadeless schedules.
            Avoids that a single transaction abort leads to a series of transaction
            rollbacks. A strategy to prevent cascading aborts is to disallow a
            transaction from reading uncommitted changes from another transaction in the same schedule.
                
                In other words, if some transaction Tj wants to read value updated or written by some other transaction Ti, then the commit of Tj must read it after the commit of Ti.
                
                **Example:** Consider the following schedule involving two transactions T1 and T2.
                
                [Untitled Database](https://www.notion.so/3f28db5fb5d4423ebfacf63eda02fc31?pvs=21)
                
                This schedule is cascadeless. Since the updated value of **A** is read by T2 only after the updating transaction i.e. T1 commits.
                
                **Example:** Consider the following schedule involving two transactions T1 and T2.
                
                [Untitled Database](https://www.notion.so/def0ac6ada8248c39a8cb802b5851544?pvs=21)
                
                It is a recoverable schedule but it does not avoid cascading aborts. It can be seen that if T1 aborts, T2 will have to be aborted too in order to maintain the correctness of the schedule as T2 has already read the uncommitted value written by T1.
                
            3. **Strict Schedule:**A schedule is strict if for any two transactions T, T, if a write operation of T precedes a conflicting operation of T (either read or write), then the commit or abort event of T also precedes that conflicting operation of T.In other words, T can read or write updated or written value of T only after T commits/aborts.
                
                i
                
                j
                
                i
                
                j
                
                i
                
                j
                
                j
                
                i
                
                i
                
                **Example:** Consider the following schedule involving two transactions T1 and T2.
                
                [Untitled Database](https://www.notion.so/8f7de398df524f2a9dd34b252863edc6?pvs=21)
                
                This is a strict schedule since T2 reads and writes A which is written by T1 only after the commit of T1.
                
        2. **Non-Recoverable Schedule:Example:** Consider the following schedule involving two transactions T and T.
            
            1
            
            2
            
            [Untitled Database](https://www.notion.so/585a095522694a979e39d3f3ea613333?pvs=21)
            
            T2 read the value of A written by T1, and committed. T1 later aborted, therefore the value read by T2 is wrong, but since T2 committed, this schedule is **non-recoverable**.
            

**Note –** It can be seen that:

1. Cascadeless schedules are stricter than recoverable schedules or are a subset of recoverable schedules.
2. Strict schedules are stricter than cascadeless schedules or are a subset of cascadeless schedules.
3. Serial schedules satisfy constraints of all recoverable, cascadeless and
strict schedules and hence is a subset of strict schedules.

The relation between various types of schedules can be depicted as:

!https://media.geeksforgeeks.org/wp-content/uploads/Types-of-schedules.png

**Example:** Consider the following schedule:

```
S:R1(A), W2(A), Commit2, W1(A), W3(A), Commit3, Commit1
```

Which of the following is true?(A) The schedule is view serializable schedule and strict recoverable schedule(B) The schedule is non-serializable schedule and strict recoverable schedule(C) The schedule is non-serializable schedule and is not strict recoverable schedule.(D) The Schedule is serializable schedule and is not strict recoverable schedule

**Solution:** The schedule can be re-written as:-

[Untitled Database](https://www.notion.so/cb3cd99ae65848f99f832ae1dc3d5b50?pvs=21)

First of all, it is a view serializable schedule as it has view equal serial schedule T1 —> T2 —> T3
 which satisfies the initial and updated reads and final write on 
variable A which is required for view serializability. Now we can see 
there is write – write pair done by transactions T1 followed by T3 which is violating the above-mentioned condition of strict schedules as T3 is supposed to do write operation only after T1 commits which is violated in the given schedule. Hence the given schedule is serializable but not strict recoverable.So, option (D) is correct.
*** Types of Schedules based Recoverability in DBMS
Generally, there are three types of schedule given as follows:

!https://media.geeksforgeeks.org/wp-content/uploads/1-264.png

**1. Recoverable Schedule –** A
 schedule is said to be recoverable if it is recoverable as name 
suggest. Only reads are allowed before write operation on same data. 
Only reads (Ti->Tj) is permissible.

**Example –** 

```
S1: R1(x),W1(x), R2(x), R1(y), R2(y),
W2(x), W1(y),C1,C2;
```

Given schedule follows order of **Ti->Tj => C1->C2**.
 Transaction T1 is executed before T2 hence there is no chances of 
conflict occur. R1(x) appears before W1(x) and transaction T1 is 
committed before T2 i.e. completion of first transaction performed first
 update on data item x, hence given schedule is recoverable.

Lets see example of **unrecoverable schedule** to clear the concept more: 

```
S2: R1(x), R2(x), R1(z), R3(x), R3(y), W1(x),
W3(y), R2(y), W2(z),W2(y), C1,C2,C3;
```

**Ti->Tj => C2->C3**
 but W3(y) executed before W2(y) which leads to conflicts thus it must 
be committed before T2 transaction. So given schedule is unrecoverable. 
if **Ti->Tj => C3->C2** is given in schedule then it will become recoverable schedule.

**Note:**
 A committed transaction should never be rollback. It means that reading
 value from uncommitted transaction and commit it will enter the current
 transaction into inconsistent or unrecoverable state this is called *Dirty Read problem*.

**Example:**

!https://media.geeksforgeeks.org/wp-content/uploads/2-189.png

**2. Cascadeless Schedule –** When no **read** or **write-write** occurs before execution of transaction then corresponding schedule is called cascadeless schedule.

**Example –** 

```
S3: R1(x), R2(z), R3(x), R1(z), R2(y), R3(y), W1(x), C1,
         W2(z),W3(y),W2(y),C3,C2;
```

In this schedule **W3(y)** and **W2(y)** overwrite conflicts and there is no read, therefore given schedule is cascadeless schedule.

**Special Case –** A
 committed transaction desired to abort. As given below all the 
transactions are reading committed data hence it’s cascadeless 
schedule.

!https://media.geeksforgeeks.org/wp-content/uploads/3-151.png

**3. Strict Schedule –** if schedule contains no **read** or **write** before commit then it is known as strict schedule. Strict schedule is strict in nature.

**Example –** 

```
S4: R1(x), R2(x), R1(z), R3(x), R3(y),
W1(x),C1,W3(y),C3, R2(y),W2(z),W2(y),C2;
```

In this schedule no read-write or write-write conflict arises before commit hence its strict schedule:

!https://media.geeksforgeeks.org/wp-content/uploads/4-96.png

**4. Cascading Abort –** Cascading
 Abort can also be rollback. If transaction T1 abort as T2 read data 
that written by T1 which is not committed. Hence it’s cascading 
rollback.

**Example:**

!https://media.geeksforgeeks.org/wp-content/uploads/6-54.png

Correlation between Strict, Cascadeless and Recoverable schedule:

!https://media.geeksforgeeks.org/wp-content/uploads/7-43.png

From above figure: 

1. Strict schedules are all recoverable and cascadeless schedules 
2. All cascadeless schedules are recoverable 

Types of Schedules based Recoverability in DBMS
*** Conflict Serializability in DBMS
As discussed in [Concurrency control](https://www.geeksforgeeks.org/concurrency-control-in-dbms/),
 serial schedules have less resource utilization and low throughput. To 
improve it, two or more transactions are run concurrently. But 
concurrency of transactions may lead to inconsistency in database. To 
avoid this, we need to check whether these concurrent schedules are 
serializable or not.

**Conflict Serializable:**
 A schedule is called conflict serializable if it can be transformed 
into a serial schedule by swapping non-conflicting operations.

**Conflicting operations:** Two operations are said to be conflicting if all conditions satisfy:

- They belong to different transactions
- They operate on the same data item
- At Least one of them is a write operation

Example: –

- **Conflicting** operations pair (R(A), W(A)) because they belong to two different transactions on same data item A and one of them is write operation.
    
    1
    
    2
    
- Similarly, (W(A), W(A)) and (W(A), R(A)) pairs are also **conflicting**.
    
    1
    
    2
    
    1
    
    2
    
- On the other hand, (R(A), W(B)) pair is **non-conflicting** because they operate on different data item.
    
    1
    
    2
    
- Similarly, ((W(A), W(B)) pair is **non-conflicting.**
    
    1
    
    2
    

Consider the following schedule:

```
S1: R1(A), W1(A), R2(A), W2(A), R1(B), W1(B), R2(B), W2(B)

```

If Oi and Oj are two operations in a transaction and Oi< Oj (Oi is executed before Oj), same order will follow in the schedule as well. Using this property, we can get two transactions of schedule S1 as: 

```
T1: R1(A), W1(A), R1(B), W1(B)
T2: R2(A), W2(A), R2(B), W2(B)

```

**Possible Serial Schedules are: T1->T2 or T2->T1**

- > **Swapping non-conflicting operation**s R(A) and R(B) in S1, the schedule becomes,
    
    2
    
    1
    

```
S11: R1(A), W1(A), R1(B),W2(A), R2(A),W1(B), R2(B), W2(B)

```

- > Similarly, s**wapping non-conflicting operations** W(A) and W(B) in S11, the schedule becomes,
    
    2
    
    1
    

```
S12: R1(A), W1(A), R1(B), W1(B), R2(A), W2(A), R2(B), W2(B)

```

S12 is a serial schedule in which all operations of T1 are 
performed before starting any operation of T2. Since S has been 
transformed into a serial schedule S12 by swapping non-conflicting 
operations of S1, S1 is conflict serializable.

Let us take another Schedule:

```
S2: R2(A), W2(A), R1(A), W1(A), R1(B), W1(B), R2(B), W2(B)

```

Two transactions will be:

```
T1: R1(A), W1(A), R1(B), W1(B)
T2: R2(A), W2(A), R2(B), W2(B)

```

**Possible Serial Schedules are: T1->T2 or T2->T1**

Original Schedule is:

```
S2: R2(A), W2(A),R1(A), W1(A), R1(B), W1(B),R2(B), W2(B)

```

Swapping non-conflicting operations R1(A) and R2(B) in S2, the schedule becomes,

```
S21: R2(A), W2(A), R2(B),W1(A), R1(B), W1(B), R1(A),W2(B)
```

Similarly, swapping non-conflicting operations W1(A) and W2(B) in S21, the schedule becomes,

```
S22: R2(A), W2(A), R2(B), W2(B), R1(B), W1(B), R1(A), W1(A)

```

In schedule S22, all operations of T2 are performed first, but operations of T1 are not in order (order should be R1(A), W1(A), R1(B), W1(B)). So S2 is not conflict serializable.

**Conflict Equivalent:**
 Two schedules are said to be conflict equivalent when one can be 
transformed to another by swapping non-conflicting operations. In the 
example discussed above, S11 is conflict equivalent to S1 (S1 can be 
converted to S11 by swapping non-conflicting operations). Similarly, S11
 is conflict equivalent to S12 and so on.

***Note 1:**
 Although S2 is not conflict serializable, but still it is conflict 
equivalent to S21 and S21 because S2 can be converted to S21 and S22 by 
swapping non-conflicting operations.*

***Note 2:**
 The schedule which is conflict serializable is always conflict 
equivalent to one of the serial schedule. S1 schedule discussed above 
(which is conflict serializable) is equivalent to serial schedule 
(T1->T2).*

**Question:** **Consider the following schedules involving two transactions. Which one of the following statement is true?**

S1: R1(X) R1(Y) R2(X) R2(Y) W2(Y) W1(X) S2: R1(X) R2(X) R2(Y) W2(Y) R1(Y) W1(X)

- Both S1 and S2 are conflict serializable
- Only S1 is conflict serializable
- Only S2 is conflict serializable
- None

[GATE 2007]

**Solution:** Two transactions of given schedules are:

```
 T1: R1(X) R1(Y) W1(X)
 T2: R2(X) R2(Y) W2(Y)

```

Let us first check serializability of S1:

```
S1: R1(X) R1(Y) R2(X) R2(Y) W2(Y) W1(X)

```

To convert it to a serial schedule, we have to swap 
non-conflicting operations so that S1 becomes equivalent to serial 
schedule T1->T2 or T2->T1. In this case, to convert it to a serial
 schedule, we must have to swap R2(X) and W1(X) but they are conflicting. So S1 can’t be converted to a serial schedule.

Now, let us check serializability of S2:

```
S2:R1(X) R2(X) R2(Y) W2(Y) R1(Y) W1(X)

```

Swapping non conflicting operations R1(X) and R2(X) of S2, we get

```
S2’: R2(X)R1(X) R2(Y) W2(Y) R1(Y) W1(X)

```

Again, swapping non conflicting operations R1(X) and R2(Y) of S2’, we get

```
S2’’: R2(X) R2(Y)R1(X) W2(Y) R1(Y) W1(X)

```

Again, swapping non conflicting operations R1(X) and W2(Y) of S2’’, we get

```
S2’’’: R2(X) R2(Y) W2(Y) R1(X) R1(Y) W1(X)

```

which is equivalent to a serial schedule T2->T1.

So, **correct option is C**. Only S2 is conflict serializable.

**Related Article:** [View Serializability](https://www.geeksforgeeks.org/view-serializability-in-dbms-transactions/) [Precedence Graph For Testing Conflict Serializability](https://www.geeksforgeeks.org/precedence-graph-testing-conflict-serializability/)

Article
 contributed by Sonal Tuteja. Please write comments if you find anything
 incorrect, or you want to share more information about the topic 
discussed above
*** Precedence Graph For Testing Conflict Serializability in DBMS
Prerequisite: [Conflict Serializability](https://www.geeksforgeeks.org/conflict-serializability/)

**Precedence Graph** or **Serialization Graph** is used commonly to test Conflict Serializability of a schedule.It is a directed Graph (V, E) consisting of a set of nodes V = {T1, T2, T3……….Tn} and a set of directed edges E = {e1, e2, e3………………em}.The graph contains one node for each Transaction Ti. An edge ei is of the form Tj –> Tk where Tj is the starting node of ei and Tk is the ending node of ei. An edge ei is constructed between nodes Tj to Tk if one of the operations in Tj appears in the schedule before some conflicting operation in Tk .

The Algorithm can be written as:

1. Create a node T in the graph for each participating transaction in the schedule.
2. For the conflicting operation read_item(X) and write_item(X) – If a Transaction T executes a read_item (X) after T executes a write_item (X), draw an edge from T to T in the graph.
    
    j
    
    i
    
    i
    
    j
    
3. For the conflicting operation write_item(X) and read_item(X) – If a Transaction T executes a write_item (X) after T executes a read_item (X), draw an edge from T to T in the graph.
    
    j
    
    i
    
    i
    
    j
    
4. For the conflicting operation write_item(X) and write_item(X) – If a Transaction T executes a write_item (X) after T executes a write_item (X), draw an edge from T to T in the graph.
    
    j
    
    i
    
    i
    
    j
    
5. **The Schedule S is serializable if there is no cycle in the precedence graph**.

If
 there is no cycle in the precedence graph, it means we can construct a 
serial schedule S’ which is conflict equivalent to schedule S.The serial schedule S’ can be found by [Topological Sorting](https://www.geeksforgeeks.org/topological-sorting-indegree-based-solution/) of the acyclic precedence graph. Such schedules can be more than 1.

For example,Consider the schedule S :

```
 S : r1(x) r1(y) w2(x) w1(x) r2(y)
```

**Creating Precedence graph:**

1. Make two nodes corresponding to Transaction T and T.
    
    1
    
    2
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/2-11.png
    
2. For the conflicting pair r1(x) w2(x), where r1(x) happens before w2(x), draw an edge from T to T.
    
    1
    
    2
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/3-10.png
    
3. For the conflicting pair w2(x) w1(x), where w2(x) happens before w1(x), draw an edge from T to T.
    
    2
    
    1
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/4-9.png
    

Since the graph is cyclic, we can conclude that it is **not conflict serializable** to any schedule serial schedule.Let us try to infer a serial schedule from this graph using topological ordering.The edge T1–>T2 tells that T1 should come before T2 in the linear ordering.The edge T2 –> T1 tells that T2 should come before T1 in the linear ordering.So,
 we can not predict any particular order (when the graph is cyclic). 
Therefore, no serial schedule can be obtained from this graph.Consider the another schedule S1 :

```
 S1: r1(x) r3(y) w1(x) w2(y) r3(x) w2(x)
```

The graph for this schedule is :

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/22-7.png

Since
 the graph is acyclic, the schedule is conflict serializable. Performing
 Topological Sort on this graph would give us a possible serial schedule
 that is conflict equivalent to schedule S1.In Topological Sort, we first select the node with in-degree 0, which is T1. This would be followed by T3 and T2.So, **S1 is conflict serializable** since it is **conflict equivalent** to the **serial schedule T1 T3 T2.**

Source: Operating Systems book, Silberschatz, Galvin and Gagne

This article is contributed by **Saloni Baweja**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](https://write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.
 
*** Condition of schedules to View-equivalent
Two schedules S1 and S2 are said to be view-equivalent if below conditions are satisfied :

**1) Initial Read** If a transaction T1 reading data item A from database in S1 then in S2 also T1 should read A from database.

```
  T1     T2     T3
-------------------
        R(A)
 W(A)
               R(A)
        R(B)
```

Transaction T2 is reading A from database.

**2)Updated Read** If Ti is reading A which is updated by Tj in S1 then in S2 also Ti should read A which is updated by Tj.

```
  T1     T2     T3         T1    T2    T3
-------------------       ----------------
 W(A)                    W(A)
         W(A)                               R(A)
                 R(A)               W(A)
```

Above two schedule are not view-equivalent as in S1 :T3 is reading A updated by T2, in S2 T3 is reading A updated by T1.

**3)Final Write operation** If a transaction T1 updated A at last in S1, then in S2 also T1 should perform final write operations.

```
 T1       T2        T1     T2
------------    ---------------
 R(A)              R(A)
         W(A)     W(A)
 W(A)                    W(A)
```

Above two schedules are not view-equivalent as Final write operation in S1 is done by T1 while in S2 done by T2.

**View Serializability:** A Schedule is called view serializable if it is view equal to a serial schedule (no overlapping transactions).

Below are the previous Year Gate Question asked on this topic https://www.geeksforgeeks.org/dbms-gq/transactions-and-concurrency-control-gq/

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Types of Schedules based Recoverability in DBMS
*** Deadlock in DBMS
In a
 database, a deadlock is an unwanted situation in which two or more 
transactions are waiting indefinitely for one another to give up locks. 
Deadlock is said to be one of the most feared complications in DBMS as 
it brings the whole system to a Halt.

**Example –** let us understand the concept of Deadlock with an example : Suppose, Transaction T1 holds a lock on some rows in the Students table and **needs to update** some rows in the Grades table. Simultaneously, Transaction **T2 holds** locks on those very rows (Which T1 needs to update) in the Grades table **but needs** to update the rows in the Student table **held by Transaction T1**.

Now,
 the main problem arises. Transaction T1 will wait for transaction T2 to
 give up the lock, and similarly, transaction T2 will wait for 
transaction T1 to give up the lock. As a consequence, All activity comes
 to a halt and remains at a standstill forever unless the DBMS detects 
the deadlock and aborts one of the transactions.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/deadlock.png

Deadlock in DBMS

**Deadlock Avoidance –** When
 a database is stuck in a deadlock, It is always better to avoid the 
deadlock rather than restarting or aborting the database. The deadlock 
avoidance method is suitable for smaller databases whereas the deadlock 
prevention method is suitable for larger databases. One method of 
avoiding deadlock is using application-consistent logic. In the 
above-given example, Transactions that access Students and  Grades 
should always access the tables in the same order. In this way, in the 
scenario described above, Transaction T1 simply waits for transaction T2
 to release the lock on  Grades before it begins. When transaction T2 
releases the lock, Transaction T1 can proceed freely. Another method
 for avoiding deadlock is to apply both row-level locking mechanism and 
READ COMMITTED isolation level. However, It does not guarantee to remove
 deadlocks completely.

**Deadlock Detection –** When
 a transaction waits indefinitely to obtain a lock, The database 
management system should detect whether the transaction is involved in a
 deadlock or not.

**Wait-for-graph** is one of the 
methods for detecting the deadlock situation. This method is suitable 
for smaller databases. In this method, a graph is drawn based on the 
transaction and their lock on the resource. If the graph created has a 
closed-loop or a cycle, then there is a deadlock. For the above-mentioned scenario, the Wait-For graph is drawn below

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/transaction1.png

**Deadlock prevention –** For
 a large database, the deadlock prevention method is suitable. A 
deadlock can be prevented if the resources are allocated in such a way 
that deadlock never occurs. The DBMS analyzes the operations whether 
they can create a deadlock situation or not, If they do, that 
transaction is never allowed to be executed.

Deadlock prevention mechanism proposes two schemes :

- **Wait-Die Scheme –** In this scheme, If a transaction requests a resource that is locked by
another transaction, then the DBMS simply checks the timestamp of both
transactions and allows the older transaction to wait until the resource is available for execution. Suppose, there are two transactions T1
and T2, and Let the timestamp of any transaction T be TS (T). Now, If
there is a lock on T2 by some other transaction and T1 is requesting for resources held by T2, then DBMS performs the following actions:
    
    Checks
     if TS (T1) < TS (T2) – if T1 is the older transaction and T2 has 
    held some resource, then it allows T1 to wait until resource is 
    available for execution. That means if a younger transaction has locked 
    some resource and an older transaction is waiting for it, then an older 
    transaction is allowed to wait for it till it is available. If T1 is an 
    older transaction and has held some resource with it and if T2 is 
    waiting for it, then T2 is killed and restarted later with random delay 
    but with the same timestamp. i.e. if the older transaction has held some
     resource and the younger transaction waits for the resource, then the 
    younger transaction is killed and restarted with a very minute delay 
    with the same timestamp. This scheme allows the older transaction to wait but kills the younger one. 
    
- **Wound Wait Scheme –** In this scheme, if an older transaction requests for a resource held by a
younger transaction, then an older transaction forces a younger
transaction to kill the transaction and release the resource. The
younger transaction is restarted with a minute delay but with the same
timestamp. If the younger transaction is requesting a resource that is
held by an older one, then the younger transaction is asked to wait till the older one releases it.

**Following table list the differences between Wait – Die and Wound -Wait scheme prevention schemes :**

[Untitled Database](https://www.notion.so/4a4f15bbdd8b4394aa00bb332f1b0015?pvs=21)

**References –** [docs.oracle](https://docs.oracle.com/javadb/10.8.3.0/devguide/derbydev.pdf) [difference between wait-die and wound-wait](https://stackoverflow.com/questions/32794142/what-is-the-difference-between-wait-die-and-wound-wait) Book – [Fundamentals of Database Systems by Navathe](https://amzn.to/2TjTV7l)
*** Starvation in DBMS
Starvation or Livelock is the situation when a transaction has to wait for an indefinite period of time to acquire a lock.

### **Reasons for Starvation –**

- If the waiting scheme for locked items is unfair. ( priority queue )
- Victim selection (the same transaction is selected as a victim repeatedly )
- Resource leak.
- Via denial-of-service attack.

Starvation can be best explained with the help of an example –

Suppose
 there are 3 transactions namely T1, T2, and T3 in a database that is 
trying to acquire a lock on data item ‘ I ‘. Now, suppose the scheduler 
grants the lock to T1(maybe due to some priority), and the other two 
transactions are waiting for the lock. As soon as the execution of T1 is
 over, another transaction T4 also comes over and requests a lock on 
data item I. Now, this time the scheduler grants lock to T4, and T2, T3 
has to wait again. In this way, if new transactions keep on requesting 
the lock, T2 and T3 may have to wait for an indefinite period of time, 
which leads to **Starvation**.

### **Solutions to starvation –**

1. **Increasing Priority –** Starvation occurs when a transaction has to wait for an indefinite time, In this
situation, we can increase the priority of that particular
transaction/s. But the drawback with this solution is that it may happen that the other transaction may have to wait longer until the highest
priority transaction comes and proceeds.
2. **Modification in Victim Selection algorithm –** If a transaction has been a victim of repeated selections, then the
algorithm can be modified by lowering its priority over other
transactions.
3. **First Come First Serve approach –** A fair scheduling approach i.e FCFS can be adopted, In which the
transaction can acquire a lock on an item in the order, in which the
requested the lock.
4. **Wait-die and wound wait scheme –** These are the schemes that use the timestamp ordering mechanism of transactions. For detailed study refer: [Wait die and Wound wait scheme](https://www.geeksforgeeks.org/deadlock-in-dbms/)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

# Starvation in DBMS

- Difficulty Level :
[Easy](https://www.geeksforgeeks.org/easy/)
- Last Updated :
24 Dec, 2021

Starvation or Livelock is the situation when a transaction has to wait for an indefinite period of time to acquire a lock.

### **Reasons for Starvation –**

- If the waiting scheme for locked items is unfair. ( priority queue )
- Victim selection (the same transaction is selected as a victim repeatedly )
- Resource leak.
- Via denial-of-service attack.

Starvation can be best explained with the help of an example –

Suppose
 there are 3 transactions namely T1, T2, and T3 in a database that is 
trying to acquire a lock on data item ‘ I ‘. Now, suppose the scheduler 
grants the lock to T1(maybe due to some priority), and the other two 
transactions are waiting for the lock. As soon as the execution of T1 is
 over, another transaction T4 also comes over and requests a lock on 
data item I. Now, this time the scheduler grants lock to T4, and T2, T3 
has to wait again. In this way, if new transactions keep on requesting 
the lock, T2 and T3 may have to wait for an indefinite period of time, 
which leads to **Starvation**.

### **Solutions to starvation –**

1. **Increasing Priority –** Starvation occurs when a transaction has to wait for an indefinite time, In this
situation, we can increase the priority of that particular
transaction/s. But the drawback with this solution is that it may happen that the other transaction may have to wait longer until the highest
priority transaction comes and proceeds.
2. **Modification in Victim Selection algorithm –** If a transaction has been a victim of repeated selections, then the
algorithm can be modified by lowering its priority over other
transactions.
3. **First Come First Serve approach –** A fair scheduling approach i.e FCFS can be adopted, In which the
transaction can acquire a lock on an item in the order, in which the
requested the lock.
4. **Wait-die and wound wait scheme –** These are the schemes that use the timestamp ordering mechanism of transactions. For detailed study refer: [Wait die and Wound wait scheme](https://www.geeksforgeeks.org/deadlock-in-dbms/)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Lock Based Concurrency Control Protocol in DBMS
First things first, I hope you are familiar to some of the concepts [relating to Transactions](https://www.geeksforgeeks.org/concurrency-control-introduction/).

- What is a [Recoverable Schedule](https://www.geeksforgeeks.org/dbms-recoverability-of-schedules/)?
- What are Cascading Rollbacks and Cascadeless schedules?
- Determining [if a schedule is Conflict Serializable](https://www.geeksforgeeks.org/precedence-graph-testing-conflict-serializability/).

Now, we all know the four properties a transaction must follow. Yes, you got that right, I mean the **[ACID** properties](https://www.geeksforgeeks.org/acid-properties-in-dbms/). Concurrency control techniques are used to ensure that the *Isolation* (or non-interference) property of concurrently executing transactions is maintained.

*A
 trivial question I would like to pose in front of you, (I know you must
 know this but still) why do you think that we should have interleaving 
execution of transactions if it may lead to problems such as 
Irrecoverable Schedule, Inconsistency and many more threats.Why not just let it be Serial schedules and we may live peacefully, no complications at all.*

Yes, the performance effects the efficiency too much which is not acceptable.Hence
 a Database may provide a mechanism that ensures that the schedules are 
either conflict or view serializable and recoverable (also preferably 
cascadeless). Testing for a schedule for Serializability after it has 
executed is obviously *too late!*So we need Concurrency Control Protocols that ensures Serializability.

**Concurrency-control protocols :**
 allow concurrent schedules, but ensure that the schedules are 
conflict/view serializable, and are recoverable and maybe even 
cascadeless.These protocols do not examine the precedence graph as 
it is being created, instead a protocol imposes a discipline that avoids
 non-serializable schedules.Different concurrency control protocols 
provide different advantages between the amount of concurrency they 
allow and the amount of overhead that they impose.We’ll be learning 
some protocols which are important for GATE CS. Questions from this 
topic is frequently asked and it’s recommended to learn this concept. 
(At the end of this series of articles I’ll try to list all theoretical 
aspects of this concept for students to revise quickly and they may find
 the material in one place.) Now, let’s get going:

Different categories of protocols:

- **Lock Based Protocol**
    - Basic 2-PL
    - Conservative 2-PL
    - Strict 2-PL
    - Rigorous 2-PL
- **Graph Based Protocol**
- **Time-Stamp Ordering Protocol**
- **Multiple Granularity Protocol**
- **Multi-version Protocol**

For GATE we’ll be focusing on the First three protocols.

**Lock Based Protocols –**A
 lock is a variable associated with a data item that describes a status 
of data item with respect to possible operation that can be applied to 
it. They synchronize the access by concurrent transactions to the 
database items. It is required in this protocol that all the data items 
must be accessed in a mutually exclusive manner. Let me introduce you to
 two common locks which are used and some terminology followed in this 
protocol.

1. **Shared Lock (S):** also known as
Read-only lock. As the name suggests it can be shared between
transactions because while holding this lock the transaction does not
have the permission to update data on the data item. S-lock is requested using lock-S instruction.
2. **Exclusive Lock (X):**
Data item can be both read as well as written.This is Exclusive and
cannot be held simultaneously on the same data item. X-lock is requested using lock-X instruction.

**Lock Compatibility Matrix –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/1-28.png

- A transaction may be granted a lock on an item if the requested lock is compatible with locks already held on the item by othertransactions.
- Any number of transactions can hold shared locks on an item, but if any
transaction holds an exclusive(X) on the item no other transaction may
hold any lock on the item.
- If a lock cannot be granted, the
requesting transaction is made to wait till all incompatible locks held
by other transactions have been released. Then the lock is granted.

**Upgrade / Downgrade locks :** A transaction that holds a lock on an item **A** is allowed under certain condition to change the lock state from one state to another.Upgrade: A S(A) can be upgraded to X(A) if Ti is the only transaction holding the S-lock on element A.Downgrade:
 We may downgrade X(A) to S(A) when we feel that we no longer want to 
write on data-item A. As we were holding X-lock on A, we need not check 
any conditions.So, by now we are introduced with the types of 
locks and how to apply them. But wait, just by applying locks if our 
problems could’ve been avoided then life would’ve been so simple! If you
 have done Process Synchronization under OS you must be familiar with 
one consistent problem, starvation and Deadlock! We’ll be discussing 
them shortly, but just so you know we have to apply Locks but they must 
follow a set of protocols to avoid such undesirable problems. Shortly 
we’ll use 2-Phase Locking (2-PL) which will use the concept of Locks to 
avoid deadlock. So, applying simple locking, we may not always produce 
Serializable results, it may lead to Deadlock Inconsistency.

### Problem With Simple Locking…

Consider the Partial Schedule:

[Untitled Database](https://www.notion.so/1475360d581242caa44d89421e5acb02?pvs=21)

**Deadlock –** consider the above execution phase. Now, **T1** holds an Exclusive lock over B, and **T2** holds a Shared lock over A. Consider Statement 7, **T2** requests for lock on B, while in Statement 8 **T1** requests lock on A. This as you may notice imposes a **Deadlock** as none can proceed with their execution.

**Starvation –**
 is also possible if concurrency control manager is badly designed. For 
example: A transaction may be waiting for an X-lock on an item, while a 
sequence of other transactions request and are granted an S-lock on the 
same item. This may be avoided if the concurrency control manager is 
properly designed.

Phew… I hope you are now familiar with why we 
should study Concurrency Control Protocols. Moreover, you should be 
familiar with basics of Lock Based Protocols and problems with Simple 
Locking.Next we’ll discuss 2-PL and its categories, implementation 
along with the advantages and pitfalls of using them. Questions on Lock 
based protocols are common in GATE, also we’ll further discuss about 
Graph based, Timestamp and some fun questions on Thomas Write Rule. Till
 then, happy learning.
 
*** Two Phase Locking Protocol
We have discussed briefly the [first type of Concurrency Control Protocol](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-lock-based-protocol/), i.e., Lock-based Protocol.

Now, recalling where we last left off, there are two types of Locks available **Shared S(a)** and **Exclusive X(a)**. Implementing this lock system without any restrictions gives us the Simple Lock-based protocol (or *Binary Locking*), but it has its own disadvantages, they do **not guarantee Serializability**. Schedules may follow the preceding rules but a non-serializable schedule may result.

To guarantee serializability, we must follow some additional protocol *concerning the positioning of locking and unlocking operations* in
 every transaction. This is where the concept of Two-Phase Locking(2-PL)
 comes into the picture, 2-PL ensures serializability. Now, let’s dig 
deep!

### Two-Phase Locking –

A transaction is said to follow the Two-Phase Locking protocol if Locking and Unlocking can be done in two phases.

1. **Growing Phase:** New locks on data items may be acquired but none can be released.
2. **Shrinking Phase:** Existing locks may be released but no new locks can be acquired.

**Note –**
 If lock conversion is allowed, then upgrading of lock( from S(a) to 
X(a) ) is allowed in the Growing Phase, and downgrading of lock (from 
X(a) to S(a)) must be done in shrinking phase.

Let’s see a transaction implementing 2-PL.

[Untitled Database](https://www.notion.so/1557e40877e14dcbae2e7444b25835af?pvs=21)

This is just a skeleton transaction that shows how unlocking and locking work with 2-PL. Note for: **Transaction T1**:

- The growing Phase is from steps 1-3.
- The shrinking Phase is from steps 5-7.
- Lock Point at 3

**Transaction T2**:

- The growing Phase is from steps 2-6.
- The shrinking Phase is from steps 8-9.
- Lock Point at 6

Hey, wait! What is **LOCK POINT?**
 The Point at which the growing phase ends, i.e., when a transaction 
takes the final lock it needs to carry on its work. Now look at the 
schedule, you’ll surely understand.

I have said that 2-PL ensures serializability, but there are still some drawbacks of 2-PL. Let’s glance at the drawbacks:

- [Cascading Rollback](https://www.geeksforgeeks.org/dbms-recoverability-of-schedules/) is possible under 2-PL.
- [Deadlocks](https://www.geeksforgeeks.org/deadlock-in-dbms/) and [Starvation](https://www.geeksforgeeks.org/starvation-in-dbms/) are possible.

**Cascading Rollbacks in 2-PL –** Let’s see the following Schedule:

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/12122.png

Take a moment to analyze the schedule. Yes, you’re correct, because of Dirty Read in T2 and T3 in lines 8 and 12 respectively, when T1 failed we have to roll back others also. Hence, **Cascading Rollbacks are possible in 2-PL.**
 I have taken skeleton schedules as examples because it’s easy to 
understand when it’s kept simple. When explained with real-time 
transaction problems with many variables, it becomes very complex.

**Deadlock in 2-PL –** Consider this simple example, it will be easy to understand. Say we have two transactions T1 and T2.

```
Schedule:   Lock-X1(A)   Lock-X2(B)  Lock-X1(B)  Lock-X2(A)
```

Drawing the precedence graph, you may detect the loop. So Deadlock is also possible in 2-PL.

Two-phase
 locking may also limit the amount of concurrency that occurs in a 
schedule because a Transaction may not be able to release an item after 
it has used it. This may be because of the protocols and other 
restrictions we may put on the schedule to ensure serializability, 
deadlock freedom, and other factors. This is the price we have to pay to
 ensure serializability and other factors, hence it can be considered as
 a bargain between concurrency and maintaining the ACID properties.

The above-mentioned type of 2-PL is called **Basic 2PL**. To sum it up it ensures Conflict Serializability but ***does not***
 prevent Cascading Rollback and Deadlock. Further, we will study three 
other types of 2PL, Strict 2PL, Conservative 2PL, and Rigorous 2PL.

**GATE related questions:**

1. [GATE CS 2016-2 | Question 61](https://www.geeksforgeeks.org/gate-gate-cs-2016-set-2-question-61/)
2. [GATE CS 1999 | Question 31](https://www.geeksforgeeks.org/gate-gate-cs-1999-question-31/)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Categories of Two Phase Locking (Strict, Rigorous & Conservative)
Now that we are familiar with what is Two-Phase [Locking (2-PL)](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-two-phase-locking-2-pl/)
 and the basic rules which should be followed which ensures 
serializability. Moreover, we came across problems with 2-PL, Cascading 
Aborts, and Deadlocks. Now, we turn towards the enhancements made on 
2-PL which try to make the protocol nearly error-free. Briefly, we allow
 some modifications to 2-PL to improve it.

There are three categories:

1. Strict 2-PL
2. Rigorous 2-PL
3. Conservative 2-PL

Now
 recall the rules followed in Basic 2-PL, over that we make some extra 
modifications. Let’s now see what are the modifications and what 
drawbacks they solve.

### Strict 2-PL –

This requires that in addition to the lock being 2-Phase **all Exclusive(X) locks** held by the transaction be released until *after* the Transaction Commits. Following Strict 2-PL ensures that our schedule is:

- Recoverable
- Cascadeless

Hence,
 it gives us freedom from Cascading Abort which was still there in Basic
 2-PL and moreover guarantee Strict Schedules but still, *Deadlocks are possible*!

### Rigorous 2-PL –

This requires that in addition to the lock being 2-Phase **all Exclusive(X) and Shared(S) locks** held by the transaction be released until *after* the Transaction Commits. Following Rigorous 2-PL ensures that our schedule is:

- Recoverable
- Cascadeless

Hence,
 it gives us freedom from Cascading Abort which was still there in Basic
 2-PL and moreover guarantee Strict Schedules but still, *Deadlocks are possible*!

> Note:
 The difference between Strict 2-PL and Rigorous 2-PL is that Rigorous 
is more restrictive, it requires both Exclusive and Shared locks to be 
held until after the Transaction commits and this is what makes the 
implementation of Rigorous 2-PL easier.
> 

### Conservative 2-PL –

A.K.A **Static 2-PL**,
 this protocol requires the transaction to lock all the items it access 
before the Transaction begins execution by predeclaring its read-set and
 write-set. If any of the predeclared items needed cannot be locked, the
 transaction does not lock any of the items, instead, it waits until all
 the items are available for locking.

Conservative 2-PL is *Deadlock free* and but it does not ensure a Strict schedule(More about this [here](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-two-phase-locking-2-pl-iii/)!).
 However, it is difficult to use in practice because of the need to 
predeclare the read-set and the write-set which is not possible in many 
situations. In practice, the most popular variation of 2-PL is Strict 
2-PL.

Venn Diagram below shows the classification of schedules 
that are rigorous and strict. The universe represents the schedules that
 can be serialized as 2-PL. Now as the diagram suggests, and it can also
 be logically concluded, if a schedule is Rigorous then it is Strict. We
 can also think in another way, say we put a restriction on a schedule 
which makes it strict, adding another to the list of restrictions make 
it Rigorous. Take a moment to again analyze the diagram and you’ll 
definitely get it.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/33-6.png

**Image –** Venn Diagram showing categories of languages under 2-PL

Now,
 let’s see the schedule below, tell me if this schedule can be locked 
using 2-PL, and if yes, show how and what class of 2-PL does your answer
 belongs to.

[Untitled Database](https://www.notion.so/e39b46d740ab4b6097ae96f0b66f5219?pvs=21)

Yes, the schedule is conflict serializable, so we can try implementing 2-PL. So, let’s try…

**Solution:**

[Untitled Database](https://www.notion.so/84536f204b6143a7af4e7303563cfd0b?pvs=21)

Now,
 this is one way I choose to implement the locks on A and B. You may try
 a different sequence but remember to follow the 2-PL protocol. With 
that said, observe that our locks are released after Commit operation so
 this satisfies Strict 2-PL protocol.

By now, I guess you must’ve
 got the idea of how to differentiate between types of 2-PL. Remember 
the theory as problems come in the examination sometimes just based on 
theoretical knowledge. Next, we’ll look at some examples of Conservative
 2-PL and how does it differ from the above two types of 2-PL. What 
makes it Deadlock free and also so difficult to implement. Then we’ll 
conclude the topic of 2-PL. Shortly we’ll move on to another type of 
Lock-based Protocol- Graph-Based Protocols. They are also very 
interesting and provides a unique method to deal with the problem of 
Deadlocks! So we’ll learn a new type of locking protocol, that will 
conclude the topic of Lock-based Protocol for GATE, till then Happy 
Learning.

> GATE-related question: GATE CS | IT 2004 | Question 77
*** Two Phase Locking (2-PL) Concurrency Control Protocol
Prerequisite – [Basics of](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-two-phase-locking-2-pl/) Two-Phase [Locking protocol(2-PL)](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-two-phase-locking-2-pl/), [Types of 2-PL](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-two-phase-locking-2-pl-ii/).

Now, we know both Strict 2-PL and Rigorous 2-PL *avoids* Cascading Rollbacks and *ensures*
 a Strict schedule but still cannot guarantee that our schedule is 
Deadlock free. We have seen both Strict and Rigorous 2-PL are similar in
 application and a general misconception is common that Conservative 
2-PL also follows the same sets of protocols as the above two. For 
clarity let’s go through Conservative 2-PL in detail.

### Conservative 2-PL –

A.K.A **Static 2-PL**,
 this protocol requires the transaction to lock all the items it 
accesses before the Transaction begins execution by predeclaring its 
read-set and write-set. If any of the predeclared items needed cannot be
 locked, the transaction does not lock any of the items, instead, it 
waits until all the items are available for locking. So the operation on
 data cannot start until we lock all the items required.

Now let’s see an interesting example on Conservative 2-PL. Tell me if the following schedule follows Conservative 2-PL? 

```
Schedule:  Lock-X(A)  Lock-X(B)  Read(A)  Read(B)  Write(A)  Unlock(A)Commit  Unlock(B)
```

Do you think the above Schedule *does not follow*
 Conservative 2-PL? Don’t confuse the protocol as just a modified 
version of Rigorous 2-PL, We can release the locks whenever we want, but
 we need to lock all the data items before carrying out any operation. 
This is what makes it Deadlock-free. The above schedule follows 
Conservative 2-PL.

Some interesting traits about Conservative 2-PL: 

- Schedule following this will not have a Growing Phase as we’ve seen in Basic,
Strict, and Rigorous 2-PL. As locking the data before using it is
mandatory so this protocol has *no Growing phase*. Moreover, this
rule makes it Deadlock free as if an item is not available for locking
the transaction releases all the locks and tries again later, i.e, *no Hold and Wait*. This makes one of the [four necessary conditions for deadlock](https://www.geeksforgeeks.org/operating-system-process-management-deadlock-introduction/) void.
- We only have to lock all the items beforehand, so releasing or unlocking
them has no restrictions as we had in Strict or Rigorous 2-PL.
- As no operations are done before acquiring all the locks, we have no
Growing phase in this protocol, unlike Basic, Strict, Rigorous 2-PL.
- Although we get a Deadlock free schedule in this protocol, we may still face drawbacks like *Cascading Rollbacks*. So this protocol does *not ensure Strict Schedules*. This is a disadvantage in comparison to Strict and Rigorous 2-PL.

Let’s
 discuss an example now. See how the schedule below follows Conservative
 2-PL but does not follow Strict and Rigorous 2-PL. 

[Untitled Database](https://www.notion.so/cb5dc12d59ea400880ccde2bfbff14dd?pvs=21)

Look
 at the schedule, it completely follows Conservative 2-PL, but fails to 
meet the requirements of Strict and Rigorous 2-PL, that is because we 
unlock A and B before the transaction commits.

**How can cascading abort happen in Conservative 2-PL?** This can happen because a Transaction may carry out a *Dirty Read* from another Transaction. We don’t have such restrictions in our protocol so this situation is possible.

Look at the Example given above, we have a Dirty Read operation from **T1** to **T2** at Step 8. If **T1** aborts, then **T2** would be rolled back.

GATE related question: [GATE-CS-2016 (Set 1) | Question 61](https://www.geeksforgeeks.org/gate-gate-cs-2016-set-1-question-61/)
*** Thomas Write Rule in DBMS
[Timestamp Ordering Protocol](https://www.geeksforgeeks.org/dbms-introduction-timestamp-deadlock-prevention-schemes/) states that if Ri(X) and Wj(X) are conflicting operations then Ri (X) is processed before Wj(X) if and only if TS(Ti) < TS(Tj).
 Whenever a schedule does not follow a serializability order according 
to the Timestamp, a user generally rejects it and rollback the 
Transaction. Some operations on the other hand are harmless and can be 
allowed.

Thomas Write 
Rule allows such operations and is a modification on the Basic Timestamp
 Ordering protocol. In Thomas Write Rule users *ignore outdated writes*. Moreover, of all the Concurrency Protocols that have been discussed, *Concurrency is imposed on Schedules* that *are Conflict Serializable*, in Thomas Write Rule, the most important improvement is a user can *achieve Concurrency with View Serializable schedules*.

First, let’s state what is Thomas Write Rule and then what are the modifications and improvements it succeeds over the [Basic TO protocol](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-timestamp-ordering-protocols/).

**Thomas Write Rule –** Thomas Write Rule does not enforce *Conflict* Serializability but rejects fewer Write Operations by modifying the check Operations for W_item(X) 

1. If **R_TS(X) > TS(T)**, then abort and roll back T and reject the operation.
2. If **W_TS(X) > TS(T)**, then don’t execute the Write Operation and continue processing. This is a case of *Outdated or Obsolete Writes*. Remember, outdated writes are ignored in Thomas Write Rule but a
Transaction following Basic TO protocol will abort such a Transaction.
3. If neither the condition in 1 or 2 occurs, then and only then execute the W_item(X) operation of T and set W_TS(X) to TS(T)

**Outdated Write Example –** The
 main update in Thomas Write Rule is ignoring the Obsolete Write 
Operations. This is done because some transaction with a timestamp 
greater than TS(T) (i.e., a transaction after T in TS ordering) has 
already written the value of X. Hence, logically user can ignore the 
Write(X) operation of T which becomes obsolete. Let us see this through 
an example:

Suppose a user has a schedule in which two transactions T1 and T2. Now, **TS(T2) < TS(T1)**. This means T1 arrived after T2 and hence has a larger TS value than T2. This implies that serializability of schedule allowed is **T2 –> T1** . Consider the partial schedule given below:

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/3333-1.png

**Image –** Example of Outdated Write

Obsolete Writes are hence ignored in this rule which is in accordance with the 2nd
 protocol. It seems to be more logical as users skip an unnecessary 
procedure of restarting the entire transaction. This protocol is just a 
modification to the Basic TO protocol.

**Basic TO Protocol v/s Thomas Write Rule –** Suppose a user has a schedule in which two transactions T1 and T2. Now, **TS(T2) < TS(T1)**. This implies that serializability of schedule allowed is **T2 –> T1** . Consider the two protocols, let us see what types of Operations will be allowed and not allowed under them. **Ri(A)** implies Read and **Wi(A)**
 implies Write operation. Now, let us look at the types of partial 
schedules allowed in both Basic TO and Thomas Write Rule, you’ll 
understand the difference in operations of both the protocol. User 
distinguish in operations Allowed and Not Allowed in both of the 
Protocols.

[Untitled Database](https://www.notion.so/6cbf4bcb356340488331149856451b3b?pvs=21)

Thus, from the above list, this modification used in Thomas Write Rule in comparison to the Basic TO protocol.

Reference: Database System Concepts, Fifth Edition [Silberschatz, Korth, Sudarshan], Chapter-16
*** Timestamp based Concurrency Control
Concurrency Control can be implemented in [different ways](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-lock-based-protocol/). One way to implement it is by using [Locks](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-lock-based-protocol/). Now, let us discuss Time Stamp Ordering Protocol.

As earlier introduced, **Timestamp**
 is a unique identifier created by the DBMS to identify a transaction. 
They are usually assigned in the order in which they are submitted to 
the system. Refer to the timestamp of a transaction *T* as ***TS(T)***. For the basics of Timestamp, you may refer [here](https://www.geeksforgeeks.org/dbms-introduction-timestamp-deadlock-prevention-schemes/).

**Timestamp Ordering Protocol –** The
 main idea for this protocol is to order the transactions based on their
 Timestamps. A schedule in which the transactions participate is then 
serializable and the only *equivalent serial schedule permitted* has the transactions in the order of their Timestamp Values. Stating simply, the schedule is equivalent to the particular *Serial Order* corresponding to the *order of the Transaction timestamps*. An algorithm must ensure that, for each item accessed by *Conflicting Operations*
 in the schedule, the order in which the item is accessed does not 
violate the ordering. To ensure this, use two Timestamp Values relating 
to each database item **X**.

- **W­_TS(X)** is the largest timestamp of any transaction that executed **write(X)** successfully.
- **R_TS(X)** is the largest timestamp of any transaction that executed **read(X)** successfully.

**Basic Timestamp Ordering –** Every transaction is issued a timestamp based on when it enters the system. Suppose, if an old transaction Ti has timestamp TS(Ti), a new transaction Tj is assigned timestamp TS(Tj) such that **TS(Ti) < TS(Tj)**.
 The protocol manages concurrent execution such that the timestamps 
determine the serializability order. The timestamp ordering protocol 
ensures that any conflicting read and write operations are executed in 
timestamp order. Whenever some Transaction *T* tries to issue a R_item(X) or a W_item(X), the Basic TO algorithm compares the timestamp of *T* with **R_TS(X) & W_TS(X)** to ensure that the Timestamp order is not violated. This describes the Basic TO protocol in the following two cases.

1. Whenever a Transaction *T* issues a **W_item(X)** operation, check the following conditions:
    - If ***R_TS(X) > TS(T)*** or if ***W_TS(X) > TS(T)***, then abort and rollback T and reject the operation. else,
    - Execute W_item(X) operation of T and set W_TS(X) to TS(T).
2. Whenever a Transaction *T* issues a **R_item(X)** operation, check the following conditions:
    - If ***W_TS(X) > TS(T)***, then abort and reject T and reject the operation, else
    - If W_TS(X) <= TS(T), then execute the R_item(X) operation of T and set R_TS(X) to the larger of TS(T) and current R_TS(X).

Whenever
 the Basic TO algorithm detects two conflicting operations that occur in
 an incorrect order, it rejects the latter of the two operations by 
aborting the Transaction that issued it. Schedules produced by Basic TO 
are guaranteed to be *conflict serializable*. Already discussed that using Timestamp can ensure that our schedule will be *[deadlock free](https://www.geeksforgeeks.org/dbms-introduction-timestamp-deadlock-prevention-schemes/)*.

One drawback of the Basic TO protocol is that **Cascading Rollback** is still possible. Suppose we have a Transaction T1 and T2 has used a value written by T1. If T1
 is aborted and resubmitted to the system then, T must also be aborted 
and rolled back. So the problem of Cascading aborts still prevails.

Let’s gist the Advantages and Disadvantages of Basic TO protocol: 

- Timestamp Ordering protocol ensures serializability since the precedence graph will be of the form:

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/22-6.png

**Image –** Precedence Graph for TS ordering

- Timestamp protocol ensures freedom from deadlock as no transaction ever waits.
- But the schedule may *not be cascade free*, and may not even be recoverable.

**Strict Timestamp Ordering –** A variation of Basic TO is called **Strict TO**
 ensures that the schedules are both Strict and Conflict Serializable. 
In this variation, a Transaction T that issues a R_item(X) or W_item(X) 
such that TS(T) > W_TS(X) has its read or write operation delayed 
until the Transaction **T‘** that wrote the values of X has committed or aborted.

**Related GATE Questions –**

1. [GATE | GATE CS 2010 | Question 20](https://www.geeksforgeeks.org/gate-gate-cs-2010-question-20/)
2. [GATE | GATE-CS-2017 (Set 1) | Question 46](https://www.geeksforgeeks.org/gate-gate-cs-2017-set-1-question-46/)
3. [GATE | GATE-IT-2004 | Question 21](https://www.geeksforgeeks.org/gate-gate-it-2004-question-21/)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Multiple Granularity Locking in DBMS
Prerequisite – [Timestamp Ordering Protocols](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-timestamp-ordering-protocols/) The
 various Concurrency Control schemes have used different methods and 
every individual Data item as the unit on which synchronization is 
performed. A certain drawback of this technique is if a transaction Ti needs to access the entire database, and a locking protocol is used, then Ti must lock each item in the database. It is less efficient, it would be simpler if Ti
 could use a single lock to lock the entire database. But, if it 
considers the second proposal, this should not in fact overlook the 
certain flaw in the proposed method. Suppose another transaction just 
needs to access a few data items from a database, so locking the entire 
database seems to be unnecessary moreover it may cost us a loss of 
Concurrency, which was our primary goal in the first place. To bargain 
between Efficiency and Concurrency. Use Granularity.

Let’s start by understanding what is meant by Granularity.

**Granularity –** It is the size of the data item allowed to lock. Now *Multiple Granularity*
 means hierarchically breaking up the database into blocks that can be 
locked and can be tracked needs what needs to lock and in what fashion. 
Such a hierarchy can be represented graphically as a tree.

For 
example, consider the tree, which consists of four levels of nodes. The 
highest level represents the entire database. Below it is nodes of type **area**;
 the database consists of exactly these areas. The area has children 
nodes which are called files. Every area has those files that are its 
child nodes. No file can span more than one area.

Finally, each 
file has child nodes called records. As before, the file consists of 
exactly those records that are its child nodes, and no record can be 
present in more than one file. Hence, the levels starting from the top 
level are: 

- database
- area
- file
- record

!https://media.geeksforgeeks.org/wp-content/uploads/22-3.png

**Figure –** Multi Granularity tree Hierarchy

Consider the above diagram for the example given, each node in the tree can be locked individually. As in the [2-phase locking protocol](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-two-phase-locking-2-pl/),
 it shall use shared and exclusive lock modes. When a transaction locks a
 node, in either shared or exclusive mode, the transaction also 
implicitly locks all the descendants of that node in the same lock mode.
 For example, if transaction Ti gets an explicit lock on file Fc
 in exclusive mode, then it has an implicit lock in exclusive mode on 
all the records belonging to that file. It does not need to lock the 
individual records of Fc explicitly. this is the main difference between Tree-Based [Locking](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-graph-based-protocol/) and Hierarchical locking for multiple granularities.

Now,
 with locks on files and records made simple, how does the system 
determine if the root node can be locked? One possibility is for it to 
search the entire tree but the solution nullifies the whole purpose of 
the multiple-granularity locking scheme. A more efficient way to gain 
this knowledge is to introduce a new lock mode, called *Intention lock mode*.

**Intention Mode Lock –** In addition to **S** and **X** lock modes, there are three additional lock modes with multiple granularities: 

- **Intention-Shared (IS):** explicit locking at a lower level of the tree but only with shared locks.
- **Intention-Exclusive (IX):** explicit locking at a lower level with exclusive or shared locks.
- **Shared & Intention-Exclusive (SIX):** the subtree rooted by that node is locked explicitly in shared mode and explicit locking is being done at a lower level with exclusive mode
locks.

The compatibility matrix for these lock modes are described below:

!https://media.geeksforgeeks.org/wp-content/uploads/33-4.png

**Figure –** Multi Granularity tree Hierarchy

The
 multiple-granularity locking protocol uses the intention lock modes to 
ensure serializability. It requires that a transaction Ti that attempts to lock a node must follow these protocols: 

1. Transaction T must follow the lock-compatibility matrix.
    
    i
    
2. Transaction T must lock the root of the tree first, and it can lock it in any mode.
    
    i
    
3. Transaction T can lock a node in S or IS mode only if T currently has the parent of the node-locked in either IX or IS mode.
    
    i
    
    i
    
4. Transaction T can lock a node in X, SIX, or IX mode only if T currently has the parent of the node-locked in either IX or SIX modes.
    
    i
    
    i
    
5. Transaction T can lock a node only if T has not previously unlocked any node (i.e., T is two-phase).
    
    i
    
    i
    
    i
    
6. Transaction T can unlock a node only if T currently has none of the children of the node-locked.
    
    i
    
    i
    

Observe
 that the multiple-granularity protocol requires that locks be acquired 
in top-down (root-to-leaf) order, whereas locks must be released in 
bottom-up (leaf to-root) order. As an illustration of the protocol, consider the tree given above and the transactions: 

- Say transaction T reads record R in file F. Then, T needs to lock the database, area A, and F in IS mode (and in that order), and finally to lock R in S mode.
    
    1
    
    a2
    
    a
    
    2
    
    1
    
    a
    
    a2
    
- Say transaction T modifies record R in file F . Then, T needs to lock the database, area A, and file F (and in that order) in IX mode, and at last to lock R in X mode.
    
    2
    
    a9
    
    a
    
    2
    
    1
    
    a
    
    a9
    
- Say transaction T reads all the records in file F. Then, T needs to lock the database and area A (and in that order) in IS mode, and at last to lock F in S mode.
    
    3
    
    a
    
    3
    
    1
    
    a
    
- Say transaction T reads the entire database. It can do so after locking the database in S mode.
    
    4
    

Note that transactions T1, T3, and T4 *can access the database concurrently*. Transaction T2 can execute concurrently with T1, but not with either T3 or T4. This protocol enhances *concurrency and reduces lock overhead*.
 Deadlock is still possible in the multiple-granularity protocol, as it 
is in the two-phase locking protocol. These can be eliminated by using 
certain [deadlock elimination techniques](https://www.geeksforgeeks.org/deadlock-in-dbms/).
*** Graph Based Concurrency Control Protocol in DBMS
Graph Based Protocols are yet another way of implementing [Lock Based Protocols](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-lock-based-protocol/).

As
 we know the prime problems with Lock Based Protocol has been avoiding 
Deadlocks and ensuring a Strict Schedule. We’ve seen that Strict 
Schedules are possible with following [Strict or Rigorous 2-PL](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-two-phase-locking-2-pl-ii/). We’ve even seen that Deadlocks can be avoided if we follow [Conservative 2-PL](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-two-phase-locking-2-pl-iii/) but the problem with this protocol is it cannot be used practically. Graph Based Protocols are used as an alternative to 2-PL. *Tree Based Protocols is a simple implementation of Graph Based Protocol*.

A prerequisite of this protocol is that we know the order to access a Database Item. For this we implement a **Partial Ordering** on a set of the **Database Items (D) *{d1, d2, d3, ….., dn}*** . The protocol following the implementation of Partial Ordering is stated as-

- If ***di –> dj*** then any transaction accessing both d and d must access d before accessing d.
    
    i
    
    j
    
    i
    
    j
    
- Implies that the set **D** may now be viewed as a directed acyclic graph (DAG), called a *database graph*.

**Tree Based Protocol –**

- Partial Order on Database items determines a tree like structure.
- Only Exclusive Locks are allowed.
- The first lock by T may be on any data item. Subsequently, a data Q can be locked by T only if the parent of Q is currently locked by T.
    
    i
    
    i
    
    i
    
- Data items can be unlocked at any time.

Following the Tree based Protocol ensures **Conflict Serializability and Deadlock Free** schedule. We need not wait for unlocking a Data item as we did in 2-PL protocol, thus increasing the concurrency.

Now, let us see an Example, following is a *Database Graph* which will be used as a reference for locking the items subsequently.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/222-1.png

**Image –** Database Graph Let’s
 look at an example based on the above Database Graph. We have three 
Transactions in this schedule and this is a skeleton example, i.e, we 
will only see how Locking and Unlocking works, let’s keep this simple 
and not make this complex by adding operations on data. 

[Untitled Database](https://www.notion.so/af6e448043f94d2db4c186e5eafeda94?pvs=21)

From the above example, first see that the schedule is Conflict Serializable. Serializability for Locks can be written as **T2 –> T1 –> T3**. Data items Locked and Unlocked are following the same rule as given above and follows the Database Graph.

Thus, let’s revise once more what are the key points of Graph Based Protocols. **Advantage –**

- Ensures Conflict Serializable Schedule.
- Ensures Deadlock Free Schedule
- Unlocking can be done anytime

With some advantages comes some Disadvantages also. **Disadvantage –**

- Unnecessary locking overheads may happen sometimes, like if we want both D and E,
then at least we have to lock B to follow the protocol.
- ***Cascading Rollbacks*** is still a problem. We don’t follow a rule of when Unlock operation may occur so this problem persists for this protocol. Overall this protocol is mostly known
and used for its unique way of implementing Deadlock Freedom. References: Database System Concepts,
Fifth Edition [Silberschatz, Korth, Sudarshan], Chapter-16.
*** Introduction to TimeStamp and Deadlock Prevention Schemes in DBMS
**Deadlock** occurs when each transaction **T** in a schedule of *two or more* transaction waiting for some item locked by some other transaction **T‘**
 in the set. Thus, both end up in a deadlock situation, waiting for the 
other to release the lock on the item. Deadlocks are a common problem 
and we have introduced the problem while solving the Concurrency Control
 by the introduction of [Locks](https://www.geeksforgeeks.org/dbms-concurrency-control-protocols-lock-based-protocol/). Deadlock avoidance is a major issue and some protocols were suggested to avoid them, like [Conservative 2-PL](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-two-phase-locking-2-pl-iii/) and [Graph Based protocols](https://www.geeksforgeeks.org/dbms-concurrency-control-protocol-graph-based-protocol/) but some drawbacks are still there.

Here, we will discuss a new concept of **Transaction Timestamp TS(Ti)**.
 A timestamp is a unique identifier created by the DBMS to identify a 
transaction. They are usually assigned in the order in which they are 
submitted to the system, so a timestamp may be thought of as the 
transaction start time.

There may be different ways of generating timestamps such as

- A simple counter that increments each time its value is assigned to a transaction. They may be numbered ***1, 2, 3…***. Though we’ll have to reset the counter from time to time to avoid overflow.
- Using the current date/time from the system clock. Just ensuring that no two
transactions are given the same value in the same clock tick, we will
always get a unique timestamp. This method is widely used.

### Deadlock Prevention Schemes based on Timestamp…

As
 discussed, Timestamps are unique identifiers assigned to each 
transaction. They are based on the order in which Transactions are 
started. Say if **T1** starts before **T2** then **TS(T1)** will be less than (<) **TS(T2)**.

There are two schemes to prevent deadlock called *wound-wait* and *wait-die*. Say there are two transactions **Ti** and **Tj**, now say **Ti** tries to lock an item *X* but item *X* is already locked by some **Tj**, now in such a conflicting situation the two schemes which prevent deadlock. We’ll use this context shortly.

- **Wait_Die :** An older transaction is allowed to wait for a younger transaction,
whereas a younger transaction requesting an item held by an older
transaction is aborted and restarted. From the context above, if **TS(Ti) < TS(Tj)**, then (T older than T) T is allowed to wait; otherwise *abort Ti* (T younger than T) *and restart it later with the same timestamp.*
    
    i
    
    j
    
    i
    
    i
    
    j
    
- **Wound_Wait :** It is just the opposite of the Wait_Die technique. Here, a younger
transaction is allowed to wait for an older one, whereas if an older
transaction requests an item held by the younger transaction, we preempt the younger transaction by aborting it. From the context above, if **TS(Ti) < TS(Tj)**, then (T older than T) T is aborted (i.e., T wounds T) and restarts it later with the same Timestamp; *otherwise (Ti younger than Tj) Ti is allowed to wait.*
    
    i
    
    j
    
    j
    
    i
    
    j
    

Thus,
 both the schemes end up aborting the younger of the two transactions 
that may be involved in a deadlock. It is done on the basis of the 
assumption that aborting the younger transaction will waste less 
processing which is logical. In such a case there cannot be a cycle 
since we are waiting linearly in both cases. For GATE the theory for these two methods is enough, for more on this you may refer [here](https://stackoverflow.com/questions/32794142/what-is-the-difference-between-wait-die-and-wound-wait).

Another group of protocols which prevents deadlock but *does not require Timestamps*. They are discussed below:

- **No-waiting Algorithm :** This follows a simple approach, if a Transaction is unable to obtain a
lock, it is immediately aborted and then restarted after a certain time
delay without checking if a deadlock will occur or not. Here, no
Transaction ever waits so there is no possibility for deadlock. This method is somewhat not practical. It may cause transaction to abort and restart unnecessarily.
- **Cautious Waiting :** If **Ti** tries to lock an item *X* but is not able to do because *X* is locked by some **Tj**. In such a conflict, if **Tj** is not waiting for some other locked item, then **Ti** is allowed to wait, otherwise *abort **Ti**.*

Another approach, to deal with deadlock is deadlock detection, we can use [Wait-for-Graph](https://www.geeksforgeeks.org/deadlock-in-dbms/). This uses a similar approach when we used to check for cycles while checking for serializability.

**Starvation:** One
 problem that may occur when we use locking is starvation which occurs 
when a transaction cannot proceed for an indefinite period of time while
 other transactions in the system continue normally. This may occur if 
the waiting scheme for locked items is unfair, giving priority to some 
transactions over others. We may have some solutions for Starvation. One
 is using a **first come first serve** queue; transactions 
are enabled to lock an item in the order in which they originally 
requested the lock. This is a widely used mechanism to reduce 
starvation. Our Concurrency Control Manager is responsible to schedule 
the transactions, so it employs different methods to overcome them. You 
may refer [this](https://www.geeksforgeeks.org/starvation-in-dbms/) for detailed explanation.

Try this question : [GATE | GATE-CS-2017 (Set 1) | Question 46](https://www.geeksforgeeks.org/gate-gate-cs-2017-set-1-question-46/)

Next, we’ll discuss the famous Timestamp Ordering Protocol and Thomas Write rule. Till then Happy Learning!

Reference: Database System Concepts, Fifth Edition [Silberschatz, Korth, Sudarshan], Chapter-16.
*** Implementation of Locking in DBMS
Locking
 protocols are used in database management systems as a means of 
concurrency control. Multiple transactions may request a lock on a data 
item simultaneously. Hence, we require a mechanism to manage the locking
 requests made by transactions. Such a mechanism is called as **Lock Manager**.
 It relies on the process of message passing where transactions and lock
 manager exchange messages to handle the locking and unlocking of data 
items.

**Data structure used in Lock Manager –**The data structure required for implementation of locking is called as **Lock table**.

1. It is a hash table where name of data items are used as hashing index.
2. Each locked data item has a linked list associated with it.
3. Every node in the linked list represents the transaction which requested for
lock, mode of lock requested (mutual/exclusive) and current status of
the request (granted/waiting).
4. Every new lock request for the data item will be added in the end of linked list as a new node.
5. Collisions in hash table are handled by technique of separate chaining.

Consider the following example of lock table:

!https://media.geeksforgeeks.org/wp-content/uploads/Slide1-4.jpg

**Explanation:** In the above figure, the locked data items present in lock table are 5, 47, 167 and 15.

The transactions which have requested for lock have been represented by a linked list shown below them using a downward arrow.

Each node in linked list has the name of transaction which has requested the data item like T33, T1, T27 etc.

The colour of node represents the status i.e. whether lock has been granted or waiting.

Note
 that a collision has occurred for data item 5 and 47. It has been 
resolved by separate chaining where each data item belongs to a linked 
list. The data item is acting as header for linked list containing the 
locking request.

**Working of Lock Manager –**

1. Initially the lock table is empty as no data item is locked.
2. Whenever lock manager receives a lock request from a transaction T on a particular data item Q following cases may arise:
    
    i
    
    i
    
    - If Q is not already locked, a linked list will be created and lock will be granted to the requesting transaction T.
        
        i
        
        i
        
    - If the data item is already locked, a new node will be added at the end of its linked list containing the information about request made by T.
        
        i
        
3. If the lock mode requested by T is compatible with lock mode of transaction currently having the lock, T will acquire the lock too and status will be changed to ‘granted’. Else, status of T’s lock will be ‘waiting’.
    
    i
    
    i
    
    i
    
4. If a transaction T wants to unlock the data item it is currently holding, it will send an
unlock request to the lock manager. The lock manager will delete T’s node from this linked list. Lock will be granted to the next transaction in the list.
    
    i
    
    i
    
5. Sometimes transaction T may have to be aborted. In such a case all the waiting request made by T will be deleted from the linked lists present in lock table. Once abortion is complete, locks held by T will also be released.
    
    i
    
    i
    
    i
    

**Reference –**Database system Concepts, 6th edition

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

# Implementation of Locking in DBMS

- Difficulty Level :
[Medium](https://www.geeksforgeeks.org/medium/)
- Last Updated :
04 Feb, 2020

Locking
 protocols are used in database management systems as a means of 
concurrency control. Multiple transactions may request a lock on a data 
item simultaneously. Hence, we require a mechanism to manage the locking
 requests made by transactions. Such a mechanism is called as **Lock Manager**.
 It relies on the process of message passing where transactions and lock
 manager exchange messages to handle the locking and unlocking of data 
items.

**Data structure used in Lock Manager –**The data structure required for implementation of locking is called as **Lock table**.

1. It is a hash table where name of data items are used as hashing index.
2. Each locked data item has a linked list associated with it.
3. Every node in the linked list represents the transaction which requested for
lock, mode of lock requested (mutual/exclusive) and current status of
the request (granted/waiting).
4. Every new lock request for the data item will be added in the end of linked list as a new node.
5. Collisions in hash table are handled by technique of separate chaining.

Consider the following example of lock table:

!https://media.geeksforgeeks.org/wp-content/uploads/Slide1-4.jpg

**Explanation:** In the above figure, the locked data items present in lock table are 5, 47, 167 and 15.

The transactions which have requested for lock have been represented by a linked list shown below them using a downward arrow.

Each node in linked list has the name of transaction which has requested the data item like T33, T1, T27 etc.

The colour of node represents the status i.e. whether lock has been granted or waiting.

Note
 that a collision has occurred for data item 5 and 47. It has been 
resolved by separate chaining where each data item belongs to a linked 
list. The data item is acting as header for linked list containing the 
locking request.

**Working of Lock Manager –**

1. Initially the lock table is empty as no data item is locked.
2. Whenever lock manager receives a lock request from a transaction T on a particular data item Q following cases may arise:
    
    i
    
    i
    
    - If Q is not already locked, a linked list will be created and lock will be granted to the requesting transaction T.
        
        i
        
        i
        
    - If the data item is already locked, a new node will be added at the end of its linked list containing the information about request made by T.
        
        i
        
3. If the lock mode requested by T is compatible with lock mode of transaction currently having the lock, T will acquire the lock too and status will be changed to ‘granted’. Else, status of T’s lock will be ‘waiting’.
    
    i
    
    i
    
    i
    
4. If a transaction T wants to unlock the data item it is currently holding, it will send an
unlock request to the lock manager. The lock manager will delete T’s node from this linked list. Lock will be granted to the next transaction in the list.
    
    i
    
    i
    
5. Sometimes transaction T may have to be aborted. In such a case all the waiting request made by T will be deleted from the linked lists present in lock table. Once abortion is complete, locks held by T will also be released.
    
    i
    
    i
    
    i
    

**Reference –**Database system Concepts, 6th edition

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Challenges of database security in DBMS
Seeing
 the vast increase in volume and speed of threats to databases and many 
information assets, research efforts need to be consider to the 
following issuessuch as data quality, intellectual property rights, and database survivability.

Let’s discuss them one by one.

**1. Data quality –**

- The database community basically needs techniques and some organizational
solutions to assess and attest the quality of data. These techniques may include the simple mechanism such as quality stamps that are posted on
different websites. We also need techniques that will provide us more
effective integrity semantics verification tools for assessment of data
quality, based on many techniques such as record linkage.
- We also need application-level recovery techniques to automatically repair the incorrect data.
- The ETL that is extracted transform and load tools widely used for loading
the data in the data warehouse are presently grappling with these
issues.

**2. Intellectual property rights –**As the 
use of Internet and intranet is increasing day by day, legal and 
informational aspects of data are becoming major concerns for many 
organizations. To address this concerns watermark technique are used 
which will help to protect content from unauthorized duplication and 
distribution by giving the provable power to the ownership of the 
content.

Traditionally they are dependent upon the availability of
 a large domain within which the objects can be altered while retaining 
its essential or important properties.However, research is needed to
 access the robustness of many such techniques and the study and 
investigate many different approaches or methods that aimed to prevent 
intellectual property rights violation.

**3. Database survivability –**Database
 systems need to operate and continued their functions even with the 
reduced capabilities, despite disruptive events such as information 
warfare attacksA DBMS in addition to making every effort to prevent 
an attack and detecting one in the event of the occurrence should be 
able to do the following:

- **Confident:**We should take immediate action to eliminate the attacker’s access to the system and
to isolate or contain the problem to prevent further spread.
- **Damage assessment:**Determine the extent of the problem, including failed function and corrupted data.
- **Recover:**Recover corrupted or lost data and repair or reinstall failed function to reestablish a normal level of operation.
- **Reconfiguration:**Reconfigure to allow the operation to continue in a degraded mode while recovery proceeds.
- **Fault treatment:**To the extent possible, identify the weakness exploited in the attack and takes steps to prevent a recurrence.
** File structures (sequential files, indexing, B and B+ trees)
*** File Organization in DBMS | Set 1
A 
database consist of a huge amount of data. The data is grouped within a 
table in RDBMS, and each table have related records. A user can see that
 the data is stored in form of tables, but in actual this huge amount of
 data is stored in physical memory in form of files.

**File –**
 A file is named collection of related information that is recorded on 
secondary storage such as magnetic disks, magnetic tapes and optical 
disks.

**What is File Organization?** File 
Organization refers to the logical relationships among various records 
that constitute the file, particularly with respect to the means of 
identification and access to any specific record. In simple terms, 
Storing the files in certain order is called file Organization. **File Structure** refers to the format of the label and data blocks and of any logical control record.

### Types of File Organizations –

Various
 methods have been introduced to Organize files. These particular 
methods have advantages and disadvantages on the basis of access or 
selection . Thus it is all upon the programmer to decide the best suited
 file Organization method according to his requirements. Some types of File Organizations are : 

- Sequential File Organization
- Heap File Organization
- Hash File Organization
- B+ Tree File Organization
- Clustered File Organization

We
 will be discussing each of the file Organizations in further sets of 
this article along with differences and advantages/ disadvantages of 
each file Organization methods. 

### Sequential File Organization –

The
 easiest method for file Organization is Sequential method. In this 
method the file are stored one after another in a sequential manner. 
There are two ways to implement this method:

- **Pile File Method –** This method is quite simple, in which we store the records in a
sequence i.e one after other in the order in which they are inserted
into the tables.

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization01.png

1. **Insertion of new record –** Let the R1, R3 and so on upto R5 and R4 be four records in the sequence.
Here, records are nothing but a row in any table. Suppose a new record
R2 has to be inserted in the sequence, then it is simply placed at the
end of the file. 

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization00.png

- **Sorted File Method –**In this method, As the name itself suggest whenever a new record has to be inserted, it is always inserted in a sorted (ascending or descending)
manner. Sorting of records may be based on any primary key or any other
key.

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization66.png

1. **Insertion of new record –** Let us assume that there is a preexisting sorted sequence of four records
R1, R3, and so on upto R7 and R8. Suppose a new record R2 has to be
inserted in the sequence, then it will be inserted at the end of the
file and then it will sort the sequence . 

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization22.png

**Pros and Cons of Sequential File Organization –** **Pros –**

- Fast and efficient method for huge amount of data.
- Simple design.
- Files can be easily stored in magnetic tapes i.e cheaper storage mechanism.

**Cons –**

- Time wastage as we cannot jump on a particular record that is required, but
we have to move in a sequential manner which takes our time.
- Sorted file method is inefficient as it takes time and space for sorting records.

### Heap File Organization –

Heap
 File Organization works with data blocks. In this method records are 
inserted at the end of the file, into the data blocks. No Sorting or 
Ordering is required in this method. If a data block is full, the new 
record is stored in some other block, Here the other data block need not
 be the very next data block, but it can be any block in the memory. It 
is the responsibility of DBMS to store and manage the new records. 

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization33.png

**Insertion of new record –** Suppose
 we have four records in the heap R1, R5, R6, R4 and R3 and suppose a 
new record R2 has to be inserted in the heap then, since the last data 
block i.e data block 3 is full it will be inserted in any of the data 
blocks selected by the DBMS, lets say data block 1.

!https://media.geeksforgeeks.org/wp-content/uploads/FileOrganization44.png

If
 we want to search, delete or update data in heap file Organization the 
we will traverse the data from the beginning of the file till we get the
 requested record. Thus if the database is very huge, searching, 
deleting or updating the record will take a lot of time.

**Pros and Cons of Heap File Organization –** **Pros –**

- Fetching and retrieving records is faster than sequential record but only in case of small databases.
- When there is a huge number of data needs to be loaded into the database at a time, then this method of file Organization is best suited.

**Cons –**

- Problem of unused memory blocks.
- Inefficient for larger databases.

Read next set : [(DBMS File Organization-Set 2) | Hashing in DBMS](https://www.geeksforgeeks.org/hashing-in-dbms/)
*** File Organization in DBMS | Set 2
In
 a database management system, When we want to retrieve a particular 
data, It becomes very inefficient to search all the index values and 
reach the desired data. In this situation, Hashing technique comes into 
picture.

**Hashing**
 is an efficient technique to directly search the location of desired 
data on the disk without using index structure. Data is stored at the 
data blocks whose address is generated by using hash function. The 
memory location where these records are stored is called as data block 
or data bucket.

```
Prerequisite -Hashing Data Structure
```

### Hash File Organization:

- **Data bucket –** Data buckets are the memory locations where the records are stored. These buckets are also considered as *Unit Of Storage*.
- **Hash Function –** Hash function is a mapping function that maps all the set of search
keys to actual record address. Generally, hash function uses the primary key to generate the hash index – address of the data block. Hash
function can be simple mathematical function to any complex mathematical function.
- **Hash Index-**The prefix of an entire
hash value is taken as a hash index. Every hash index has a depth value
to signify how many bits are used for computing a hash function. These
bits can address 2n buckets. When all these bits are consumed ? then the depth value is increased linearly and twice the buckets are allocated.

### Static Hashing:

In
 static hashing, when a search-key value is provided, the hash function 
always computes the same address. For example, if we want to generate an
 address for STUDENT_ID = 104 using mod (5) hash function, it always 
results in the same bucket address 4.  There will not be any changes to 
the bucket address here. Hence a number of data buckets in the memory 
for this static hashing remain constant throughout.

**Operations:**

- **Insertion –** When a new record is inserted into the table, The hash function h generates a bucket address for the new record based on its hash key K. Bucket
address = h(K)
- **Searching –** When a record needs
to be searched, The same hash function is used to retrieve the bucket
address for the record. For Example, if we want to retrieve the whole
record for ID 104, and if the hash function is mod (5) on that ID, the
bucket address generated would be 4. Then we will directly got to
address 4 and retrieve the whole record for ID 104. Here ID acts as a
hash key.
- **Deletion –** If we want to delete a
record, Using the hash function we will first fetch the record which is
supposed to be deleted. Then we will remove the records for that
address in memory.
- **Updation –** The data record that needs to be updated is first searched using hash function, and then the data record is updated.

Now,
 If we want to insert some new records into the file But the data bucket
 address generated by the hash function is not empty or the data already
 exists in that address. This becomes a critical situation to handle. 
 This situation in the static hashing is called **bucket overflow**.
 How will we insert data in this case? There are several methods 
provided to overcome this situation. Some commonly used methods are 
discussed below:

1. **Open Hashing –** In Open
hashing method, next available data block is used to enter the new
record, instead of overwriting the older one. This method is also
called linear probing. For example, D3 is a new record that needs to be inserted, the hash function generates the address as 105. But it is
already full. So the system searches next available data bucket, 123 and assigns D3 to it.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot_2017-12-15-11-32-52-657_com.miui_.gallery.png
    
2. **Closed hashing –** In Closed hashing method, a new data bucket is allocated with same
address and is linked it after the full data bucket. This method is also known as overflow chaining. For example, we have to insert a new
record D3 into the tables. The static hash function generates the data
bucket address as 105. But this bucket is full to store the new data. In this case is a new data bucket is added at the end of 105 data bucket
and is linked to it. Then new record D3 is inserted into the new bucket.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot_2017-12-15-11-33-15-548_com.miui_.gallery.png
    
    - **Quadratic probing :** Quadratic probing is very much similar to open hashing or linear
    probing. Here, The only difference between old and new bucket is linear. Quadratic function is used to determine the new bucket address.
    - **Double Hashing :** Double Hashing is another method similar to linear probing. Here the
    difference is fixed as in linear probing, but this fixed difference is
    calculated by using another hash function. That’s why the name is double hashing.

### Dynamic Hashing –

The drawback 
of static hashing is that that it does not expand or shrink dynamically 
as the size of the database grows or shrinks.  In Dynamic hashing, data 
buckets grows or shrinks (added or removed dynamically) as the records 
increases or decreases. Dynamic hashing is also known as extended 
hashing. In dynamic hashing, the hash function is made to produce a 
large number of values. For Example, there are three data records D1, D2
 and D3 . The hash function generates three addresses 1001, 0101 and 
1010 respectively.  This method of storing considers only part of this 
address – especially only first one bit to store the data. So it tries 
to load three of them at address 0 and 1.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot_2017-12-15-11-33-48-964_com.miui_.gallery.png

But
 the problem is that No bucket address is remaining for D3. The bucket 
has to grow dynamically to accommodate D3. So it changes the address 
have 2 bits rather than 1 bit, and then it updates the existing data to 
have 2 bit address. Then it tries to accommodate D3.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot_2017-12-15-11-34-08-506_com.miui_.gallery.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** File Organization in DBMS | Set 3
Prerequisite – [DBMS | File Organization – Set 1](https://www.geeksforgeeks.org/dbms-file-organization-set-1/), [File Organization-Set 2](https://www.geeksforgeeks.org/hashing-in-dbms/)

### B+ Tree File Organization –

B+
 Tree, as the name suggests, It uses a tree like structure to store 
records in File. It uses the concept of Key indexing where the primary 
key is used to sort the records. For each primary key, an index value is
 generated and mapped with the record. An index of a record is the 
address of record in the file.

B+ Tree is very much similar to 
binary search tree, with the only difference that instead of just two 
children, it can have more than two. All the information is stored in 
leaf node and the intermediate nodes acts as pointer to the leaf nodes. 
The information in leaf nodes always remain a sorted sequential linked 
list.

!https://media.geeksforgeeks.org/wp-content/uploads/B-TREE.bmp

In the above diagram 56 is the root node which is also called the main node of the tree. The
 intermediate nodes here, just consist the address of leaf nodes. They 
do not contain any actual record. Leaf nodes consist of the actual 
record. All leaf nodes are balanced.

**Pros and Cons of B+ Tree File Organization –** 

**Pros –** 

- Tree traversal is easier and faster.
- Searching becomes easy as all records are stored only in leaf nodes and are sorted sequential linked list.
- There is no restriction on B+ tree size. It may grows/shrink as the size of data increases/decreases.

**Cons –**

- Inefficient for static tables.

### Cluster File Organization –

In
 cluster file organization, two or more related tables/records are 
stored within same file known as clusters. These files will have two or 
more tables in the same data block and the key attributes which are used
 to map these table together are stored only once.

Thus it lowers
 the cost of searching and retrieving various records in different files
 as they are now combined and kept in a single cluster. For example we have two tables or relation Employee and Department. These table are related to each other.

!https://media.geeksforgeeks.org/wp-content/uploads/555-1.png

Therefore these table are allowed to combine using a join operation and can be seen in a cluster file.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot_2017-12-24-22-31-39-248_com.miui_.gallery.png

If
 we have to insert, update or delete any record we can directly do so. 
Data is sorted based on the primary key or the key with which searching 
is done. **Cluster key** is the key with which joining of the table is performed.

**Types of Cluster File Organization –** There are two ways to implement this method:

1. **Indexed Clusters –** In Indexed clustering the records are group based on the cluster key and
stored together. The above mentioned example of the Employee and
Department relationship is an example of Indexed Cluster where the
records are based on the Department ID.
2. **Hash Clusters –** This is very much similar to indexed cluster with only difference that
instead of storing the records based on cluster key, we generate hash
key value and store the records with same hash key value.
*** Introduction of B-Tree
**Introduction:** B-Tree is a self-balancing search tree. In most of the other self-balancing search trees (like [AVL](https://www.geeksforgeeks.org/avl-tree-set-1-insertion/)
 and Red-Black Trees), it is assumed that everything is in main memory. 
To understand the use of B-Trees, we must think of the huge amount of 
data that cannot fit in main memory. When the number of keys is high, 
the data is read from disk in the form of blocks. Disk access time is 
very high compared to the main memory access time. The main idea of 
using B-Trees is to reduce the number of disk accesses. Most of the tree
 operations (search, insert, delete, max, min, ..etc ) require O(h) disk
 accesses where h is the height of the tree. B-tree is a fat tree. The 
height of B-Trees is kept low by putting maximum possible keys in a 
B-Tree node. Generally, the B-Tree node size is kept equal to the disk 
block size. Since the height of the B-tree is low so total disk accesses
 for most of the operations are reduced significantly compared to 
balanced Binary Search Trees like AVL Tree, Red-Black Tree, ..etc.**Time Complexity of B-Tree:** 

[Untitled Database](https://www.notion.so/79bf6c0f775e4648838f6652c537d824?pvs=21)

**“n” is the total number of elements in the B-tree.Properties of B-Tree:** 

1. All leaves are at the same level.
2. A B-Tree is defined by the term *minimum degree* ‘t’. The value of t depends upon disk block size.
3. Every node except root must contain at least t-1 keys. The root may contain minimum 1 key.
4. All nodes (including root) may contain at most 2*t – 1 keys.
5. Number of children of a node is equal to the number of keys in it plus 1.
6. All keys of a node are sorted in increasing order. The child between two
keys k1 and k2 contains all keys in the range from k1 and k2.
7. B-Tree grows and shrinks from the root which is unlike Binary Search Tree.
Binary Search Trees grow downward and also shrink from downward.
8. Like other balanced Binary Search Trees, time complexity to search, insert and delete is O(log n).
9. Insertion of a Node in B-Tree happens only at Leaf Node.

Following
 is an example of B-Tree of minimum order 5. Note that in practical 
B-Trees, the value of the minimum order is much more than 5. 

!https://media.geeksforgeeks.org/wp-content/uploads/20200506235136/output253.png

We
 can see in the above diagram that all the leaf nodes are at the same 
level and all non-leaf have no empty sub-tree and have keys one less 
than the number of their children.**Interesting Facts:**

1. The minimum height of the B-Tree that can exist with n number of nodes and m is the maximum number of children of a node can have is:
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-4974d14eb09e2aa02812c10bfae1d384_l3.svg
    
2. The maximum height of the B-Tree that can exist with n number of nodes and t is the minimum number of children that a non-root node can have is: and
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-2e1c8dff8846e85741fc93ac16504d45_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-359bb7100d35c9ad3e811f723d7081ff_l3.svg
    

**Traversal in B-Tree:** Traversal
 is also similar to Inorder traversal of Binary Tree. We start from the 
leftmost child, recursively print the leftmost child, then repeat the 
same process for remaining children and keys. In the end, recursively 
print the rightmost child.

**Search Operation in B-Tree:** Search
 is similar to the search in Binary Search Tree. Let the key to be 
searched be k. We start from the root and recursively traverse down. For
 every visited non-leaf node, if the node has the key, we simply return 
the node. Otherwise, we recur down to the appropriate child (The child 
which is just before the first greater key) of the node. If we reach a 
leaf node and don’t find k in the leaf node, we return NULL.

**Logic:** Searching
 a B-Tree is similar to searching a binary tree. The algorithm is 
similar and goes with recursion. At each level, the search is optimized 
as if the key value is not present in the range of parent then the key 
is present in another branch. As these values limit the search they are 
also known as limiting value or separation value. If we reach a leaf 
node and don’t find the desired key then it will display NULL.

**Algorithm for Searching an Element:-**

```
    BtreeSearch(x, k)
    i = 1

    while i ≤ n[x] and k ≥ keyi[x]        // n[x] means number of keys in x node
           do i = i + 1

    if i  n[x] and k = keyi[x]
           then return (x, i)

    if leaf [x]
           then return NIL

    else
           return BtreeSearch(ci[x], k)
```

**Example: Searching 120 in the given B-Tree.** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200506235136/output253.png

**Solution:** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200507002340/output254.png

!https://media.geeksforgeeks.org/wp-content/uploads/20200507002453/output255.png

!https://media.geeksforgeeks.org/wp-content/uploads/20200507002619/output256.png

In
 this example, we can see that our search was reduced by just limiting 
the chances where the key containing the value could be present. 
Similarly if within the above example we’ve to look for 180, then the 
control will stop at step 2 because the program will find that the key 
180 is present within the current node. And similarly, if it’s to seek 
out 90 then as 90 < 100 so it’ll go to the left subtree automatically
 and therefore the control flow will go similarly as shown within the 
above example.

`// C++ implementation of search() and traverse() methods`

`#include<iostream>`

`using` `namespace` `std;`

`// A BTree node`

`class` `BTreeNode`

`{`

`int` `*keys;  // An array of keys`

`int` `t;      // Minimum degree (defines the range for number of keys)`

`BTreeNode **C; // An array of child pointers`

`int` `n;     // Current number of keys`

`bool` `leaf; // Is true when node is leaf. Otherwise false`

`public:`

`BTreeNode(int` `_t, bool` `_leaf);   // Constructor`

`// A function to traverse all nodes in a subtree rooted with this node`

`void` `traverse();`

`// A function to search a key in the subtree rooted with this node.`

`BTreeNode *search(int` `k);   // returns NULL if k is not present.`

`// Make the BTree friend of this so that we can access private members of this`

`// class in BTree functions`

`friend` `class` `BTree;`

`};`

`// A BTree`

`class` `BTree`

`{`

`BTreeNode *root; // Pointer to root node`

`int` `t;  // Minimum degree`

`public:`

`// Constructor (Initializes tree as empty)`

`BTree(int` `_t)`

`{  root = NULL;  t = _t; }`

`// function to traverse the tree`

`void` `traverse()`

`{  if` `(root != NULL) root->traverse(); }`

`// function to search a key in this tree`

`BTreeNode* search(int` `k)`

`{  return` `(root == NULL)? NULL : root->search(k); }`

`};`

`// Constructor for BTreeNode class`

`BTreeNode::BTreeNode(int` `_t, bool` `_leaf)`

`{`

`// Copy the given minimum degree and leaf property`

`t = _t;`

`leaf = _leaf;`

`// Allocate memory for maximum number of possible keys`

`// and child pointers`

`keys = new` `int[2*t-1];`

`C = new` `BTreeNode *[2*t];`

`// Initialize the number of keys as 0`

`n = 0;`

`}`

`// Function to traverse all nodes in a subtree rooted with this node`

`void` `BTreeNode::traverse()`

`{`

`// There are n keys and n+1 children, traverse through n keys`

`// and first n children`

`int` `i;`

`for` `(i = 0; i < n; i++)`

`{`

`// If this is not leaf, then before printing key[i],`

`// traverse the subtree rooted with child C[i].`

`if` `(leaf == false)`

`C[i]->traverse();`

`cout << " "` `<< keys[i];`

`}`

`// Print the subtree rooted with last child`

`if` `(leaf == false)`

`C[i]->traverse();`

`}`

`// Function to search key k in subtree rooted with this node`

`BTreeNode *BTreeNode::search(int` `k)`

`{`

`// Find the first key greater than or equal to k`

`int` `i = 0;`

`while` `(i < n && k > keys[i])`

`i++;`

`// If the found key is equal to k, return this node`

`if` `(keys[i] == k)`

`return` `this;`

`// If the key is not found here and this is a leaf node`

`if` `(leaf == true)`

`return` `NULL;`

`// Go to the appropriate child`

`return` `C[i]->search(k);`

`}`

The above code doesn’t contain the driver program. We will be covering the complete program in our next post on [B-Tree Insertion](https://www.geeksforgeeks.org/b-tree-set-1-insert-2/).There are two conventions to define a B-Tree, one is to define by minimum degree (followed in [Cormen book](http://www.flipkart.com/introduction-algorithms-3rd/p/itmczynzhyhxv2gs?pid=9788120340077&affid=sandeepgfg)),
 second is define by order. We have followed the minimum degree 
convention and will be following same in coming posts on B-Tree. The 
variable names used in the above program are also kept same as Cormen 
book for better readability.

**Applications of B-Trees:**

- It is used in large databases to access data stored in the disk
- Searching of data in a data set can be achieved in significantly less time using B tree
- With the indexing feature multilevel indexing can be achieved.
- Most of the servers also use B-tree approach.

**Insertion and Deletion** [B-Tree Insertion](https://www.geeksforgeeks.org/b-tree-set-1-insert-2/) [B-Tree Deletion](https://www.geeksforgeeks.org/b-tree-set-3delete/)**References:** [Introduction to Algorithms 3rd Edition by Clifford Stein, Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest](http://www.flipkart.com/introduction-algorithms-3rd/p/itmczynzhyhxv2gs?pid=9788120340077&affid=sandeepgfg)Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Persistent data structures
6.All 
the data structures discussed here so far are non-persistent (or 
ephemeral). A persistent data structure is a data structure that always 
preserves the previous version of itself when it is modified. They can 
be considered as ‘immutable’ as updates are not in-place.

A data structure is **partially persistent** if all versions can be accessed but only the newest version can be modified. **Fully persistent**
 if every version can be both accessed and modified. Confluently 
persistent is when we merge two or more versions to get a new version. 
This induces a DAG on the version graph.

Persistence can be 
achieved by simply copying, but this is inefficient in CPU and RAM usage
 as most operations will make only a small change in the DS. Therefore, a
 better method is to exploit the similarity between the new and old 
versions to share structure between them.

**Examples:**

1. **Linked List Concatenation :** Consider the problem of concatenating two singly linked lists with n
and m as the number of nodes in them. Say n>m. We need to keep the
versions, i.e., we should be able to original list.One way is to
make a copy of every node and do the connections. O(n + m) for traversal of lists and O(1) each for adding (n + m – 1) connections.Other
way, and a more efficient way in time and space, involves traversal of
only one of the two lists and fewer new connections. Since we assume
m<n, we can pick list with m nodes to be copied. This means O(m) for
traversal and O(1) for each one of the (m) connections. We must copy it
otherwise the original form of list wouldn’t have persisted.
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/persistence-linked-list.png
    
2. **Binary Search Tree Insertion :** Consider the problem of insertion of a new node in a binary search
tree. Being a binary search tree, there is a specific location where the new node will be placed. All the nodes in the path from the new node to the root of the BST will observe a change in structure (cascading). For example, the node for which the new node is the child will now have a
new pointer. This change in structure induces change in the complete
path up to the root. Consider tree below with value for node listed
inside each one of them.
    
    !https://media.geeksforgeeks.org/wp-content/cdn-uploads/persistent-Tree.png
    

**Approaches to make data structures persistent**For
 the methods suggested below, updates and access time and space of 
versions vary with whether we are implementing full or partial 
persistence.

1. **Path copying:** Make a copy of
the node we are about to update. Then proceed for update. And finally,
cascade the change back through the data structure, something very
similar to what we did in example two above. This causes a chain of
updates, until you reach a node no other node points to — the root.How to access the state at time t? Maintain an array of roots indexed by timestamp.
2. **Fat nodes:** As the name suggests, we make every node store its modification history, thereby making it ‘fat’.
3. **Nodes with boxes:** Amortized O(1) time and space can be achieved for access and updates.
This method was given by Sleator, Tarjan and their team. For a tree, it
involves using a modification box that can hold:a. one modification
to the node (the modification could be of one of the pointers, or to the node’s key or to some other node-specific data)b. the time when the mod was appliedThe timestamp is crucial for reaching to the version of the node we care
about. One can read more about it here. With this algorithm, given any
time t, at most one modification box exists in the data structure with
time t. Thus, a modification at time t splits the tree into three parts: onepart contains the data from before time t, one part contains the data from after time t, andone part was unaffected by the modification (Source: [MIT OCW](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-854j-advanced-algorithms-fall-2005/lecture-notes/persistent.pdf) ).Non-tree data structures may require more than one modification box, but limited to in-degree of the node for amortized O(1).

**Useful Links:**

1. [MIT OCW](https://www.youtube.com/watch?v=T0yzrZL1py0) (Erik Demaine)
2. [MIT OCW](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-854j-advanced-algorithms-fall-2005/lecture-notes/persistent.pdf) (David Karger)
3. [Dann Toliver’s talk](https://www.youtube.com/watch?v=2XH_q494U3U)
4. [Wiki Page](https://en.wikipedia.org/wiki/Persistent_data_structure)

**[Persistent Segment Tree | Set 1 (Introduction)](https://www.geeksforgeeks.org/persistent-segment-tree-set-1-introduction/)**

This article is contributed by **Yash Varyani**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-24-12-30-04-MasterAdvancedInArticleAd.webp
*** Ternary Search Tree
A 
ternary search tree is a special trie data structure where the child 
nodes of a standard trie are ordered as a binary search tree.

**Representation of ternary search trees:** Unlike
 trie(standard) data structure where each node contains 26 pointers for 
its children, each node in a ternary search tree contains only 3 
pointers: 1. The left pointer points to the node whose value is less than the value in the current node. 2. The equal pointer points to the node whose value is equal to the value in the current node. 3. The right pointer points to the node whose value is greater than the value in the current node.Apart
 from above three pointers, each node has a field to indicate 
data(character in case of dictionary) and another field to mark end of a
 string. So, more or less it is similar to BST which stores data 
based on some order. However, data in a ternary search tree is 
distributed over the nodes. e.g. It needs 4 nodes to store the word 
“Geek”.

Below figure shows how exactly the words in a ternary search tree are stored?

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Ternary-Search-Tree.png

One
 of the advantage of using ternary search trees over tries is that 
ternary search trees are a more space efficient (involve only three 
pointers per node as compared to 26 in standard tries). Further, ternary
 search trees can be used any time a hashtable would be used to store 
strings.Tries are suitable when there is a proper distribution of 
words over the alphabets so that spaces are utilized most efficiently. 
Otherwise ternary search trees are better. Ternary search trees are 
efficient to use(in terms of space) when the strings to be stored share a
 common prefix.

**Applications of ternary search trees:** **1.** Ternary
 search trees are efficient for queries like “Given a word, find the 
next word in dictionary(near-neighbor lookups)” or “Find all telephone 
numbers starting with 9342 or “typing few starting characters in a web 
browser displays all website names with this prefix”(Auto complete 
feature)”.**2.** Used in spell checks: Ternary search 
trees can be used as a dictionary to store all the words. Once the word 
is typed in an editor, the word can be parallelly searched in the 
ternary search tree to check for correct spelling.

**Implementation:** Following is C implementation of ternary search tree. The operations implemented are, search, insert, and traversal.

`// C++ program to demonstrate Ternary Search Tree (TST)`

`// insert, traverse and search operations`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`#define MAX 50`

`// A node of ternary search tree`

`struct` `Node {`

`char` `data;`

`// True if this character is last character of one of`

`// the words`

`unsigned isEndOfString = 1;`

`Node *left, *eq, *right;`

`};`

`// A utility function to create a new ternary search tree`

`// node`

`Node* newNode(char` `data)`

`{`

`Node* temp = new` `Node();`

`temp->data = data;`

`temp->isEndOfString = 0;`

`temp->left = temp->eq = temp->right = NULL;`

`return` `temp;`

`}`

`// Function to insert a new word in a Ternary Search Tree`

`void` `insert(Node** root, char* word)`

`{`

`// Base Case: Tree is empty`

`if` `(!(*root))`

`*root = newNode(*word);`

`// If current character of word is smaller than root's`

`// character, then insert this word in left subtree of`

`// root`

`if` `((*word) < (*root)->data)`

`insert(&((*root)->left), word);`

`// If current character of word is greater than root's`

`// character, then insert this word in right subtree of`

`// root`

`else` `if` `((*word) > (*root)->data)`

`insert(&((*root)->right), word);`

`// If current character of word is same as root's`

`// character,`

`else` `{`

`if` `(*(word + 1))`

`insert(&((*root)->eq), word + 1);`

`// the last character of the word`

`else`

`(*root)->isEndOfString = 1;`

`}`

`}`

`// A recursive function to traverse Ternary Search Tree`

`void` `traverseTSTUtil(Node* root, char* buffer, int` `depth)`

`{`

`if` `(root) {`

`// First traverse the left subtree`

`traverseTSTUtil(root->left, buffer, depth);`

`// Store the character of this node`

`buffer[depth] = root->data;`

`if` `(root->isEndOfString) {`

`buffer[depth + 1] = '\0';`

`cout << buffer << endl;`

`}`

`// Traverse the subtree using equal pointer (middle`

`// subtree)`

`traverseTSTUtil(root->eq, buffer, depth + 1);`

`// Finally Traverse the right subtree`

`traverseTSTUtil(root->right, buffer, depth);`

`}`

`}`

`// The main function to traverse a Ternary Search Tree.`

`// It mainly uses traverseTSTUtil()`

`void` `traverseTST(struct` `Node* root)`

`{`

`char` `buffer[MAX];`

`traverseTSTUtil(root, buffer, 0);`

`}`

`// Function to search a given word in TST`

`int` `searchTST(Node* root, char* word)`

`{`

`if` `(!root)`

`return` `0;`

`if` `(*word < (root)->data)`

`return` `searchTST(root->left, word);`

`else` `if` `(*word > (root)->data)`

`return` `searchTST(root->right, word);`

`else` `{`

`if` `(*(word + 1) == '\0')`

`return` `root->isEndOfString;`

`return` `searchTST(root->eq, word + 1);`

`}`

`}`

`// Driver program to test above functions`

`int` `main()`

`{`

`Node* root = NULL;`

`char` `cat[] = "cat";`

`char` `cats[] = "cats";`

`char` `up[] = "up";`

`char` `bug[] = "bug";`

`char` `bu[] = "bu";`

`insert(&root, cat);`

`insert(&root, cats);`

`insert(&root, up);`

`insert(&root, bug);`

`cout << "Following is traversal of ternary search "`

`"tree\n";`

`traverseTST(root);`

`cout << "\nFollowing are search results for cats, bu "`

`"and cat respectively\n";`

`searchTST(root, cats) ? cout << "Found\n"`

`: cout << "Not Found\n";`

`searchTST(root, bu) ? cout << "Found\n"`

`: cout << "Not Found\n";`

`searchTST(root, cat) ? cout << "Found\n"`

`: cout << "Not Found\n";`

`return` `0;`

`}`

`// This code is contributed by tapeshdua420.`

**Output:**

```
Following is traversal of ternary search tree
bug
cat
cats
up

Following are search results for cats, bu and cat respectively
Found
Not Found
Found
```

**Time Complexity:** The time complexity of 
the ternary search tree operations is similar to that of binary search 
tree. i.e. the insertion, deletion, and search operations take time 
proportional to the height of the ternary search tree. The space is 
proportional to the length of the string to be stored.

**Reference:** http://en.wikipedia.org/wiki/Ternary_search_treeThis article is compiled by **[Aashish Barnwal](https://www.facebook.com/barnwal.aashish)** and
 reviewed by GeeksforGeeks team. Please write comments if you find 
anything incorrect, or you want to share more information about the 
topic discussed above.
*** RopRopes Data Structure (Fast String Concatenation)es Data Structure (Fast String Concatenation)
One 
of the most common operations on strings is appending or concatenation. 
Appending to the end of a string when the string is stored in the 
traditional manner (i.e. an array of characters) would take a minimum of
 O(n) time (where n is the length of the original string). We can reduce time taken by append using Ropes Data Structure. 

**Ropes Data Structure**

A Rope is a binary tree structure where each node except the leaf nodes, ***contains the number of characters present to the left of that node***. Leaf nodes contain the actual string broken into substrings (size of these substrings can be decided by the user).Consider the image below.

!https://media.geeksforgeeks.org/wp-content/uploads/ropeDataStructure-1.jpg

The
 image shows how the string is stored in memory. Each leaf node contains
 substrings of the original string and all other nodes contain the 
number of characters present to the left of that node. The idea behind 
storing the number of characters to the left is to minimise the cost of 
finding the character present at i-th position.**Advantages** 1. Ropes drastically cut down the cost of appending two strings. 2. Unlike arrays, ropes do not require large contiguous memory allocations. 3. Ropes do not require O(n) additional memory to perform operations like insertion/deletion/searching. 4.
 In case a user wants to undo the last concatenation made, he can do so 
in O(1) time by just removing the root node of the tree.**Disadvantages** 1. The complexity of source code increases. 2. Greater chances of bugs. 3. Extra memory required to store parent nodes. 4. Time to access i-th character increases.**Now let’s look at a situation that explains why Ropes are a good substitute to monolithic string arrays.** Given two strings a[] and b[]. Concatenate them in a third string c[].Examples: 

```
Input  : a[] = "This is ", b[] = "an apple"
Output : "This is an apple"

Input  : a[] = "This is ", b[] = "geeksforgeeks"
Output : "This is geeksforgeeks"
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

**Method 1 (Naive method)**

We
 create a string c[] to store concatenated string. We first traverse a[]
 and copy all characters of a[] to c[]. Then we copy all characters of 
b[] to c[]. **Implementation:**

`// Simple C++ program to concatenate two strings`

`#include <iostream>`

`using` `namespace` `std;`

`// Function that concatenates strings a[0..n1-1]`

`// and b[0..n2-1] and stores the result in c[]`

`void` `concatenate(char` `a[], char` `b[], char` `c[],`

`int` `n1, int` `n2)`

`{`

`// Copy characters of A[] to C[]`

`int` `i;`

`for` `(i=0; i<n1; i++)`

`c[i] = a[i];`

`// Copy characters of B[]`

`for` `(int` `j=0; j<n2; j++)`

`c[i++] = b[j];`

`c[i] = '\0';`

`}`

`// Driver code`

`int` `main()`

`{`

`char` `a[] =  "Hi This is geeksforgeeks. ";`

`int` `n1 = sizeof(a)/sizeof(a[0]);`

`char` `b[] =  "You are welcome here.";`

`int` `n2 = sizeof(b)/sizeof(b[0]);`

`// Concatenate a[] and b[] and store result`

`// in c[]`

`char` `c[n1 + n2 - 1];`

`concatenate(a, b, c, n1, n2);`

`for` `(int` `i=0; i<n1+n2-1; i++)`

`cout << c[i];`

`return` `0;`

`}`

**Output:** 

```
Hi This is geeksforgeeks. You are welcome here
```

**Time complexity :** O(n)Now let’s try to solve the same problem using Ropes.

**Method 2 (Rope structure method)**

This rope structure can be utilized to concatenate two strings in constant time. 1. Create a new root node (that stores the root of the new concatenated string) 2. Mark the left child of this node, the root of the string that appears first. 3. Mark the right child of this node, the root of the string that appears second.And that’s it. Since this method only requires to make a new node, it’s complexity is **O(1)**.Consider the image below (Image source : https://en.wikipedia.org/wiki/Rope_(data_structure))

!https://media.geeksforgeeks.org/wp-content/uploads/RopeDS.jpg

**Implementation:**

`// C++ program to concatenate two strings using`

`// rope data structure.`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Maximum no. of characters to be put in leaf nodes`

`const` `int` `LEAF_LEN = 2;`

`// Rope structure`

`class` `Rope`

`{`

`public:`

`Rope *left, *right, *parent;`

`char` `*str;`

`int` `lCount;`

`};`

`// Function that creates a Rope structure.`

`// node --> Reference to pointer of current root node`

`//   l  --> Left index of current substring (initially 0)`

`//   r  --> Right index of current substring (initially n-1)`

`//   par --> Parent of current node (Initially NULL)`

`void` `createRopeStructure(Rope *&node, Rope *par,`

`char` `a[], int` `l, int` `r)`

`{`

`Rope *tmp = new` `Rope();`

`tmp->left = tmp->right = NULL;`

`// We put half nodes in left subtree`

`tmp->parent = par;`

`// If string length is more`

`if` `((r-l) > LEAF_LEN)`

`{`

`tmp->str = NULL;`

`tmp->lCount = (r-l)/2;`

`node = tmp;`

`int` `m = (l + r)/2;`

`createRopeStructure(node->left, node, a, l, m);`

`createRopeStructure(node->right, node, a, m+1, r);`

`}`

`else`

`{`

`node = tmp;`

`tmp->lCount = (r-l);`

`int` `j = 0;`

`tmp->str = new` `char[LEAF_LEN];`

`for` `(int` `i=l; i<=r; i++)`

`tmp->str[j++] = a[i];`

`}`

`}`

`// Function that prints the string (leaf nodes)`

`void` `printstring(Rope *r)`

`{`

`if` `(r==NULL)`

`return;`

`if` `(r->left==NULL && r->right==NULL)`

`cout << r->str;`

`printstring(r->left);`

`printstring(r->right);`

`}`

`// Function that efficiently concatenates two strings`

`// with roots root1 and root2 respectively. n1 is size of`

`// string represented by root1.`

`// root3 is going to store root of concatenated Rope.`

`void` `concatenate(Rope *&root3, Rope *root1, Rope *root2, int` `n1)`

`{`

`// Create a new Rope node, and make root1`

`// and root2 as children of tmp.`

`Rope *tmp = new` `Rope();`

`tmp->parent = NULL;`

`tmp->left = root1;`

`tmp->right = root2;`

`root1->parent = root2->parent = tmp;`

`tmp->lCount = n1;`

`// Make string of tmp empty and update`

`// reference r`

`tmp->str = NULL;`

`root3 = tmp;`

`}`

`// Driver code`

`int` `main()`

`{`

`// Create a Rope tree for first string`

`Rope *root1 = NULL;`

`char` `a[] =  "Hi This is geeksforgeeks. ";`

`int` `n1 = sizeof(a)/sizeof(a[0]);`

`createRopeStructure(root1, NULL, a, 0, n1-1);`

`// Create a Rope tree for second string`

`Rope *root2 = NULL;`

`char` `b[] =  "You are welcome here.";`

`int` `n2 = sizeof(b)/sizeof(b[0]);`

`createRopeStructure(root2, NULL, b, 0, n2-1);`

`// Concatenate the two strings in root3.`

`Rope *root3 = NULL;`

`concatenate(root3, root1, root2, n1);`

`// Print the new concatenated string`

`printstring(root3);`

`cout << endl;`

`return` `0;`

`}`

**Output:** 

```
Hi This is geeksforgeeks. You are welcome here.
```

**Time Complexity**: O(1)

**Auxiliary Space:** O(1)This article is contributed by **Akhil Goel**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

RopRopes Data Structure (Fast String Concatenation)es Data Structure (Fast String Concatenation)
** Trie in DBMS
*** BK-Tree | Introduction & Implementation
BK 
Tree or Burkhard Keller Tree is a data structure that is used to perform
 spell check based on Edit Distance (Levenshtein distance) concept. BK 
trees are also used for approximate string matching. Various auto 
correct feature in many software can be implemented based on this data 
structure.

```
Pre-requisites :Edit distance ProblemMetric tree
```

Let’s
 say we have a dictionary of words and then we have some other words 
which are to be checked in the dictionary for spelling errors. We need 
to have collection of all words in the dictionary which are very close 
to the given word. For instance if we are checking a word “**ruk**” we will have **{“truck”,”buck”,”duck”,……}**.
 Therefore, spelling mistake can be corrected by deleting a character 
from the word or adding a new character in the word or by replacing the 
character in the word by some appropriate one. Therefore, we will be 
using the edit distance as a measure for correctness and matching of the
 misspelled word from the words in our dictionary.Now, let’s see the
 structure of our BK Tree. Like all other trees, BK Tree consists of 
nodes and edges. The nodes in the BK Tree will represent the individual 
words in our dictionary and there will be exactly the same number of 
nodes as the number of words in our dictionary. The edge will contain 
some integer weight that will tell us about the edit-distance from one 
node to another. Lets say we have an edge from node **u** to node **v** having some edge-weight **w**, then w is the edit-distance required to turn the string u to v.

Consider our dictionary with words : **{ “help” , “hell” , “hello”}**. Therefore, for this dictionary our BK Tree will look like the below one.

!https://media.geeksforgeeks.org/wp-content/uploads/17554971_1350416058376194_212983783_n.png

Every
 node in the BK Tree will have exactly one child with same 
edit-distance. In case, if we encounter some collision for edit-distance
 while inserting, we will then propagate the insertion process down the 
children until we find an appropriate parent for the string node.

Every insertion in the BK Tree will start from our root node. Root node can be any word from our dictionary.For example, let’s add another word “**shell**” to the above dictionary. Now our **Dict[] = {“help” , “hell” , “hello” , “shell”}**. It is now evident that “**shell**” has same edit-distance as “**hello**” has from the root node “**help**”
 i.e 2. Hence, we encounter a collision. Therefore, we deal this 
collision by recursively doing this insertion process on the 
pre-existing colliding node.So, now instead of inserting “**shell**” at the root node “**help**“, we will now insert it to the colliding node “**hello**“. Therefore, now the new node “**shell**” is added to the tree and it has node “**hello**”
 as its parent with the edge-weight of 2(edit-distance). Below pictorial
 representation describes the BK Tree after this insertion.

!https://media.geeksforgeeks.org/wp-content/uploads/17555358_1350416071709526_1845256507_n.png

So,
 till now we have understood how we will build our BK Tree. Now, the 
question arises that how to find the closest correct word for our 
misspelled word? First of all, we need to set a tolerance value. This **tolerance value**
 is simply the maximum edit distance from our misspelled word to the 
correct words in our dictionary. So, to find the eligible correct words 
within the tolerance limit, Naive approach will be to iterate over all 
the words in the dictionary and collect the words which are within the 
tolerance limit. But this approach has **O(n*m*n)** time complexity(**n** is the number of words in dict[], **m** is average size of correct word and **n** is length of misspelled word) which times out for larger size of dictionary.

Therefore,
 now the BK Tree comes into action. As we know that each node in BK Tree
 is constructed on basis of edit-distance measure from its parent. 
Therefore, we will directly be going from root node to specific nodes 
that lie within the tolerance limit. Lets, say our tolerance limit is **TOL** and the edit-distance of the current node from the misspelled word is **dist**.
 Therefore, now instead of iterating over all its children we will only 
iterate over its children that have edit distance in range

[**dist-TOL , dist+TOL**]. This will reduce our complexity by a large extent. We will discuss this in our time complexity analysis.Consider the below constructed BK Tree.

!https://media.geeksforgeeks.org/wp-content/uploads/17555345_1350416661709467_503833975_n.png

Let’s say we have a misspelled word “**oop**” and the tolerance limit is 2. Now, we will see how we will collect the expected correct for the given misspelled word.Iteration
 1: We will start checking the edit distance from the root node. D(“oop”
 -> “help”) = 3. Now we will iterate over its children having edit 
distance in range [ D-TOL , D+TOL ] i.e [1,5]Iteration 2: Let’s 
start iterating from the highest possible edit distance child i.e node 
“loop” with edit distance 4.Now once again we will find its edit 
distance from our misspelled word. D(“oop”,”loop”) = 1. here D = 1 
i.e D <= TOL , so we will add “loop” to the expected correct word 
list and process its child nodes having edit distance in range 
[D-TOL,D+TOL] i.e [1,3]

Iteration 3: Now, we are at node “troop” .
 Once again we will check its edit distance from misspelled word . 
D(“oop”,”troop”)=2 .Here again D <= TOL , hence again we will add 
“troop” to the expected correct word list.

We will proceed the 
same for all the words in the range [D-TOL,D+TOL] starting from the root
 node till the bottom most leaf node. This, is similar to a DFS 
traversal on a tree, with selectively visiting the child nodes whose 
edge weight lie in some given range.Therefore, at the end we will be left with only 2 expected words for the misspelled word “**oop**” i.e **{“loop”,** **“troop”}**

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ program to demonstrate working of BK-Tree`

`#include "bits/stdc++.h"`

`using` `namespace` `std;`

`// maximum number of words in dict[]`

`#define MAXN 100`

`// defines the tolerance value`

`#define TOL  2`

`// defines maximum length of a word`

`#define LEN 10`

`struct` `Node`

`{`

`// stores the word of the current Node`

`string word;`

`// links to other Node in the tree`

`int` `next[2*LEN];`

`// constructors`

`Node(string x):word(x)`

`{`

`// initializing next[i] = 0`

`for(int` `i=0; i<2*LEN; i++)`

`next[i] = 0;`

`}`

`Node() {}`

`};`

`// stores the root Node`

`Node RT;`

`// stores every Node of the tree`

`Node tree[MAXN];`

`// index for current Node of tree`

`int` `ptr;`

`int` `min(int` `a, int` `b, int` `c)`

`{`

`return` `min(a, min(b, c));`

`}`

`// Edit Distance`

`// Dynamic-Approach O(m*n)`

`int` `editDistance(string& a,string& b)`

`{`

`int` `m = a.length(), n = b.length();`

`int` `dp[m+1][n+1];`

`// filling base cases`

`for` `(int` `i=0; i<=m; i++)`

`dp[i][0] = i;`

`for` `(int` `j=0; j<=n; j++)`

`dp[0][j] = j;`

`// populating matrix using dp-approach`

`for` `(int` `i=1; i<=m; i++)`

`{`

`for` `(int` `j=1; j<=n; j++)`

`{`

`if` `(a[i-1] != b[j-1])`

`{`

`dp[i][j] = min( 1 + dp[i-1][j],  // deletion`

`1 + dp[i][j-1],  // insertion`

`1 + dp[i-1][j-1] // replacement`

`);`

`}`

`else`

`dp[i][j] = dp[i-1][j-1];`

`}`

`}`

`return` `dp[m][n];`

`}`

`// adds curr Node to the tree`

`void` `add(Node& root,Node& curr)`

`{`

`if` `(root.word == ""` `)`

`{`

`// if it is the first Node`

`// then make it the root Node`

`root = curr;`

`return;`

`}`

`// get its editDistance from the Root Node`

`int` `dist = editDistance(curr.word,root.word);`

`if` `(tree[root.next[dist]].word == "")`

`{`

`/* if no Node exists at this dist from root`

`* make it child of root Node*/`

`// incrementing the pointer for curr Node`

`ptr++;`

`// adding curr Node to the tree`

`tree[ptr] = curr;`

`// curr as child of root Node`

`root.next[dist] = ptr;`

`}`

`else`

`{`

`// recursively find the parent for curr Node`

`add(tree[root.next[dist]],curr);`

`}`

`}`

`vector <string> getSimilarWords(Node& root,string& s)`

`{`

`vector < string > ret;`

`if` `(root.word == "")`

`return` `ret;`

`// calculating editdistance of s from root`

`int` `dist = editDistance(root.word,s);`

`// if dist is less than tolerance value`

`// add it to similar words`

`if` `(dist <= TOL) ret.push_back(root.word);`

`// iterate over the string having tolerance`

`// in range (dist-TOL , dist+TOL)`

`int` `start = dist - TOL;`

`if` `(start < 0)`

`start = 1;`

`while` `(start <= dist + TOL)`

`{`

`vector <string> tmp =`

`getSimilarWords(tree[root.next[start]],s);`

`for` `(auto` `i : tmp)`

`ret.push_back(i);`

`start++;`

`}`

`return` `ret;`

`}`

`// driver program to run above functions`

`int` `main(int` `argc, char` `const` `*argv[])`

`{`

`// dictionary words`

`string dictionary[] = {"hell","help","shell","smell",`

`"fell","felt","oops","pop","oouch","halt"`

`};`

`ptr = 0;`

`int` `sz = sizeof(dictionary)/sizeof(string);`

`// adding dict[] words on to tree`

`for(int` `i=0; i<sz; i++)`

`{`

`Node tmp = Node(dictionary[i]);`

`add(RT,tmp);`

`}`

`string w1 = "ops";`

`string w2 = "helt";`

`vector < string > match = getSimilarWords(RT,w1);`

`cout << "similar words in dictionary for : "` `<< w1 << ":\n";`

`for` `(auto` `x : match)`

`cout << x << endl;`

`match = getSimilarWords(RT,w2);`

`cout << "Correct words in dictionary for "` `<< w2 << ":\n";`

`for` `(auto` `x : match)`

`cout << x << endl;`

`return` `0;`

`}`

**Output**

`similar words in dictionary for : ops:
oops
pop
Correct words in dictionary for helt:
hell
help
shell
fell
felt
halt`

**Time Complexity:** It is quite evident that the time complexity majorly depends on the tolerance limit. We will be considering **tolerance limit** to be **2**.
 Now, roughly estimating, the depth of BK Tree will be log n, where n is
 the size of dictionary. At every level we are visiting 2 nodes in the 
tree and performing edit distance calculation. Therefore, our Time 
Complexity will be **O(L1*L2*log n)**, here **L1** is the average length of word in our dictionary and **L2** is the length of misspelled. Generally, L1 and L2 will be small.

**References**

- https://en.wikipedia.org/wiki/BK-tree
- https://issues.apache.org/jira/browse/LUCENE-2230

This article is contributed by **[Nitish Kumar](https://www.linkedin.com/in/nk17kumar)**.
 If you like GeeksforGeeks and would like to contribute, you can also 
write an article using write.geeksforgeeks.org or mail your article to 
review-team@geeksforgeeks.org. See your article appearing on the 
GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Fibonacci Heap | Set 1 (Introduction)
Heaps are mainly used for implementing priority queue. We have discussed below heaps in previous posts.

- [Binary Heap](http://geeksquiz.com/binary-heap/)
- [Binomial Heap](https://www.geeksforgeeks.org/binomial-heap-2/)

In terms of Time Complexity, Fibonacci Heap beats both Binary and Binomial Heaps.

Below are [amortized time complexities](https://www.geeksforgeeks.org/analysis-algorithm-set-5-amortized-analysis-introduction/) of **Fibonacci Heap**.

```
1) Find Min:Θ(1)     [Same as both Binary and Binomial]
2) Delete Min:O(Log n) [Θ(Log n) in both Binary and Binomial]
3) Insert: Θ(1)     [Θ(Log n) in Binary and Θ(1) in Binomial]
4) Decrease-Key:Θ(1)     [Θ(Log n) in both Binary and Binomial]
5) Merge: Θ(1)     [Θ(m Log n) or Θ(m+n) in Binary and
                            Θ(Log n) in Binomial]
```

Like [Binomial Heap](https://www.geeksforgeeks.org/binomial-heap-2/),
 Fibonacci Heap is a collection of trees with min-heap or max-heap 
property. In Fibonacci Heap, trees can have any shape even all trees can
 be single nodes (This is unlike Binomial Heap where every tree has to 
be Binomial Tree).

Below is an example Fibonacci Heap taken from [here](https://www.cs.princeton.edu/~wayne/teaching/fibonacci-heap.pdf).

!https://media.geeksforgeeks.org/wp-content/uploads/Fibonacci-Heap.png

Fibonacci
 Heap maintains a pointer to minimum value (which is root of a tree). 
All tree roots are connected using circular doubly linked list, so all 
of them can be accessed using single ‘min’ pointer.

The main idea
 is to execute operations in “lazy” way. For example merge operation 
simply links two heaps, insert operation simply adds a new tree with 
single node. The operation extract minimum is the most complicated 
operation. It does delayed work of consolidating trees. This makes 
delete also complicated as delete first decreases key to minus infinite,
 then calls extract minimum.

**Below are some interesting facts about Fibonacci Heap**

1. The reduced time complexity of Decrease-Key has importance in Dijkstra and
Prim algorithms. With Binary Heap, time complexity of these algorithms
is O(VLogV + ELogV). If Fibonacci Heap is used, then time complexity is
improved to O(VLogV + E)
2. Although Fibonacci Heap looks promising time complexity wise, it has been found slow in practice as hidden
constants are high (Source [Wiki](https://en.wikipedia.org/wiki/Fibonacci_heap)).
3. Fibonacci heap are mainly called so because Fibonacci numbers are used in the
running time analysis. Also, every node in Fibonacci Heap has degree at
most O(log n) and the size of a subtree rooted in a node of degree k is
at least F, where F is the kth Fibonacci number.
    
    k+2
    
    k
    

We will soon be discussing Fibonacci Heap operations in detail.

This article is contributed by **Shivam**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Leftist Tree | Leftist Heap
A leftist tree or leftist heap is a priority queue implemented with a variant of a binary heap. Every node has an **s-value (or rank or distance)** which is the distance to the nearest leaf. In contrast to a binary heap (Which is always a [complete binary tree](https://www.geeksforgeeks.org/binary-tree-set-3-types-of-binary-tree/)), a leftist tree may be very unbalanced. Below are [time complexities](https://www.geeksforgeeks.org/analysis-algorithm-set-5-amortized-analysis-introduction/) of **Leftist Tree / Heap**.

```
Function       Complexity              Comparison
1) Get Min:O(1)      [same as both Binary and Binomial]
2) Delete Min:O(Log n)  [same as both Binary and Binomial]
3) Insert:O(Log n)  [O(Log n) in Binary and O(1) in
                            Binomial and O(Log n) for worst case]
4) Merge: O(Log n)  [O(Log n) in Binomial]
```

A leftist tree is a binary tree with properties:

1. **Normal Min Heap Property :** key(i) >= key(parent(i))
2. **Heavier on left side :** dist(right(i)) <= dist(left(i)). Here, dist(i) is the number of edges on the
shortest path from node i to a leaf node in extended binary tree
representation (In this representation, a null child is considered as
external or leaf node). The shortest path to a descendant external node
is through the right child. Every subtree is also a leftist tree and
dist( i ) = 1 + dist( right( i ) ).

**Example:**
 The below leftist tree is presented with its distance calculated for 
each node with the procedure mentioned above. The rightmost node has a 
rank of 0 as the right subtree of this node is null and its parent has a
 distance of 1 by dist( i ) = 1 + dist( right( i )). The same is 
followed for each node and their s-value( or rank) is calculated.

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_tree.jpg

From above second property, we can draw two conclusions :

1. The path from root to rightmost leaf is the shortest path from root to a leaf.
2. If the path to rightmost leaf has x nodes, then leftist heap has atleast 2 – 1 nodes. This means the length of path to rightmost leaf is O(log n) for a leftist heap with n nodes.
    
    x
    

**Operations :**

1. The main operation is merge().
2. deleteMin() (or extractMin() can be done by removing root and calling merge() for left and right subtrees.
3. insert() can be done be create a leftist tree with single key (key to be
inserted) and calling merge() for given tree and tree with single node.

**Idea behind Merging :** Since right subtree is smaller, the idea is to merge right subtree of a tree with other tree. Below are abstract steps.

1. Put the root with smaller value as the new root.
2. Hang its left subtree on the left.
3. Recursively merge its right subtree and the other tree.
4. Before returning from recursion: – Update dist() of merged root. – Swap left
and right subtrees just below root, if needed, to keep leftist property
of merged result

**Detailed Steps for Merge:**

1. Compare the roots of two heaps.
2. Push the smaller key into an empty stack, and move to the right child of smaller key.
3. Recursively compare two keys and go on pushing the smaller key onto the stack and move to its right child.
4. Repeat until a null node is reached.
5. Take the last node processed and make it the right child of the node at top
of the stack, and convert it to leftist heap if the properties of
leftist heap are violated.
6. Recursively go on popping the elements from the stack and making them the right child of new stack top.

**Example:** Consider two leftist heaps given below:

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_heap1.jpg

Merge them into a single leftist heap

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_heap2.jpg

The
 subtree at node 7 violates the property of leftist heap so we swap it 
with the left child and retain the property of leftist heap.

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_heap3.jpg

Convert to leftist heap. Repeat the process

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_heap4.jpg

!https://media.geeksforgeeks.org/wp-content/uploads/leftist_heap5.jpg

The
 worst case time complexity of this algorithm is O(log n) in the worst 
case, where n is the number of nodes in the leftist heap. **Another example of merging two leftist heap:**

!https://media.geeksforgeeks.org/wp-content/uploads/mergingTwoLeftistTree-1.jpg

**Implementation of leftist Tree / leftist Heap:**

`//C++ program for leftist heap / leftist tree`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Node Class Declaration`

`class` `LeftistNode`

`{`

`public:`

`int` `element;`

`LeftistNode *left;`

`LeftistNode *right;`

`int` `dist;`

`LeftistNode(int` `& element, LeftistNode *lt = NULL,`

`LeftistNode *rt = NULL, int` `np = 0)`

`{`

`this->element = element;`

`right = rt;`

`left = lt,`

`dist = np;`

`}`

`};`

`//Class Declaration`

`class` `LeftistHeap`

`{`

`public:`

`LeftistHeap();`

`LeftistHeap(LeftistHeap &rhs);`

`~LeftistHeap();`

`bool` `isEmpty();`

`bool` `isFull();`

`int` `&findMin();`

`void` `Insert(int` `&x);`

`void` `deleteMin();`

`void` `deleteMin(int` `&minItem);`

`void` `makeEmpty();`

`void` `Merge(LeftistHeap &rhs);`

`LeftistHeap & operator =(LeftistHeap &rhs);`

`private:`

`LeftistNode *root;`

`LeftistNode *Merge(LeftistNode *h1,`

`LeftistNode *h2);`

`LeftistNode *Merge1(LeftistNode *h1,`

`LeftistNode *h2);`

`void` `swapChildren(LeftistNode * t);`

`void` `reclaimMemory(LeftistNode * t);`

`LeftistNode *clone(LeftistNode *t);`

`};`

`// Construct the leftist heap`

`LeftistHeap::LeftistHeap()`

`{`

`root = NULL;`

`}`

`// Copy constructor.`

`LeftistHeap::LeftistHeap(LeftistHeap &rhs)`

`{`

`root = NULL;`

`*this` `= rhs;`

`}`

`// Destruct the leftist heap`

`LeftistHeap::~LeftistHeap()`

`{`

`makeEmpty( );`

`}`

`/* Merge rhs into the priority queue.`

`rhs becomes empty. rhs must be different`

`from this.*/`

`void` `LeftistHeap::Merge(LeftistHeap &rhs)`

`{`

`if` `(this` `== &rhs)`

`return;`

`root = Merge(root, rhs.root);`

`rhs.root = NULL;`

`}`

`/* Internal method to merge two roots.`

`Deals with deviant cases and calls recursive Merge1.*/`

`LeftistNode *LeftistHeap::Merge(LeftistNode * h1,`

`LeftistNode * h2)`

`{`

`if` `(h1 == NULL)`

`return` `h2;`

`if` `(h2 == NULL)`

`return` `h1;`

`if` `(h1->element < h2->element)`

`return` `Merge1(h1, h2);`

`else`

`return` `Merge1(h2, h1);`

`}`

`/* Internal method to merge two roots.`

`Assumes trees are not empty, and h1's root contains`

`smallest item.*/`

`LeftistNode *LeftistHeap::Merge1(LeftistNode * h1,`

`LeftistNode * h2)`

`{`

`if` `(h1->left == NULL)`

`h1->left = h2;`

`else`

`{`

`h1->right = Merge(h1->right, h2);`

`if` `(h1->left->dist < h1->right->dist)`

`swapChildren(h1);`

`h1->dist = h1->right->dist + 1;`

`}`

`return` `h1;`

`}`

`// Swaps t's two children.`

`void` `LeftistHeap::swapChildren(LeftistNode * t)`

`{`

`LeftistNode *tmp = t->left;`

`t->left = t->right;`

`t->right = tmp;`

`}`

`/* Insert item x into the priority queue, maintaining`

`heap order.*/`

`void` `LeftistHeap::Insert(int` `&x)`

`{`

`root = Merge(new` `LeftistNode(x), root);`

`}`

`/* Find the smallest item in the priority queue.`

`Return the smallest item, or throw Underflow if empty.*/`

`int` `&LeftistHeap::findMin()`

`{`

`return` `root->element;`

`}`

`/* Remove the smallest item from the priority queue.`

`Throws Underflow if empty.*/`

`void` `LeftistHeap::deleteMin()`

`{`

`LeftistNode *oldRoot = root;`

`root = Merge(root->left, root->right);`

`delete` `oldRoot;`

`}`

`/* Remove the smallest item from the priority queue.`

`Pass back the smallest item, or throw Underflow if empty.*/`

`void` `LeftistHeap::deleteMin(int` `&minItem)`

`{`

`if` `(isEmpty())`

`{`

`cout<<"Heap is Empty"<<endl;`

`return;`

`}`

`minItem = findMin();`

`deleteMin();`

`}`

`/* Test if the priority queue is logically empty.`

`Returns true if empty, false otherwise*/`

`bool` `LeftistHeap::isEmpty()`

`{`

`return` `root == NULL;`

`}`

`/* Test if the priority queue is logically full.`

`Returns false in this implementation.*/`

`bool` `LeftistHeap::isFull()`

`{`

`return` `false;`

`}`

`// Make the priority queue logically empty`

`void` `LeftistHeap::makeEmpty()`

`{`

`reclaimMemory(root);`

`root = NULL;`

`}`

`// Deep copy`

`LeftistHeap &LeftistHeap::operator =(LeftistHeap & rhs)`

`{`

`if` `(this` `!= &rhs)`

`{`

`makeEmpty();`

`root = clone(rhs.root);`

`}`

`return` `*this;`

`}`

`// Internal method to make the tree empty.`

`void` `LeftistHeap::reclaimMemory(LeftistNode * t)`

`{`

`if` `(t != NULL)`

`{`

`reclaimMemory(t->left);`

`reclaimMemory(t->right);`

`delete` `t;`

`}`

`}`

`// Internal method to clone subtree.`

`LeftistNode *LeftistHeap::clone(LeftistNode * t)`

`{`

`if` `(t == NULL)`

`return` `NULL;`

`else`

`return` `new` `LeftistNode(t->element, clone(t->left),`

`clone(t->right), t->dist);`

`}`

`//Driver program`

`int` `main()`

`{`

`LeftistHeap h;`

`LeftistHeap h1;`

`LeftistHeap h2;`

`int` `x;`

`int` `arr[]= {1, 5, 7, 10, 15};`

`int` `arr1[]= {22, 75};`

`h.Insert(arr[0]);`

`h.Insert(arr[1]);`

`h.Insert(arr[2]);`

`h.Insert(arr[3]);`

`h.Insert(arr[4]);`

`h1.Insert(arr1[0]);`

`h1.Insert(arr1[1]);`

`h.deleteMin(x);`

`cout<< x <<endl;`

`h1.deleteMin(x);`

`cout<< x <<endl;`

`h.Merge(h1);`

`h2 = h;`

`h2.deleteMin(x);`

`cout<< x << endl;`

`return` `0;`

`}`

**Output**

`1
22
5`

This article is contributed by **Sheena Kohli and Minal Sunil Parchand**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.
 
*** K-ary Heap
Prerequisite – [Binary Heap](http://geeksquiz.com/binary-heap/)

K-ary
 heaps are a generalization of binary heap(K=2) in which each node have K
 children instead of 2. Just like binary heap, it follows two 
properties:

1. Nearly complete binary tree, with all levels having maximum number of nodes except the last,
which is filled in left to right manner.
2. Like Binary Heap, it can be divided into two categories:
    1. Max k-ary heap (key at root is greater than all descendants and same is recursively true for all nodes).
    2. Min k-ary heap (key at root is lesser than all descendants and same is recursively true for all nodes)

**Examples:**

```
3-ary max heap - root node is maximum
                 of all nodes
         10
     /    |   \
   7      9     8
 / | \   /
4  6  5 7

3-ary min heap -root node is minimum
                of all nodes
         10
      /   |  \
    12    11  13
  / | \
14 15 18
```

The height of a complete k-ary tree with n-nodes is given by logkn.

**Applications of K-ary Heap**:

- K-ary heap when used in the implementation of [priority queue](http://geeksquiz.com/priority-queue-set-1-introduction/) allows faster decrease key operation as compared to binary heap ( O(logn)) for binary heap vs O(logn) for K-ary heap). Nevertheless, it causes the complexity of extractMin() operation to increase to O(k log n) as compared to the complexity of O(logn) when using binary heaps for priority queue. This allows K-ary heap to
be more efficient in algorithms where decrease priority operations are
more common than extractMin() operation.Example: [Dijkstra’s](https://www.geeksforgeeks.org/greedy-algorithms-set-6-dijkstras-shortest-path-algorithm/) algorithm for single source shortest path and [Prim’s](https://www.geeksforgeeks.org/greedy-algorithms-set-5-prims-minimum-spanning-tree-mst-2/) algorithm for minimum spanning tree
    
    2
    
    k
    
    k
    
    2
    
- K-ary heap has better memory cache behaviour than a binary heap which allows
them to run more quickly in practice, although it has a larger worst
case running time of both extractMin() and delete() operation (both
being O(k log n) ).
    
    k
    

**Implementation:**

Assuming 0 based indexing of array, an array represents a K-ary heap such that for any node we consider:

- Parent of the node at index i (except root node) is located at index (i-1)/k
- Children of the node at index i are at indices (k*i)+1 , (k*i)+2 …. (k*i)+k
- The last non-leaf node of a heap of size n is located at index (n-2)/k

**buildHeap()** : Builds a heap from an input array.

This
 function runs a loop starting from the last non-leaf node all the way 
upto the root node, calling a function restoreDown(also known as 
maHeapify) for each index that restores the passed index at the correct 
position of the heap by shifting the node down in the K-ary heap 
building it in a bottom up manner.

*Why do we start the loop from the last non-leaf node ?* Because
 all the nodes after that are leaf nodes which will trivially satisfy 
the heap property as they don’t have any children and hence, are already
 roots of a K-ary max heap.

**restoreDown() (or maxHeapify)** : Used to maintain heap property.

It
 runs a loop where it finds the maximum of all the node’s children, 
compares it with its own value and swaps if the max(value of all 
children) > (value at node). It repeats this step until the node is 
restored into its original position in the heap.

**extractMax() :** Extracting the root node.

A
 k-ary max heap stores the largest element in its root. It returns the 
root node, copies last node to the first, calls restore down on the 
first node thus maintaining the heap property.

**insert() :** Inserting a node into the heap

This
 can be achieved by inserting the node at the last position and calling 
restoreUp() on the given index to restore the node at its proper 
position in the heap. restoreUp() iteratively compares a given node with
 its parent, since in a max heap the parent is always greater than or 
equal to its children nodes, the node is swapped with its parent only 
when its key is greater than the parent.

Combining the above, following is the C++ implementation of K-ary heap.

`// C++ program to demonstrate all operations of`

`// k-ary Heap`

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`// function to heapify (or restore the max- heap`

`// property). This is used to build a k-ary heap`

`// and in extractMin()`

`// att[] -- Array that stores heap`

`// len -- Size of array`

`// index -- index of element to be restored`

`//         (or heapified)`

`void` `restoreDown(int` `arr[], int` `len, int` `index,`

`int` `k)`

`{`

`// child array to store indexes of all`

`// the children of given node`

`int` `child[k+1];`

`while` `(1)`

`{`

`// child[i]=-1 if the node is a leaf`

`// children (no children)`

`for` `(int` `i=1; i<=k; i++)`

`child[i] = ((k*index + i) < len) ?`

`(k*index + i) : -1;`

`// max_child stores the maximum child and`

`// max_child_index holds its index`

`int` `max_child = -1, max_child_index ;`

`// loop to find the maximum of all`

`// the children of a given node`

`for` `(int` `i=1; i<=k; i++)`

`{`

`if` `(child[i] != -1 &&`

`arr[child[i]] > max_child)`

`{`

`max_child_index = child[i];`

`max_child = arr[child[i]];`

`}`

`}`

`// leaf node`

`if` `(max_child == -1)`

`break;`

`// swap only if the key of max_child_index`

`// is greater than the key of node`

`if` `(arr[index] < arr[max_child_index])`

`swap(arr[index], arr[max_child_index]);`

`index = max_child_index;`

`}`

`}`

`// Restores a given node up in the heap. This is used`

`// in decreaseKey() and insert()`

`void` `restoreUp(int` `arr[], int` `index, int` `k)`

`{`

`// parent stores the index of the parent variable`

`// of the node`

`int` `parent = (index-1)/k;`

`// Loop should only run till root node in case the`

`// element inserted is the maximum restore up will`

`// send it to the root node`

`while` `(parent>=0)`

`{`

`if` `(arr[index] > arr[parent])`

`{`

`swap(arr[index], arr[parent]);`

`index = parent;`

`parent = (index -1)/k;`

`}`

`// node has been restored at the correct position`

`else`

`break;`

`}`

`}`

`// Function to build a heap of arr[0..n-1] and value of k.`

`void` `buildHeap(int` `arr[], int` `n, int` `k)`

`{`

`// Heapify all internal nodes starting from last`

`// non-leaf node all the way upto the root node`

`// and calling restore down on each`

`for` `(int` `i= (n-1)/k; i>=0; i--)`

`restoreDown(arr, n, i, k);`

`}`

`// Function to insert a value in a heap. Parameters are`

`// the array, size of heap, value k and the element to`

`// be inserted`

`void` `insert(int` `arr[], int* n, int` `k, int` `elem)`

`{`

`// Put the new element in the last position`

`arr[*n] = elem;`

`// Increase heap size by 1`

`*n = *n+1;`

`// Call restoreUp on the last index`

`restoreUp(arr, *n-1, k);`

`}`

`// Function that returns the key of root node of`

`// the heap and then restores the heap property`

`// of the remaining nodes`

`int` `extractMax(int` `arr[], int* n, int` `k)`

`{`

`// Stores the key of root node to be returned`

`int` `max = arr[0];`

`// Copy the last node's key to the root node`

`arr[0] = arr[*n-1];`

`// Decrease heap size by 1`

`*n = *n-1;`

`// Call restoreDown on the root node to restore`

`// it to the correct position in the heap`

`restoreDown(arr, *n, 0, k);`

`return` `max;`

`}`

`// Driver program`

`int` `main()`

`{`

`const` `int` `capacity = 100;`

`int` `arr[capacity] = {4, 5, 6, 7, 8, 9, 10};`

`int` `n = 7;`

`int` `k = 3;`

`buildHeap(arr, n, k);`

`printf("Built Heap : \n");`

`for` `(int` `i=0; i<n; i++)`

`printf("%d ", arr[i]);`

`int` `element = 3;`

`insert(arr, &n, k, element);`

`printf("\n\nHeap after insertion of %d: \n",`

`element);`

`for` `(int` `i=0; i<n; i++)`

`printf("%d ", arr[i]);`

`printf("\n\nExtracted max is %d",`

`extractMax(arr, &n, k));`

`printf("\n\nHeap after extract max: \n");`

`for` `(int` `i=0; i<n; i++)`

`printf("%d ", arr[i]);`

`return` `0;`

`}`

**Output**

`Built Heap : 
10 9 6 7 8 4 5 

Heap after insertion of 3: 
10 9 6 7 8 4 5 3 

Extracted max is 10

Heap after extract max: 
9 8 6 7 3 4 5`

**Time Complexity Analysis**

- For a k-ary heap, with n nodes the maximum height of the given heap will be logn. So restoreUp() run for maximum of logn times (as at every iteration the node is shifted one level up is case
of restoreUp() or one level down in case of restoreDown).
    
    k
    
    k
    
- restoreDown() calls itself recursively for k children. So time complexity of this functions is O(k logn).
    
    k
    
- Insert and decreaseKey() operations call restoreUp() once. So complexity is O(logn).
    
    k
    
- Since extractMax() calls restoreDown() once, its complexity O(k logn)
    
    k
    
- Time complexity of build heap is O(n) (Analysis is similar to binary heap)
*** Why is Binary Heap Preferred over BST for Priority Queue?
A typical [Priority Queue](http://geeksquiz.com/priority-queue-set-1-introduction/) requires following operations to be efficient.

1. Get Top Priority Element (Get minimum or maximum)
2. Insert an element
3. Remove top priority element
4. Decrease Key

A [Binary Heap](http://geeksquiz.com/binary-heap/) supports above operations with following time complexities:

1. O(1)
2. O(Logn)
3. O(Logn)
4. O(Logn)

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/heapvsbst.png

A Self Balancing Binary Search Tree like [AVL Tree](https://www.geeksforgeeks.org/avl-tree-set-1-insertion/), [Red-Black Tree,](https://www.geeksforgeeks.org/red-black-tree-set-1-introduction-2/) etc can also support above operations with same time complexities.

1. Finding minimum and maximum are not naturally O(1), but can be easily
implemented in O(1) by keeping an extra pointer to minimum or maximum
and updating the pointer with insertion and deletion if required. With
deletion we can update by finding inorder predecessor or successor.
2. Inserting an element is naturally O(Logn)
3. Removing maximum or minimum are also O(Logn)
4. Decrease key can be done in O(Logn) by doing a deletion followed by insertion. See [this](http://geeksquiz.com/how-to-implement-decrease-key-or-change-key-in-binary-search-tree/) for details.

**So why is Binary Heap Preferred for Priority Queue?**

- Since Binary Heap is implemented using arrays, there is always better
locality of reference and operations are more cache friendly.
- Although operations are of same time complexity, constants in Binary Search Tree are higher.
- We can build a Binary Heap in O(n) time. Self Balancing BSTs require O(nLogn) time to construct.
- Binary Heap doesn’t require extra space for pointers.
- Binary Heap is easier to implement.
- There are variations of Binary Heap like Fibonacci Heap that can support insert and decrease-key in Θ(1) time

**Is Binary Heap always better?**
 Although Binary Heap is for Priority Queue, BSTs have their own 
advantages and the list of advantages is in-fact bigger compared to 
binary heap.

- Searching an element in self-balancing BST is O(Logn) which is O(n) in Binary Heap.
- We can print all elements of BST in sorted order in O(n) time, but Binary Heap requires O(nLogn) time.
- [Floor and ceil](https://www.geeksforgeeks.org/floor-and-ceil-from-a-bst/) can be found in O(Logn) time.
- [K’th largest/smallest element](https://www.geeksforgeeks.org/find-k-th-smallest-element-in-bst-order-statistics-in-bst/) be found in O(Logn) time by augmenting tree with an additional field.

This article is contributed by **Vivek Gupta**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Heap queue (or heapq) in Python
[Heap data structure is mainly used to represent a priority queue](https://www.geeksforgeeks.org/applications-of-heap-data-structure/). In Python, it is available using “**heapq**” module. The property of this data structure in Python is that each time the **smallest of heap element is popped(min heap)**. Whenever elements are pushed or popped, **heap structure in maintained**. The heap[0] element also returns the smallest element each time. **Let’s see various Operations on heap :**

- **heapify(iterable)** :- This function is used to **convert the iterable into a heap** data structure. i.e. in heap order.
- **heappush(heap, ele)** :- This function is used to **insert the element** mentioned in its arguments into heap. The **order is adjusted**, so as **heap structure is maintained**.
- **heappop(heap)** :- This function is used to **remove and return the smallest element** from heap. The **order is adjusted**, so as **heap structure is maintained**.

`# Python code to demonstrate working of`

`# heapify(), heappush() and heappop()`

`# importing "heapq" to implement heap queue`

`import` `heapq`

`# initializing list`

`li =` `[5, 7, 9, 1, 3]`

`# using heapify to convert list into heap`

`heapq.heapify(li)`

`# printing created heap`

`print` `("The created heap is : ",end="")`

`print` `(list(li))`

`# using heappush() to push elements into heap`

`# pushes 4`

`heapq.heappush(li,4)`

`# printing modified heap`

`print` `("The modified heap after push is : ",end="")`

`print` `(list(li))`

`# using heappop() to pop smallest element`

`print` `("The popped and smallest element is : ",end="")`

`print` `(heapq.heappop(li))`

**Output :**

```
The created heap is : [1, 3, 9, 7, 5]
The modified heap after push is : [1, 3, 4, 7, 5, 9]
The popped and smallest element is : 1
```

- **heappushpop(heap, ele)** :- This function **combines the functioning of both push and pop operations** in one statement, increasing efficiency. Heap order is maintained after this operation.
- **heapreplace(heap, ele)** :- This function also inserts and pops element in one statement, but it is different from above function. In this, **element is first popped, then the element is pushed.i.e, the value larger than the pushed value can be returned.** heapreplace() returns the smallest value originally in heap regardless of the pushed element as opposed to heappushpop().

`# Python code to demonstrate working of`

`# heappushpop() and heapreplce()`

`# importing "heapq" to implement heap queue`

`import` `heapq`

`# initializing list 1`

`li1 =` `[5, 1, 9, 4, 3]`

`# initializing list 2`

`li2 =` `[5, 7, 9, 4, 3]`

`# using heapify() to convert list into heap`

`heapq.heapify(li1)`

`heapq.heapify(li2)`

`# using heappushpop() to push and pop items simultaneously`

`# pops 2`

`print` `("The popped item using heappushpop() is : ",end="")`

`print` `(heapq.heappushpop(li1, 2))`

`# using heapreplace() to push and pop items simultaneously`

`# pops 3`

`print` `("The popped item using heapreplace() is : ",end="")`

`print` `(heapq.heapreplace(li2, 2))`

**Output**

`The popped item using heappushpop() is : 1
The popped item using heapreplace() is : 3`

- **nlargest(k, iterable, key = fun)** :- This function is used to **return the k largest elements from the iterable specified and satisfying the key if mentioned.**
- **nsmallest(k, iterable, key = fun)** :- This function is used to **return the k smallest elements from the iterable specified and satisfying the key if mentioned.**

`# Python code to demonstrate working of`

`# nlargest() and nsmallest()`

`# importing "heapq" to implement heap queue`

`import` `heapq`

`# initializing list`

`li1 =` `[6, 7, 9, 4, 3, 5, 8, 10, 1]`

`# using heapify() to convert list into heap`

`heapq.heapify(li1)`

`# using nlargest to print 3 largest numbers`

`# prints 10, 9 and 8`

`print("The 3 largest numbers in list are : ",end="")`

`print(heapq.nlargest(3, li1))`

`# using nsmallest to print 3 smallest numbers`

`# prints 1, 3 and 4`

`print("The 3 smallest numbers in list are : ",end="")`

`print(heapq.nsmallest(3, li1))`

**Output :**

```
The 3 largest numbers in list are : [10, 9, 8]
The 3 smallest numbers in list are : [1, 3, 4]
```

This article is contributed by **[Manjeet Singh](https://auth.geeksforgeeks.org/profile.php?user=manjeet_04&list=practice)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks. Please 
write comments if you find anything incorrect, or you want to share more
 information about the topic discussed above.
 

** foot notes 
