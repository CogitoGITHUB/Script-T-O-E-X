#+title:      OS (Universal)


* TODO Operating Systems (Universal)
** Basics
***  Introduction to Operating Systems

**** Overview
An operating system (OS) acts as a mediator between a computer's user and its hardware, facilitating convenient and efficient program execution.

**** Purpose
- Provide an environment for convenient and efficient program execution.
- Manage computer hardware to ensure proper system operation and prevent interference from user programs.

**** Functions of Operating System
- Convenience
- Efficiency
- Ability to Evolve
- Throughput

***** Major Functionalities
- Resource Management
- Process Management
- Storage Management
- Memory Management
- Security/Privacy Management

**** Operating System as User Interface
1. User
2. System and application programs
3. Operating system
4. Hardware

**** Components of a Computer System
- Hardware
- System programs
- Application programs

***** Fig1: Conceptual view of a computer system
[Link to image: Conceptual view of a computer system](https://media.geeksforgeeks.org/wp-content/uploads/os.png)

**** Basic Tasks Performed by Operating Systems
1. Allocation and use of computing system resources.
2. Providing an interface between computer hardware and programmers for coding, creation, and debugging of application programs.

**** Supporting Tasks
1. Creating and modifying programs and data files.
2. Accessing compilers for translating programs.
3. Loading compiled programs into memory.
4. Handling I/O programming routines.

**** I/O System Management
- I/O traffic controller
- Memory management component
- General device driver interface
- Drivers for specific hardware devices

**** Assembler
- Translates assembly language programs into object programs.
- Output: Object program and information for loader preparation.

**** Compiler
- Processes high-level languages like FORTRAN, COBOL, etc.
- Produces object programs from source programs.
- Interprets source programs.

**** Loader
- Loads object programs into memory.
- Various loading schemes: absolute, relocating, direct-linking.

**** History of Operating Systems
- Evolution over the years.

**** Types of Operating Systems
- Batch Operating System
- Time-sharing Operating System
- Distributed Operating System
- Network Operating System
- Real-time Operating System

**** Examples of Operating Systems
- Windows
- GNU/Linux
- macOS
- Android
- iOS

*** Operating System Overview

An Operating System (OS) performs essential tasks like managing files, processes, and memory, acting as the manager of all resources, i.e., a resource manager. Thus, the operating system serves as an interface between the user and the machine.

**Types of Operating Systems**

Some widely used operating systems are as follows:

1. Batch Operating System
   - **Definition:** This type of operating system does not interact with the computer directly. Instead, an operator groups similar jobs with the same requirements into batches.
   - **Advantages:**
     - Difficulty in predicting job completion times.
     - Multiple users can share the system.
     - Reduced idle time.
     - Efficient management of large repetitive tasks.
   - **Disadvantages:**
     - Requires operators with knowledge of batch systems.
     - Difficult to debug.
     - Costly.
     - Other jobs wait indefinitely if one job fails.
   - **Examples:** Payroll System, Bank Statements, etc.
   - ![Batch Operating System Image](https://media.geeksforgeeks.org/wp-content/uploads/BatchOS.jpeg)

2. Time-Sharing Operating Systems
   - **Definition:** Each task is given some time to execute, ensuring smooth operation. Also known as Multitasking Systems.
   - **Advantages:**
     - Equal opportunity for each task.
     - Reduced chance of software duplication.
     - Decreased CPU idle time.
   - **Disadvantages:**
     - Reliability issues.
     - Security and data integrity concerns.
     - Communication issues.
   - **Examples:** Multics, Unix, etc.
   - ![Time-Sharing Operating System Image](https://media.geeksforgeeks.org/wp-content/uploads/Time-Share.jpeg)

3. Distributed Operating System
   - **Definition:** Various interconnected computers communicate using a shared network, each possessing its own memory unit and CPU.
   - **Advantages:**
     - Failure of one system doesn't affect others.
     - Faster data exchange.
     - Scalability and reduced load on host computers.
   - **Disadvantages:**
     - Failure of the main network halts all communication.
     - Lack of well-defined languages and complexity.
     - High cost and complexity.
   - **Examples:** LOCUS, etc.
   - ![Distributed Operating System Image](https://media.geeksforgeeks.org/wp-content/uploads/Distributed.jpeg)

4. Network Operating System
   - **Definition:** These systems run on servers and allow shared access to files, printers, security, etc., over a network.
   - **Advantages:**
     - Stable centralized servers.
     - Centralized security management.
     - Easy integration of new technologies.
     - Remote server access.
   - **Disadvantages:**
     - Costly servers.
     - Dependency on central location.
     - Regular maintenance required.
   - **Examples:** Microsoft Windows Server, UNIX, Linux, etc.
   - ![Network Operating System Image](https://media.geeksforgeeks.org/wp-content/uploads/Network-OS.jpeg)

5. Real-Time Operating System
   - **Definition:** These systems serve real-time applications with strict time constraints.
   - **Advantages:**
     - Maximum resource utilization.
     - Minimal task shifting time.
     - Focus on running applications.
     - Error-free operation.
     - Suitable for embedded systems.
   - **Disadvantages:**
     - Limited concurrent tasks.
     - Heavy system resource usage.
     - Complex algorithms.
     - Specific device drivers required.
     - Limited thread priority settings.
   - **Examples:** Scientific experiments, medical imaging systems, industrial control systems, etc.
   - ![Real-Time Operating System Image](https://media.geeksforgeeks.org/wp-content/uploads/RTOS.jpeg)

*** Introduction to Operating System

An *Operating System* (OS) acts as a vital communication bridge between users and computer hardware, facilitating the execution of programs in a convenient and efficient manner. It manages the allocation of computer hardware resources, ensuring their proper coordination to prevent interference with the system's functionality by user programs. Analogous to a boss delegating tasks to employees, users interact with the OS by issuing commands and requests.

**What is an Operating System?**
An Operating System is a software program that executes application programs and serves as the intermediary between users and computer hardware. Its primary task is resource allocation and service management, including memory, devices, processors, and information. The OS includes various programs such as a traffic controller, scheduler, memory management module, I/O programs, and file system to manage these resources effectively.

**Important functions of an Operating System:**
1. *Security:* Utilizes password protection and other techniques to safeguard user data and prevent unauthorized access.
2. *Control over system performance:* Monitors system health, response time, and overall performance to optimize efficiency.
3. *Job accounting:* Tracks time and resources used by tasks and users for resource management.
4. *Error detecting aids:* Constantly monitors the system to detect and prevent errors.
5. *Coordination between software and users:* Assigns interpreters, compilers, and other software to users as needed.
6. *Memory Management:* Manages primary memory allocation, tracking memory usage, and allocation for processes.
7. *Processor Management:* Controls process scheduling and CPU allocation in a multi-programming environment.
8. *Device Management:* Manages device communication via drivers, tracks connected devices, and allocates them effectively.
9. *File Management:* Organizes file systems into directories for efficient navigation and usage.

Additionally, the OS provides various services to the computer system, including:
1. *Program Execution:* Executes all types of programs efficiently.
2. *Handling Input/Output Operations:* Manages inputs and outputs from various devices.
3. *Manipulation of File System:* Decides storage and manipulation of data/files.
4. *Error Detection and Handling:* Detects and handles errors, ensuring system security.
5. *Resource Allocation:* Allocates resources effectively based on system requirements.
6. *Accounting:* Tracks and records system activities for analysis and optimization.
7. *Information and Resource Protection:* Ensures the security and protection of information and resources.

These services provided by the Operating System aim to simplify programming tasks and enhance user convenience, with various operating systems offering similar functionalities to varying degrees. 
*** Real time systems

A 
real-time system means that the system is subjected to real-time, i.e., 
the response should be guaranteed within a specified timing constraint 
or the system should meet the specified deadline. For example flight 
control systems, real-time monitors, etc.

### Types of real-time systems based on timing constraints:

1. **Hard real-time system:** This type of system can never miss its deadline. Missing the deadline may
have disastrous consequences. The usefulness of results produced by a
hard real-time system decreases abruptly and may become negative if
tardiness increases. Tardiness means how late a real-time system
completes its task with respect to its deadline. Example: Flight
controller system.
2. **Soft real-time system:** This type of system can miss its deadline occasionally with some acceptably
low probability. Missing the deadline have no disastrous consequences.
The usefulness of results produced by a soft real-time system decreases
gradually with an increase in tardiness. Example: Telephone switches.

### Reference model of the real-time system:

Our reference model is characterized by three elements:

1. **A workload model:** It specifies the application supported by the system.
2. **A resource model:** It specifies the resources available to the application.
3. **Algorithms:** It specifies how the application system will use resources.

### Terms related to real-time system:

1. **Job:** A job is a small piece of work that can be assigned to a processor and may or may not require resources.
2. **Task:** A set of related jobs that jointly provide some system functionality.
3. **Release time of a job:** It is the time at which the job becomes ready for execution.
4. **Execution time of a job:** It is the time taken by the job to finish its execution.
5. **Deadline of a job:** It is the time by which a job should finish its execution. Deadline is of two types: absolute deadline and relative deadline.
6. **Response time of a job:** It is the length of time from the release time of a job to the instant when it finishes.
7. The maximum allowable response time of a job is called its relative deadline.
8. The absolute deadline of a job is equal to its relative deadline plus its release time.
9. Processors are also known as active resources. They are essential for the
execution of a job. A job must have one or more processors in order to
execute and proceed towards completion. Example: computer, transmission
links.
10. Resources are also known as passive resources. A job may or may not require a resource during its execution. Example: memory,
mutex
11. Two resources are identical if they can be used interchangeably else they are heterogeneous.

*** Tasks in Real-Time Systems

Real-time systems are subject to stringent timing constraints, where responses must be guaranteed within specified deadlines. Examples include flight control systems, real-time monitors, etc.

**Types of Tasks:**

1. **Periodic Tasks:** Jobs in periodic tasks are released at regular intervals. A periodic task is defined by four parameters: Ti = <Φi, Pi, ei, Di>, where:
   - Φi: Phase of the task, representing the release time of the first job (assumed zero if not specified).
   - Pi: Period of the task, the time interval between the release times of two consecutive jobs.
   - ei: Execution time of the task.
   - Di: Relative deadline of the task.

   Example: Consider a task Ti with period = 5 and execution time = 3. Assuming the phase is not given, the first job is released at t = 0, then at t = 5, t = 10, and so on.

   ![Periodic Tasks](https://media.geeksforgeeks.org/wp-content/uploads/real-time.png)

   The hyper-period of a set of periodic tasks is the least common multiple of their periods.

2. **Dynamic Tasks:** These are invoked by the occurrence of events, categorized based on their criticality and occurrence times.
    - *Aperiodic Tasks:* Released at arbitrary time intervals with soft deadlines or no deadlines.
    - *Sporadic Tasks:* Repeat at random instances with hard deadlines, denoted by Ti = (ei, gi, Di), where:
        - ei: Execution time of the task.
        - gi: Minimum separation between consecutive instances.
        - Di: Relative deadline of the task.

**Jitter:** Variations in release time or execution time ranges are known as jitter, affecting task scheduling.

**Precedence Constraint of Jobs:**

Jobs in a task may have precedence constraints, indicating the specific order of execution. Represented by a partial order relation <, where Ji < Jj implies Ji must complete before Jj begins. An efficient representation is using a directed graph, where vertices represent jobs, and directed edges represent precedence constraints.

Example:
Consider a task T with jobs J1, J2, J3, J4, and J5, where J2 and J5 depend on J1:

![Precedence Constraints](https://media.geeksforgeeks.org/wp-content/uploads/real-time1.png)

Set representation of precedence graph:
1. <(1) = {}
2. <(2) = {1}
3. <(3) = {}
4. <(4) = {}
5. <(5) = {1}

Another example:

![Precedence Graph Example](https://media.geeksforgeeks.org/wp-content/uploads/real-time-2.png)

From the graph, precedence constraints can be derived.

*** The-diffrence-between-multiprogramming-multitasking-multithreding-multiprocessing
**** Multiprogramming

**Definition:**
Multiprogramming involves keeping multiple programs in main memory simultaneously ready for execution. It aims to maximize CPU utilization by allowing the CPU to switch between executing processes efficiently.

**Non-Multiprogrammed System:**
- In a non-multiprogrammed system, the CPU remains idle while waiting for processes to complete I/O tasks, leading to inefficient CPU utilization.
- Other waiting processes might not get a chance to execute, causing delays.

**Multiprogrammed System:**
- In a multiprogrammed system, when a process goes for an I/O task, the CPU switches to another ready process, maximizing CPU utilization.
- Processes are selected from a job pool and brought into main memory for execution.

![Multiprogramming](https://media.geeksforgeeks.org/wp-content/cdn-uploads/multiprogramming.jpg)

**** Multiprocessing

**Definition:**
Multiprocessing involves the use of two or more CPUs within a single computer system, allowing multiple processes to execute simultaneously.

**Advantages:**
- Increases processing speed by executing multiple processes simultaneously.
- Provides reliability by allowing remaining processors to continue working if one processor fails.

![Multiprocessing](https://media.geeksforgeeks.org/wp-content/cdn-uploads/multiPROCESSINGjpg.jpg)

**Difference between Multiprogramming and Multiprocessing:**
- Multiprogramming involves executing multiple processes concurrently using a single CPU, while multiprocessing utilizes multiple physical processors for simultaneous execution.
  
**** Multitasking

**Definition:**
Multitasking refers to executing multiple tasks or processes concurrently, allowing users to interact with various applications simultaneously.

**Working:**
- In a time-sharing system, each process is assigned a specific time quantum for execution, enabling multiple processes to share CPU time effectively.
- Users perceive simultaneous execution due to rapid context switching between tasks.

![Multitasking](https://media.geeksforgeeks.org/wp-content/cdn-uploads/multitasking.jpg)

**** Multithreading

**Definition:**
Multithreading allows a single process to have multiple threads running concurrently within its context, enabling efficient resource utilization and responsiveness.

**Working:**
- Threads share resources of the parent process but execute independently, enabling parallel execution of tasks.
- Improves responsiveness by allowing other threads to continue execution if one thread is blocked or takes too long.

![Multithreading](https://media.geeksforgeeks.org/wp-content/cdn-uploads/vlc.jpg)

**Advantages:**
- Increased responsiveness and resource utilization.
- Less costly compared to creating new processes.
*** Difference-between-32bit-&-64bit-operatingsystems

In computing, there are two types of processors existing, i.e., 32-bit and 64-bit processors. These types of processors tell us how much memory a processor can access from a CPU register. For instance, 

- A 32-bit system can access \(2^{32}\) different memory addresses, i.e 4 GB of RAM or physical memory ideally, it can access more than 4 GB of RAM also. A 64-bit system can access \(2^{64}\) different memory addresses, i.e actually 18-Quintillion bytes of RAM. In short, any amount of memory greater than 4 GB can be easily handled by it.

Most computers made in the 1990s and early 2000s were 32-bit machines. The CPU register stores memory addresses, which is how the processor accesses data from RAM. One bit in the register can reference an individual byte in memory, so a **32-bit** system can address a maximum of 4 GB (4,294,967,296 bytes) of RAM. *The actual limit is often less than* *around **3.5 GB** since part of the register is used to store other temporary values besides memory addresses*. Most computers released over the past two decades were built on a 32-bit architecture, hence most operating systems were designed to run on a 32-bit processor.

A **64-bit** register can theoretically reference \(2^{64}\) bytes of memory, or 17,179,869,184 GB (16 exabytes) of memory. This is several million times more than an average workstation would need to access. What’s important is that a 64-bit computer (which means it has a 64-bit processor) can access more than 4 GB of RAM. If a computer has 8 GB of RAM, it better has a 64-bit processor. Otherwise, at least 4 GB of the memory will be inaccessible by the CPU.

A major difference between **32-bit processors and 64-bit processors** is the number of calculations per second they can perform, which affects the speed at which they can complete tasks. 64-bit processors can come in dual-core, quad-core, six-core, and eight-core versions for home computing. Multiple cores allow for an increased number of calculations per second that can be performed, which can increase the processing power and help make a computer run faster. Software programs that require many calculations to function smoothly can operate faster and more efficiently on the multi-core 64-bit processors, for the most part.

**Advantages of 64-bit over 32-bit**

- Using 64-bit one can do a lot in multi-tasking, user can easily switch between various applications without any windows hanging problems.
- Gamers can easily play High graphical games like Modern Warfare, GTA V, or use high-end software like Photoshop or CAD which takes a lot of memory since it makes multi-tasking with big software, easy and efficient for users. However, upgrading the [[https://en.wikipedia.org/wiki/Video_card][video card]] instead of getting a 64-bit processor would be more beneficial.

**Note:**

- A computer with a 64-bit processor can have a 64-bit or 32-bit version of an operating system installed. However, with a 32-bit operating system, the 64-bit processor would not run at its full capability.
- On a computer with a 64-bit processor, we can’t run a 16-bit legacy program. Many 32-bit programs will work with a 64-bit processor and operating system, but some older 32-bit programs may not function properly, or at all, due to limited or no compatibility.

*** What-happens-when-we-turn-on-the-computer

# A computer without a program running is just an inert hunk of electronics.
# The first thing a computer has to do when it is turned on is to start 
# up a special program called an operating system. The operating system’s 
# job is to help other computer programs work by handling the messy 
# details of controlling the computer’s hardware.

**** An overview of the boot process

[[https://media.geeksforgeeks.org/wp-content/cdn-uploads/20201016104502/12331.png][Boot Process Overview]]

The boot process is something that happens every time you turn your computer on. You don’t really see it, because it happens so fast. You press the power button and come back a few sec (or minutes if on slow storage like HDD) later and Windows 10, or Windows 11, or whatever Operating System you use is all loaded.

The BIOS chip tells it to look in a fixed place, usually on the lowest-numbered hard disk (the boot disk) for a special program called a boot loader (under Linux the boot loader is called Grub or LILO). The boot loader is pulled into memory and started. The boot loader’s job is to start the real operating system.

**** Functions of BIOS **

1. ** POST (Power On Self Test): **
   - The Power On Self Test happens each time you turn your computer on. It sounds complicated and that’s because it kind of is. Your computer does so much when it’s turned on and this is just part of that.
   - It initializes the various hardware devices.
   - It is an important process to ensure that all the devices operate smoothly without any conflicts. BIOSes following ACPI create tables describing the devices in the computer.
   - The POST first checks the bios and then tests the CMOS RAM.
   - If there is no problem with this then POST continues to check the CPU, hardware devices such as the Video Card, and the secondary storage devices such as the Hard Drive, Floppy Drives, Zip Drive, or CD/DVD Drives.
   - If some errors are found then an error message is displayed on the screen or a number of beeps are heard.
   - These beeps are known as POST beep codes.

2. ** Master Boot Record: ** 
   The Master Boot Record (MBR) is a small program that starts when the computer is booting, in order to find the operating system (eg. Windows XP). This complicated process (called the Boot Process) starts with the POST (Power On Self Test) and ends when the Bios searches for the MBR on the Hard Drive, which is generally located in the first sector, first head, first cylinder (cylinder 0, head 0, sector 1).

   A typical structure looks like this:

   [[https://media.geeksforgeeks.org/wp-content/uploads/mbr-2.png][Master Boot Record]]

   The bootstrap loader is stored in the computer’s EPROM, ROM, or another non-volatile memory. When the computer is turned on or restarted, it first performs the power-on-self-test, also known as POST. If the POST is successful and no issues are found, the bootstrap loader will load the operating system for the computer into memory. The computer will then be able to quickly access, load, and run the operating system.

3. ** init: **
   init is the last step of the kernel boot sequence. It looks for the file */etc/inittab* to see if there is an entry for *initdefault*.
   It is used to determine the initial run level of the system. A run-level is used to decide the initial state of the operating system. Some of the run levels are:
   - Level 0: System Halt.
   - Level 1: Single user mode.
   - Level 2: Full multiuser mode without network.
   - Level 3: Full multiuser mode with network.
   - Level 4: user definable.
   - Level 5: Full multiuser mode with network and X display manager.
   - Level 6: Reboot.

   The above design of init is called SysV- pronounced as [System five](https://en.wikipedia.org/wiki/UNIX_System_V). Several other implementations of init have been written now. Some of the popular implementations are systemd and upstart. Upstart is being used by ubuntu since 2006. More details of the upstart can be found [[https://help.ubuntu.com/community/UbuntuBootupHowto][here]].

   The next step of init is to start up various daemons that support networking and other services. X server daemon is one of the most important daemons. It manages the display, keyboard, and mouse. When X server daemon is started you see a Graphical Interface and a login screen is displayed.

This article is contributed by **Saket Kumar**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [[http://www.write.geeksforgeeks.org/][write.geeksforgeeks.org]] or mail your article to review-team@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

*** Boot-block-in-operating-systems

# Basically for a computer to start running to get an instance when it is powered 
# up or rebooted it need to have an initial program to run. And this 
# initial program which is known as **bootstrap** needs to be
# simple. It must initialize all aspects of the system, from CPU 
# registers to device controllers and the contents of the main memory, and
# then starts the operating system.

**Why ROM:**
For most of today’s computer bootstrap is stored in Read Only Memory (ROM).

1. This location is good for storage because this place doesn’t require
initialization and moreover location here is fixed so that processor can start executing when powered up or reset.
2. ROM is basically read-only memory and hence it cannot be affected by the computer virus.

The
 problem is that changing the bootstrap code basically requires changes 
in the ROM hardware chips. Because of this reason, most system nowadays 
has the tiny bootstrap loader program in the boot whose only job is to 
bring the full bootstrap program from the disk. Through this now we are 
able to change the full bootstrap program easily and the new version can
 be easily written onto the disk.

The full bootstrap program is stored in the **boot blocks**
 at a fixed location on the disk. A disk that has a boot partition is 
called a boot disk. The code in the boot ROM basically instructs the 
read controller to read the boot blocks into the memory and then starts 
the execution of code. The full bootstrap program is more complex than 
the bootstrap loader in the boot ROM, It is basically able to load the 
complete OS from a non-fixed location on disk to start the operating 
system running. Even though the complete bootstrap program is very 
small.

**Example:**
Let us try to understand this using an example of the boot process in Windows 2000.

The
 Windows 2000 basically stores its boot code in the first sector on the 
hard disk. Moreover, Windows 2000 allows the hard disk to be divided 
into one or more partitions. This one partition is basically identified 
as the boot partition which basically contains the operating system and 
the device drivers.

Booting in Windows 2000 starts by running the
 code that is placed in the system’s ROM memory. This code directs the 
system to read code directly from MBR. In addition to this, boot code 
also contains the table which lists the partition for the hard disk and 
also a flag that indicates which partition is to be boot from the 
system. Once the system identifies the boot partition it reads the first
 sector from the memory which is known as a boot sector and continues 
the process with the remainder of the boot process which includes 
loading of various system services.

The following figure shows the Booting from disk in Windows 2000.

[[https://media.geeksforgeeks.org/wp-content/uploads/111-14.png][Booting from disk in Windows 2000]]

*** UEFI(Unified-Extensible-Firmware-Interface)-and-how-is-it-different-from-BIOS
**** Unified Extensible Firmware Interface (UEFI) vs. BIOS

***** Introduction
   The Unified Extensible Firmware Interface (UEFI), like BIOS (Basic Input Output System), is firmware that runs when the computer is booted. It initializes the hardware and loads the operating system into memory. However, being a more modern solution and overcoming various limitations of BIOS, UEFI is set to replace the former.

***** Outdated Nature of BIOS

****** Background
    BIOS has been present in all IBM PC-compatible personal computers since the late 1970s. Despite some improvements over the years, it hasn't kept pace with advancements in computer hardware and software technology.

****** Limitations of BIOS
    - Inability to boot from drives larger than 2 TB: With the widespread adoption of high-capacity storage devices, such as hard drives and SSDs, BIOS's limitation of not being able to boot from drives larger than 2 TB has become a significant drawback.
    - Runs in 16-bit processor mode with only 1 MB of space: The limited processing capability and memory space of BIOS hinder its ability to efficiently handle modern computing tasks.
    - Slow booting process due to inability to initialize multiple hardware devices simultaneously: BIOS's sequential initialization process leads to longer boot times, especially in systems with multiple hardware components.

***** Difference in Booting Process

****** Booting Process With BIOS
    - Power-On Self Test (POST) to check hardware functionality: BIOS begins its execution by performing a series of tests known as the POST to ensure that essential hardware components, such as the CPU, RAM, and storage devices, are functioning correctly.
    - Searches for Master Boot Record (MBR) in the first sector of the boot device: After completing the POST, BIOS locates the Master Boot Record (MBR) in the first sector of the selected boot device, which contains information about the boot loader.
    - Loads Boot Loader into RAM and then the operating system: Once the MBR is found, BIOS loads the boot loader program into the computer's RAM, which then loads the operating system into memory for execution.

****** Booting Process With UEFI
    - Scans EFI Service Partitions for valid boot volumes: Instead of relying on the Master Boot Record (MBR), UEFI maintains a list of valid boot volumes called EFI Service Partitions. During the Power-On Self Test (POST) procedure, the UEFI firmware scans all connected storage devices for a valid GUID Partition Table (GPT), which contains information about the EFI Service Partitions.
    - Utilizes GUID Partition Table (GPT) for improved booting: Unlike the traditional MBR, which has limitations in terms of partitioning and booting, GPT provides more flexibility and robustness, allowing for larger disk sizes and more efficient booting.
    - Directly loads the OS from the EFI Service Partition: Once a valid EFI Service Partition is found, UEFI directly loads the operating system from the designated partition, bypassing the need for a separate boot loader program.

***** Advantages of UEFI over BIOS

****** Breaking Out Of Size Limitations
    - UEFI can boot from drives larger than 2.2 TB, up to a theoretical limit of 9.4 zettabytes, due to GPT's use of 64-bit entries: This significantly expands the potential boot-device size, accommodating the growing storage capacities of modern storage devices.

****** Speed and Performance
    - UEFI can operate in 32-bit or 64-bit mode with more addressable address space, resulting in faster boot processes: By utilizing modern processor architectures and addressing modes, UEFI can achieve faster boot times compared to BIOS.

****** More User-Friendly Interface
    - Supports better UI configuration with improved graphics and mouse cursor support: UEFI firmware can provide a more visually appealing and intuitive user interface compared to the text-based interface of BIOS, making it easier for users to navigate system settings and configurations.

****** Security
    - UEFI's Secure Boot feature allows only authentic drivers and services to load at boot time, enhancing security against malware and piracy: Secure Boot ensures that only digitally signed and verified boot components are executed during the boot process, protecting the system from unauthorized modifications and malicious software.

***** Compatibility and Transition
    UEFI can operate alongside BIOS, supporting legacy boot and compatibility with older operating systems: This allows for a smooth transition to UEFI-based systems without immediately rendering legacy hardware and software obsolete. Intel plans to completely replace BIOS with UEFI for all its chipsets by 2020, marking a definitive shift towards modern firmware standards in PC computing.

** System-structure-of-operating-systems
*** Microkernel-in-operating-systems

*Kernel*
Kernel is the core part of an operating system that manages system resources. 
It also acts as a bridge between the application and hardware of the 
computer. It is one of the first programs loaded on start-up (after the 
Bootloader).

[[https://media.geeksforgeeks.org/wp-content/uploads/kernel.jpeg]]

**Kernel mode and User mode of CPU operation**
The
 CPU can execute certain instructions only when it is in kernel mode. 
These instructions are called privilege instruction. They allow the 
implementation of special operations whose execution by the user program
 could interface with the functioning of the operating system or 
activity of another user program. For example, instruction for managing 
memory protection. 

- The operating system puts the CPU in kernel mode when it is executing in the kernel so, that kernel can
execute some special operation.
- The operating system puts the
CPU in user mode when a user program is in execution so, that the user
program cannot interface with the operating system program.
- User-level instruction does not require special privilege. Example are ADD,PUSH,etc.

[[https://media.geeksforgeeks.org/wp-content/uploads/box-2-1.jpg]]

The
 concept of modes can be extended beyond two, requiring more than a 
single mode bit CPUs that support virtualization use one of these extra 
bits to indicate when the virtual machine manager, VMM, is in control of
 the system. The VMM has more privileges than ordinary user programs, 
but not so many as the full kernel.

System calls are typically 
implemented in the form of software interrupts, which causes the 
hardware’s interrupt handler to transfer control over to an appropriate 
interrupt handler, which is part of the operating system, switching the 
mode bit to kernel mode in the process. The interrupt handler checks 
exactly which interrupt was generated, checks additional parameters ( 
generally passed through registers ) if appropriate, and then calls the 
appropriate kernel service routine to handle the service requested by 
the system call.

User programs’ attempts to execute illegal 
instructions ( privileged or non-existent instructions ), or to access 
forbidden memory areas, also generate software interrupts, which are 
trapped by the interrupt handler, and control is transferred to the OS, 
which issues an appropriate error message, possibly dumps data to a log (
 core ) file for later analysis, and then terminates the offending 
program.

**What is Microkernel?**
A microkernel 
is one of the classifications of the kernel. Being a kernel it manages 
all system resources. But in a microkernel, the **user services** and **kernel services** are implemented in different address spaces. The user services are kept in **user address space**, and kernel services are kept under **kernel address space**, thus also reduces the size of kernel and size of an operating system as well.

[[https://media.geeksforgeeks.org/wp-content/uploads/Microkernel.jpeg]]

It
 provides minimal services of process and memory management. The 
communication between client program/application and services running in
 user address space is established through message passing, reducing the
 speed of execution microkernel. The Operating System **remains unaffected**
 as user services and kernel services are isolated so if any user 
service fails it does not affect kernel service. Thus it adds to one of 
the advantages of a microkernel. It is easily **extendible**
 i.e. if any new services are to be added they are added to user address
 space and hence require no modification in kernel space. It is also 
portable, secure, and reliable.

**Microkernel Architecture –**
Since
 the kernel is the core part of the operating system, so it is meant for
 handling the most important services only. Thus in this architecture, 
only the most important services are inside the kernel and the rest of 
the OS services are present inside the system application program. Thus 
users are able to interact with those not-so-important services within 
the system application. And the microkernel is solely responsible for 
the most important services of the operating system they are named as 
follows: 

- Inter process-Communication
- Memory Management
- CPU-Scheduling

**Advantages of Microkernel –**

- The architecture of this kernel is small and isolated hence it can function better.
- Expansion of the system is easier, it is simply added to the system application without disturbing the kernel.

*Read next* – [[https://www.geeksforgeeks.org/operating-system-monolithic-kernel-key-differences-microkernel/][Monolithic Kernel and key differences from Microkernel]]

*** Kernel-I/O-Subsystem-in-Operating-System

*Prerequisite* – [[https://www.geeksforgeeks.org/operating-system-microkernel/][Microkernel]]
The
 kernel provides many services related to I/O. Several services such as 
scheduling, caching, spooling, device reservation, and error handling – 
are provided by the kernel, s I/O subsystem built on the hardware and 
device-driver infrastructure. The I/O subsystem is also responsible for 
protecting itself from errant processes and malicious users.

1. **I/O Scheduling –**
To schedule a set of I/O requests means to determine a good order in which to execute them. The order in which the application issues the system
call is the best choice. Scheduling can improve the overall performance
of the system, can share device access permission fairly to all the
processes, reduce the average waiting time, response time, turnaround
time for I/O to complete.
    
    OS developers implement schedules by 
    maintaining a wait queue of the request for each device. When an 
    application issue a blocking I/O system call, The request is placed in 
    the queue for that device. The I/O scheduler rearranges the order to 
    improve the efficiency of the system. 
    
2. **Buffering –**
A *buffer* is a memory area that stores data being transferred between two devices or between a device and an application. Buffering is done for three
reasons.
    1. The first is to cope with a speed mismatch between producer and consumer of a data stream. 
    2. The second use of buffering is to provide adaptation for data that have different data-transfer sizes. 
    3. The third use of buffering is to support copy semantics for the application I/O, “copy semantic ” means, suppose that an application wants to write data on a disk that is stored in its buffer. it calls the **write()** system’s call, providing a pointer to the buffer and the integer specifying the number of bytes to write. 
    
3. **Caching –**
A *cache* is a region of fast memory that holds a copy of data. Access to the
cached copy is much easier than the original file. For instance, the
instruction of the currently running process is stored on the disk,
cached in physical memory, and copied again in the CPU’s secondary and
primary cache.
    
    The main difference between a buffer and a cache is 
    that a buffer may hold only the existing copy of a data item, while a 
    cache, by definition, holds a copy on faster storage of an item that 
    resides elsewhere.
    
4. **Spooling and Device Reservation –**
A *spool* is a buffer that holds the output of a device, such as a printer that
cannot accept interleaved data streams. Although a printer can serve
only one job at a time, several applications may wish to print their
output concurrently, without having their output mixes together.
    
    The 
    OS solves this problem by preventing all output from continuing to the 
    printer. The output of all applications is spooled in a separate disk 
    file. When an application finishes printing then the spooling system 
    queues the corresponding spool file for output to the printer.
    
5. **Error Handling –**
An Os that uses protected memory can guard against many kinds of hardware
and application errors so that a complete system failure is not the
usual result of each minor mechanical glitch, Devices, and I/O transfers can fail in many ways, either for transient reasons, as when a network
becomes overloaded or for permanent reasons, as when a disk controller
becomes defective.
6. **I/O Protection –**
Errors and the issue of protection are closely related. A user process may
attempt to issue illegal I/O instructions to disrupt the normal function of a system. We can use the various mechanisms to ensure that such
disruption cannot take place in the system.
    
    To prevent illegal I/O 
    access, we define all I/O instructions to be privileged instructions. 
    The user cannot issue I/O instruction directly.

*** Monolithic-Kernel-and-key-differences-from-Microkernel

Apart from microkernel, **Monolithic Kernel**
 is another classification of Kernel. Like microkernel, this one also 
manages system resources between application and hardware, but **user services** and **kernel services**
 are implemented under the same address space. It increases the size of 
the kernel, thus increases the size of the operating system as well.

This
 kernel provides CPU scheduling, memory management, file management, and
 other operating system functions through system calls. As both services
 are implemented under the same address space, this makes operating 
system execution faster.

Below is the diagrammatic representation of Monolithic Kernel:

![Monolithic Kernel](https://media.geeksforgeeks.org/wp-content/uploads/monolithic_kernel.jpeg)

If
 any service fails the entire system crashes, and it is one of the 
drawbacks of this kernel. The entire operating system needs modification
 if the user adds a new service.

**Advantages of Monolithic Kernel –**

- One of the major advantages of having a monolithic kernel is that it
provides CPU scheduling, memory management, file management, and other
operating system functions through system calls.
- The other one is that it is a single large process running entirely in a single address space.
- It is a single static binary file. Examples of some Monolithic Kernel-based OSs are Unix, Linux, Open VMS, XTS-400, z/TPF.

**Disadvantages of Monolithic Kernel –**

- One of the major disadvantages of a monolithic kernel is that if anyone service fails it leads to an entire system failure.
- If the user has to add any new service. The user needs to modify the entire operating system.

**Key differences between Monolithic Kernel and Microkernel –**

![Key differences](https://media.geeksforgeeks.org/wp-content/uploads/Difference.jpeg)

*** Introduction-to-system-call

In computing, a **system call**
 is the programmatic way in which a computer program requests a service 
from the kernel of the operating system it is executed on. A system call
 is a way for programs to **interact with the operating system**. A computer program makes a system call when it makes a request to the operating system’s kernel. System call **provides**
 the services of the operating system to the user programs via 
Application Program Interface(API). It provides an interface between a 
process and operating system to allow user-level processes to request 
services of the operating system. System calls are the only entry points
 into the kernel system. All programs needing resources must use system 
calls.

**Services Provided by System Calls :**

1. Process creation and management
2. Main memory management
3. File Access, Directory and File system management
4. Device handling(I/O)
5. Protection
6. Networking, etc.

**Types of System Calls :** There are 5 different categories of system calls –

1. **Process control:** end, abort, create, terminate, allocate and free memory.
2. **File management:** create, open, close, delete, read file etc.
3. Device management
4. Information maintenance
5. Communication

**Examples of Windows and Unix System Calls –**

WindowsUnixProcess ControlCreateProcess()ExitProcess()WaitForSingleObject()fork()exit()wait()File ManipulationCreateFile()ReadFile()WriteFile()CloseHandle()open()read()write()close()Device ManipulationSetConsoleMode()ReadConsole()WriteConsole()ioctl()read()write()Information MaintenanceGetCurrentProcessID()SetTimer()Sleep()getpid()alarm()sleep()CommunicationCreatePipe()CreateFileMapping()MapViewOfFile()pipe()shmget()mmap()ProtectionSetFileSecurity()InitlializeSecurityDescriptor()SetSecurityDescriptorGroup()chmod()umask()chown()

# Introduction of System Call

- Difficulty Level :
[Easy](https://www.geeksforgeeks.org/easy/)
- Last Updated :
16 Aug, 2019

In computing, a **system call**
 is the programmatic way in which a computer program requests a service 
from the kernel of the operating system it is executed on. A system call
 is a way for programs to **interact with the operating system**. A computer program makes a system call when it makes a request to the operating system’s kernel. System call **provides**
 the services of the operating system to the user programs via 
Application Program Interface(API). It provides an interface between a 
process and operating system to allow user-level processes to request 
services of the operating system. System calls are the only entry points
 into the kernel system. All programs needing resources must use system 
calls.

**Services Provided by System Calls :**

1. Process creation and management
2. Main memory management
3. File Access, Directory and File system management
4. Device handling(I/O)
5. Protection
6. Networking, etc.

**Types of System Calls :** There are 5 different categories of system calls –

1. **Process control:** end, abort, create, terminate, allocate and free memory.
2. **File management:** create, open, close, delete, read file etc.
3. Device management
4. Information maintenance
5. Communication

**Examples of Windows and Unix System Calls –**

WindowsUnixProcess ControlCreateProcess()ExitProcess()WaitForSingleObject()fork()exit()wait()File ManipulationCreateFile()ReadFile()WriteFile()CloseHandle()open()read()write()close()Device ManipulationSetConsoleMode()ReadConsole()WriteConsole()ioctl()read()write()Information MaintenanceGetCurrentProcessID()SetTimer()Sleep()getpid()alarm()sleep()CommunicationCreatePipe()CreateFileMapping()MapViewOfFile()pipe()shmget()mmap()ProtectionSetFileSecurity()InitlializeSecurityDescriptor()SetSecurityDescriptorGroup()chmod()umask()chown()

*** Resource Limits with getrlimit() and setrlimit()

The `getrlimit()` and `setrlimit()` system calls can be used to get and set resource limits such as files, CPU, memory, etc., associated with a process.

> Each resource has an associated soft and hard limit.
> 
> - *Soft limit:* The soft limit is the actual limit enforced by the kernel for the corresponding resource.
> - *Hard limit:* The hard limit acts as a ceiling for the soft limit.
> 
> **The soft limit ranges between 0 and the hard limit.**

The two limits are defined by the following structure:

#+BEGIN_SRC C
struct rlimit {
  rlim_t rlim_cur;  /* Soft limit */
  rlim_t rlim_max;  /* Hard limit (ceiling for rlim_cur) */
};
#+END_SRC

The signatures of the system calls are:

- `int getrlimit(int resource, struct rlimit *rlim);`
- `int setrlimit(int resource, const struct rlimit *rlim);`

**resource** refers to the resource limits you want to retrieve or modify. To **set** both the limits, set the values with the new values to the elements of the rlimit structure. To **get** both the limits, pass the address of rlim. A successful call to `getrlimit()` sets the rlimit elements to the limits. On **success**, both return **0**. On **error**, **-1** is returned, and errno is set appropriately.

Here is a program demonstrating the system calls by changing the value one greater than the maximum file descriptor number to 3:

#+BEGIN_SRC C
#include <stdio.h>
#include <sys/resource.h>
#include <string.h>
#include <errno.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int main() {
  struct rlimit old_lim, lim, new_lim;

  // Get old limits
  if (getrlimit(RLIMIT_NOFILE, &old_lim) == 0)
    printf("Old limits -> soft limit= %ld \t"
           " hard limit= %ld \n", old_lim.rlim_cur,
           old_lim.rlim_max);
  else
    fprintf(stderr, "%s\n", strerror(errno));

  // Set new value
  lim.rlim_cur = 3;
  lim.rlim_max = 1024;

  // Set limits
  if (setrlimit(RLIMIT_NOFILE, &lim) == -1)
    fprintf(stderr, "%s\n", strerror(errno));

  // Get new limits
  if (getrlimit(RLIMIT_NOFILE, &new_lim) == 0)
    printf("New limits -> soft limit= %ld "
           "\t hard limit= %ld \n", new_lim.rlim_cur,
           new_lim.rlim_max);
  else
    fprintf(stderr, "%s\n", strerror(errno));

  return 0;
}
#+END_SRC

Output:

```
Old limits -> soft limit= 1048576      hard limit= 1048576
New limits -> soft limit= 3              hard limit= 1024
```

The old limits values may vary depending upon the system.

Now, if you try to open a new file, it will show a runtime error because a maximum of 3 files can be opened, and they are already being opened by the system (STDIN, STDOUT, STDERR).

```org
#include <stdio.h>
#include <sys/resource.h>
#include <string.h>
#include <errno.h>
#include <unistd.h>
#include <sys/types.h>
#include <sys/stat.h>
#include <fcntl.h>

int main() {
  struct rlimit old_lim, lim, new_lim;

  // Get old limits
  if (getrlimit(RLIMIT_NOFILE, &old_lim) == 0)
    printf("Old limits -> soft limit= %ld \t"
           " hard limit= %ld \n", old_lim.rlim_cur,
           old_lim.rlim_max);
  else
    fprintf(stderr, "%s\n", strerror(errno));

  // Set new value
  lim.rlim_cur = 3;
  lim.rlim_max = 1024;

  // Set limits
  if (setrlimit(RLIMIT_NOFILE, &lim) == -1)
    fprintf(stderr, "%s\n", strerror(errno));

  // Get new limits
  if (getrlimit(RLIMIT_NOFILE, &new_lim) == 0)
    printf("New limits -> soft limit= %ld \t"
           " hard limit= %ld \n", new_lim.rlim_cur,
           new_lim.rlim_max);
  else
    fprintf(stderr, "%s\n", strerror(errno));

  // Try to open a new file
  if (open("foo.txt", O_WRONLY | O_CREAT, 0) == -1)
    fprintf(stderr, "%s\n", strerror(errno));
  else
    printf("Opened successfully\n");

  return 0;
}
```

Output:

```
Old limits -> soft limit= 1048576      hard limit= 1048576
New limits -> soft limit= 3                      hard limit= 1024
Too many open files
```

There is another system call **prlimit()** that combines both the system calls.

For more details, check the manual by typing `man 2 prlimit`.
``` 

This should make it easier to read and understand the content in a single file. Let me know if you need any further adjustments!

*** Dual-mode-in-Operating-Systems

An error in one program can adversely affect many processes; it might modify data of another program or also affect the operating system. For example, if a process gets stuck in an infinite loop, then this infinite loop could affect the correct operation of other processes. To ensure the proper execution of the operating system, there are two modes of operation:

**User mode –** When the computer system is run by user applications like creating a text document or using any application program, then the system is in user mode. When the user application requests a service from the operating system or an interrupt occurs or [system call](https://www.geeksforgeeks.org/operating-system-introduction-system-call/), then there will be a transition from user to kernel mode to fulfill the requests. **Note:** To switch from kernel mode to user mode, the mode bit should be 1.

Given below image describes what happens when an interrupt occurs:

[[https://media.geeksforgeeks.org/wp-content/uploads/dual_mode.jpeg]]

**Kernel Mode –** When the system boots, hardware starts in kernel mode and when the operating system is loaded, it starts user applications in user mode. To provide protection to the hardware, we have privileged instructions which execute only in kernel mode. If the user attempts to run privileged instruction in user mode, then it will treat instruction as illegal and trap to OS. Some of the privileged instructions are:

1. Handling Interrupts
2. To switch from user mode to kernel mode.
3. Input-Output management.

**Note:** To switch from user mode to kernel mode, the mode bit should be 0.

Read next – [User Level thread Vs Kernel Level thread](https://www.geeksforgeeks.org/operating-system-user-level-thread-vs-kernel-level-thread/)

*** Privileged-and-Non-Privileged-Instructions-in-Operating-System

In any Operating System, it is necessary to have a **[Dual Mode Operation](https://www.geeksforgeeks.org/dual-mode-operations-os/)** to ensure the protection and security of the System from unauthorized or errant users. This Dual Mode separates the User Mode from the System Mode or Kernel Mode.

![Dual Mode Operation](https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-9-1.jpg)

**What are Privileged Instructions?**

> The Instructions that can run only in Kernel Mode are called Privileged Instructions.

Privileged Instructions possess the following characteristics:

(i) If any attempt is made to execute a Privileged Instruction in User Mode, then it will not be executed and treated as an illegal instruction. The Hardware traps it in the Operating System.

(ii) Before transferring the control to any User Program, it is the responsibility of the Operating System to ensure that the **Timer** is set to interrupt. Thus, if the timer interrupts then the Operating System regains the control. Thus, any instruction which can modify the contents of the Timer is Privileged Instruction.

(iii) Privileged Instructions are used by the Operating System in order to achieve correct operation.

(iv) Various examples of Privileged Instructions include:

- I/O instructions and Halt instructions
- Turn off all Interrupts
- Set the Timer
- Context Switching
- Clear the Memory or Remove a process from the Memory
- Modify entries in the Device-status table

**What are Non-Privileged Instructions?**

> The Instructions that can run only in User Mode are called Non-Privileged Instructions.

Various examples of Non-Privileged Instructions include:

- Reading the status of Processor
- Reading the System Time
- Generate any Trap Instruction
- Sending the final printout of Printer

Also, it is important to note that in order to change the mode from Privileged to Non-Privileged, we require a Non-privileged Instruction that does not generate any interrupt.

** CPU-scheduling-in-operating-systems
*** Introduction of Process Management
**Program vs Process:** A
 process is a program in execution. For example, when we write a program
 in C or C++ and compile it, the compiler creates binary code. The 
original code and binary code are both programs. When we actually run 
the binary code, it becomes a process.

A
 process is an ‘active’ entity instead of a program, which is considered
 a ‘passive’ entity. A single program can create many processes when run
 multiple times; for example, when we open a .exe or binary file 
multiple times, multiple instances begin (multiple processes are 
created).

!https://videocdn.geeksforgeeks.org/geeksforgeeks/ProcessManagement/ProcessManagement20220704172047.jpg

**What does a process look like in memory?**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/process.png

***Text Section**:* A Process, sometimes known as the Text Section, also includes the current activity represented by the value of the

> Program Counter. Stack: The stack contains temporary data, such as function parameters, returns addresses, and local variables. Data Section: Contains the global variable. Heap Section: Dynamically allocated memory to process during its run time.
> 

Refer to [this](https://www.geeksforgeeks.org/memory-layout-of-c-program/) for more details on sections.

**Attributes or Characteristics of a Process:** A process has the following attributes.

```
1. Process Id:    A unique identifier assigned by the operating system
2. Process State: Can be ready, running, etc.
3. CPU registers: Like the Program Counter (CPU registers must be saved and
                  restored when a process is swapped in and out of CPU)
4. Accounts information: Amount of CPU used for process execution, time limits, execution ID etc
5. I/O status information: For example, devices allocated to the process,
                           open files, etc
6. CPU scheduling information: For example, Priority (Different processes
                               may have different priorities, for example
                               a shorter process assigned high priority
                               in the shortest job first scheduling)
```

All of the above attributes of a process are also known as the ***context of the process***. Every process has its own [process control block](http://en.wikipedia.org/wiki/Process_control_block)(PCB), i.e each process will have a unique PCB. All of the above attributes are part of the PCB.

**States of Process:** A process is in one of the following states:

```
1. New: Newly Created Process (or) being-created process.

2. Ready: After creation process moves to Ready state, i.e. the
          process is ready for execution.

3. Run: Currently running process in CPU (only one process at
        a time can be under execution in a single processor).

4. Wait (or Block): When a process requests I/O access.

5. Complete (or Terminated): The process completed its execution.

6. Suspended Ready: When the ready queue becomes full, some processes
                    are moved to suspended ready state

7. Suspended Block: When waiting queue becomes full.
```

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/process-states1.png

**Context Switching:** The
 process of saving the context of one process and loading the context of
 another process is known as Context Switching. In simple terms, it is 
like loading and unloading the process from the running state to the 
ready state.

**When does context switching happen?** 1. When a high-priority process comes to a ready state (i.e. with higher priority than the running process) 2. An Interrupt occurs 3. User and kernel-mode switch (It is not necessary though) 4. Preemptive CPU scheduling used.

**Context Switch vs Mode Switch:** A
 mode switch occurs when the CPU privilege level is changed, for example
 when a system call is made or a fault occurs. The kernel works in more a
 privileged mode than a standard user task. If a user process wants to 
access things that are only accessible to the kernel, a mode switch must
 occur. The currently executing process need not be changed during a 
mode switch.

A mode switch typically occurs for a process context switch to occur. Only the kernel can cause a context switch.

**CPU-Bound vs I/O-Bound Processes:** A CPU-bound process requires more CPU time or spends more time in the running state. An I/O-bound process requires more I/O time and less CPU time. An I/O-bound process spends more time in the waiting state.

**Exercise:**

**1.** Which of the following need not necessarily be saved on a context switch between processes? (GATE-CS-2000) (A) General purpose registers (B) Translation lookaside buffer (C) Program counter (D) All of the above

**Answer (B)**

**Explanation:** In
 a process context switch, the state of the first process must be saved 
somehow, so that when the scheduler gets back to the execution of the 
first process, it can restore this state and continue. The state of the 
process includes all the registers that the process may be using, 
especially the program counter, plus any other operating system-specific
 data that may be necessary. A translation look-aside buffer (TLB) is a 
CPU cache that memory management hardware uses to improve virtual 
address translation speed. A TLB has a fixed number of slots that 
contain page table entries, which map virtual addresses to physical 
addresses. On a context switch, some TLB entries can become invalid, 
since the virtual-to-physical mapping is different. The simplest 
strategy to deal with this is to completely flush the TLB.

**2.**
 The time taken to switch between user and kernel modes of execution is 
t1 while the time taken to switch between two processes is t2. Which of 
the following is TRUE? (GATE-CS-2011)

(A) t1 > t2 (B) t1 = t2 (C) t1 < t2 (D) nothing can be said about the relation between t1 and t2.

**Answer: (C)**

**Explanation:** Process switching involves a mode switch. Context switching can occur only in kernel mode.

[Quiz on Process Management](https://www.geeksforgeeks.org/operating-systems-gq/process-synchronization-gq/)

Please write comments if you find anything incorrect or if you want to share more information about the topic discussed above.
*** STATES OF A PROCESS IN OPERATING SYSTEMS

States of a process are as following:

!https://media.geeksforgeeks.org/wp-content/uploads/20190604122001/states_modified.png

- **New (Create) –** In this step, the process is about to be created but not yet created,
it is the program which is present in secondary memory that will be
picked up by OS to create the process.
- **Ready –**
New -> Ready to run. After the creation of a process, the process
enters the ready state i.e. the process is loaded into the main memory.
The process here is ready to run and is waiting to get the CPU time for
its execution. Processes that are ready for execution by the CPU are
maintained in a queue for ready processes.
- **Run –** The process is chosen by CPU for execution and the instructions within
the process are executed by any one of the available CPU cores.
- **Blocked or wait –** Whenever the process requests access to I/O or needs input from the
user or needs access to a critical region(the lock for which is already
acquired) it enters the blocked or wait state. The process continues to
wait in the main memory and does not require CPU. Once the I/O operation is completed the process goes to the ready state.
- **Terminated or completed –** Process is killed as well as PCB is deleted.
- **Suspend ready –** Process that was initially in the ready state but was swapped out of
main memory(refer Virtual Memory topic) and placed onto external storage by scheduler is said to be in suspend ready state. The process will
transition back to ready state whenever the process is again brought
onto the main memory.
- **Suspend wait or suspend blocked –** Similar to suspend ready but uses the process which was performing I/O
operation and lack of main memory caused them to move to secondary
memory. When work is finished it may go to suspend ready.

**CPU and I/O Bound Processes:**
 If the process is intensive in terms of CPU operations then it is 
called CPU bound process. Similarly, If the process is intensive in 
terms of I/O operations then it is called I/O bound process. **Types of schedulers:**

1. **Long term – performance –** Makes a decision about how many processes should be made to stay in the ready state, this decides the degree of multiprogramming. Once a
decision is taken it lasts for a long time hence called long term
scheduler.
2. **Short term – Context switching time –** Short term scheduler will decide which process to be executed next and
then it will call dispatcher. A dispatcher is a software that moves
process from ready to run and vice versa. In other words, it is context
switching.
3. **Medium term – Swapping time –**
Suspension decision is taken by medium term scheduler. Medium term
scheduler is used for swapping that is moving the process from main
memory to secondary and vice versa.

**Multiprogramming –** We have many processes ready to run. There are two types of multiprogramming:

1. **Pre-emption –** Process is forcefully removed from CPU. Pre-emption is also called as time sharing or multitasking.
2. **Non pre-emption –** Processes are not removed until they complete the execution.

**Degree of multiprogramming –**
 The number of processes that can reside in the ready state at maximum 
decides the degree of multiprogramming, e.g., if the degree of 
programming = 100, this means 100 processes can reside in the ready 
state at maximum.
*** Process Table and Process Control Block (PCB)

While
 creating a process the operating system performs several operations. To
 identify the processes, it assigns a process identification number 
(PID) to each process. As the operating system supports 
multi-programming, it needs to keep track of all the processes. For this
 task, the process control block (PCB) is used to track the process’s 
execution status. Each block of memory contains information about the 
process state, program counter, stack pointer, status of opened files, 
scheduling algorithms, etc. All these information is required and must 
be saved when the process is switched from one state to another. When 
the process makes a transition from one state to another, the operating 
system must update information in the process’s PCB.

A
 process control block (PCB) contains information about the process, 
i.e. registers, quantum, priority, etc. The process table is an array of
 PCB’s, that means logically contains a PCB for all of the current 
processes in the system.

!https://media.geeksforgeeks.org/wp-content/uploads/process-table.jpg

- **Pointer –** It is a stack pointer which is required to be saved when the process is switched from one state to another to retain the current position of
the process.
- **Process state –** It stores the respective state of the process.
- **Process number –** Every process is assigned with a unique id known as process ID or PID which stores the process identifier.
- **Program counter –** It stores the counter which contains the address of the next instruction that is to be executed for the process.
- **Register –** These are the CPU registers which includes: accumulator, base, registers and general purpose registers.
- **Memory limits –** This field contains the information about memory management system used by operating system. This may include the page tables, segment tables
etc.
- **Open files list –** This information includes the list of files opened for a process.

**Miscellaneous accounting and status data –** This field includes information about the amount of CPU used, time constraints, jobs or process number, etc.The
 process control block stores the register content also known as 
execution content of the processor when it was blocked from running. 
This execution content architecture enables the operating system to 
restore a process’s execution context when the process returns to the 
running state. When the process makes a transition from one state to 
another, the operating system updates its information in the process’s 
PCB. The operating system maintains pointers to each process’s PCB in a 
process table so that it can access the PCB quickly.

!https://media.geeksforgeeks.org/wp-content/uploads/process-control-block.jpg

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Process Schedulers in Operating System

The 
process scheduling is the activity of the process manager that handles 
the removal of the running process from the CPU and the selection of 
another process on the basis of a particular strategy.

Process
 scheduling is an essential part of a Multiprogramming operating 
systems. Such operating systems allow more than one process to be loaded
 into the executable memory at a time and the loaded process shares the 
CPU using time multiplexing.

!https://videocdn.geeksforgeeks.org/geeksforgeeks/OperatingSystemProcessScheduler/OperatingSystemProcessScheduler20220707152859.jpg

There are three types of process scheduler. 

1. **Long Term or job scheduler :** It brings the new process to the ‘Ready State’. It controls ***Degree of Multi-programming***, i.e., number of process present in ready state at any point of time. It is important that the long-term scheduler make a careful selection of
both I/O and CPU-bound processes. I/O bound tasks are which use much of
their time in input and output operations while CPU bound processes are
which spend their time on CPU. The job scheduler increases efficiency by maintaining a balance between the two.
2. **Short term or CPU scheduler :** It is responsible for selecting one process from ready state for
scheduling it on the running state. Note: Short-term scheduler only
selects the process to schedule it doesn’t load the process on running. Here is when all the scheduling algorithms are used. The CPU scheduler
is responsible for ensuring there is no starvation owing to high burst
time processes.***Dispatcher*** is responsible for loading the process selected by Short-term scheduler on the CPU (Ready
to Running State) Context switching is done by dispatcher only. A
dispatcher does the following:
    1. Switching context.
    2. Switching to user mode.
    3. Jumping to the proper location in the newly loaded program.
3. **Medium-term scheduler :** It is responsible for suspending and resuming the process. It mainly does
swapping (moving processes from main memory to disk and vice versa).
Swapping may be necessary to improve the process mix or because a change in memory requirements has overcommitted available memory, requiring
memory to be freed up. It is helpful in maintaining a perfect balance
between the I/O bound and the CPU bound. It reduces the degree of
multiprogramming.
*** CPU Scheduling in Operating Systems-breakdown
*** Measure the time spent in context switch?

A *Context switch*
 is a time spent between two processes (i.e., bringing a waiting process
 into execution and sending an executing process into a waiting for 
state). This happens in multitasking. The operating system must bring 
the state information if waiting for process into memory and save the 
state information of the currently running process.

In
 order to solve this problem, we would like to record the timestamps of 
the first and last instructions of the swapping processes. The context 
switch time is the difference between the two processes.

**Let’s take an example:** Assume there are only two processes, P1 and P2. P1
 is executing and P2 is waiting for execution. At some point, the 
operating system must swap P1 and P2, let’s assume it happens at the nth
 instruction of P1. If t(x, k) indicates the timestamp in microseconds 
of the kth instruction of process x, then the context switch would take 
t(2, 1) – t(1, n).

Another issue is that swapping is governed by 
the scheduling algorithm of the operating system and there may be many 
kernel-level threads that are also doing context switches. Other 
processes could be contending for the CPU or the kernel handling 
interrupts. The user does not have any control over these extraneous 
context switches. For instance, if at time t(1, n) the kernel decides to
 handle an interrupt, then the context switch time would be overstated.

In
 order to avoid these obstacles, we must construct an environment such 
that after P1 executes, the task scheduler immediately selects P2 to 
run. This may be accomplished by constructing a data channel, such as a 
pipe between P1 and P2.

That is, let’s allow P1 to be the initial
 sender and P2 to be the receiver. Initially, P2 is blocked(sleeping) as
 it awaits the data token. When P1 executes, it delivers the data token 
over the data channel to P2 and immediately attempts to read the 
response token. A context switch results and the task scheduler must 
select another process to run. Since P2 is now in a ready-to-run state, 
it is a desirable candidate to be selected by the task scheduler for 
execution. When P2 runs, the role of P1 and P2 are swapped. P2 is now 
acting as the sender and P1 as the blocked receiver.

**To summaries –**

1. P2 blocks awaiting data from P1
2. P1 marks the starting time.
3. P1 sends a token to P2.
4. P1 attempts to read a response token from P2. This induces a context switch.
5. P2 is scheduled and receives the token.
6. P2 sends a response token to P1.
7. P2 attempts to read a response token from P1. This induces a context switch.
8. P1 is scheduled and receives the token.
9. P1 marks the end time.

The
 key is that the delivery of a data token induces a context switch. Let 
Td and Tr be the time it takes to deliver and receive a data token, 
respectively, and let Tc be the amount of time spent in a context 
switch. At step 2, P1 records the timestamp of the delivery of the 
token, and at step 9, it records the timestamp of the response. The 
amount of time elapsed, T, between these events, may be expressed by: 

```
 T = 2 * (Td + Tc + Tr)
```

**This formula arises because of the following events:** 

- P1 sends the token (3)
- CPU context switches (4)
- P2 receives it (5)
- P2 then sends the response token (6)
- CPU context switches (7)
- and finally, P1 receives it (8)
*** Preemptive and Non-Preemptive Scheduling

Prerequisite – [CPU Scheduling](https://www.geeksforgeeks.org/gate-notes-operating-system-process-scheduling/)

**1. Preemptive Scheduling:** Preemptive
 scheduling is used when a process switches from running state to ready 
state or from the waiting state to ready state. The resources (mainly 
CPU cycles) are allocated to the process for a limited amount of time 
and then taken away, and the process is again placed back in the ready 
queue if that process still has CPU burst time remaining. That process 
stays in the ready queue till it gets its next chance to execute.

Algorithms based on preemptive scheduling are: [Round Robin (RR)](https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/),[Shortest Remaining Time First (SRTF)](https://www.geeksforgeeks.org/program-shortest-job-first-scheduling-set-2srtf-make-changesdoneplease-review/), [Priority (preemptive version)](https://www.geeksforgeeks.org/program-for-preemptive-priority-cpu-scheduling/), etc.

!https://media.geeksforgeeks.org/wp-content/uploads/pre-2.png

**2. Non-Preemptive Scheduling:** Non-preemptive
 Scheduling is used when a process terminates, or a process switches 
from running to the waiting state. In this scheduling, once the 
resources (CPU cycles) are allocated to a process, the process holds the
 CPU till it gets terminated or reaches a waiting state. In the case of 
non-preemptive scheduling does not interrupt a process running CPU in 
the middle of the execution. Instead, it waits till the process 
completes its CPU burst time, and then it can allocate the CPU to 
another process.

Algorithms based on non-preemptive scheduling are: [Shortest Job First (SJF basically non preemptive)](https://www.geeksforgeeks.org/program-shortest-job-first-sjf-scheduling-set-1-non-preemptive/) and [Priority (non preemptive version)](https://www.geeksforgeeks.org/operating-system-priority-scheduling-different-arrival-time-set-2/), etc.

!https://media.geeksforgeeks.org/wp-content/uploads/nonpre.png

**Key Differences Between Preemptive and Non-Preemptive Scheduling:**

1. In preemptive scheduling, the CPU is allocated to the processes for a
limited time whereas, in Non-preemptive scheduling, the CPU is allocated to the process till it terminates or switches to the waiting state.
2. The executing process in preemptive scheduling is interrupted in the middle of execution when higher priority one comes whereas, the executing
process in non-preemptive scheduling is not interrupted in the middle of execution and waits till its execution.
3. In Preemptive
Scheduling, there is the overhead of switching the process from the
ready state to running state, vise-verse and maintaining the ready
queue. Whereas in the case of non-preemptive scheduling has no overhead
of switching the process from running state to ready state.
4. In
preemptive scheduling, if a high-priority process frequently arrives in
the ready queue then the process with low priority has to wait for a
long, and it may have to starve. , in the non-preemptive scheduling, if
CPU is allocated to the process having a larger burst time then the
processes with small burst time may have to starve.
5. Preemptive
scheduling attains flexibility by allowing the critical processes to
access the CPU as they arrive into the ready queue, no matter what
process is executing currently. Non-preemptive scheduling is called
rigid as even if a critical process enters the ready queue the process
running CPU is not disturbed.
6. Preemptive Scheduling has to
maintain the integrity of shared data that’s why it is cost associative
which is not the case with Non-preemptive Scheduling.

**Comparison Chart:**

[](https://www.notion.so/beb67398db54435aa9e202677f7aefb0?pvs=21)
*** Difference between dispatcher and scheduler

**[Schedulers](https://www.geeksforgeeks.org/gate-notes-operating-system-scheduler/)**
 are special system software that handles process scheduling in various 
ways. Their main task is to select the jobs to be submitted into the 
system and to decide which process to run.

There are [three types of Scheduler](https://www.geeksforgeeks.org/gate-notes-operating-system-scheduler/):

1. **Long-term (job) scheduler –** Due to the smaller size of main memory initially all programs are
stored in secondary memory. When they are stored or loaded in the main
memory they are called processes. This is the decision of the long-term
scheduler that how many processes will stay in the ready queue. Hence,
in simple words, the long-term scheduler decides the degree of
multi-programming of the system.
2. **Medium-term scheduler –** Most often, a running process needs I/O operation which doesn’t require a CPU. Hence during the execution of a process when an I/O operation is required then the operating system sends that process from the running
queue to the blocked queue. When a process completes its I/O operation
then it should again be shifted to the ready queue. ALL these decisions
are taken by the medium-term scheduler. Medium-term scheduling is a part of **swapping**.
3. **Short-term (CPU) scheduler –** When there are lots of processes in main memory initially all are
present in the ready queue. Among all of the processes, a single process is to be selected for execution. This decision is handled by a
short-term scheduler. Let’s have a look at the figure given below. It
may make a more clear view for you.

!https://media.geeksforgeeks.org/wp-content/uploads/22_.png

**Dispatcher –**
 A dispatcher is a special program which comes into play after the 
scheduler. When the scheduler completes its job of selecting a process, 
it is the dispatcher which takes that process to the desired 
state/queue. The dispatcher is the module that gives a process control 
over the CPU after it has been selected by the short-term scheduler. 
This function involves the following:

- Switching context
- Switching to user mode
- Jumping to the proper location in the user program to restart that program

**The Difference between the Scheduler and Dispatcher –**

Consider a situation, where various processes are residing in the ready
 queue waiting to be executed. The CPU cannot execute all of these 
processes simultaneously, so the operating system has to choose a 
particular process on the basis of the scheduling algorithm used. So, 
this procedure of selecting a process among various processes is done by

**the scheduler**

. Once the scheduler has selected a process from the queue, the

**dispatcher**

comes into the picture, and it is the dispatcher who takes that process
 from the ready queue and moves it into the running state. Therefore, 
the scheduler gives the dispatcher an ordered list of processes which 
the dispatcher moves to the CPU over time.

**Example –**

There are 4 processes in the ready queue, P1, P2, P3, P4; Their arrival 
times are t0, t1, t2, t3 respectively. A First in First out (FIFO) 
scheduling algorithm is used. Because P1 arrived first, the scheduler 
will decide it is the first process that should be executed, and the 
dispatcher will remove P1 from the ready queue and give it to the CPU. 
The scheduler will then determine P2 to be the next process that should 
be executed, so when the dispatcher returns to the queue for a new 
process, it will take P2 and give it to the CPU. This continues in the 
same way for P3, and then P4.

!https://media.geeksforgeeks.org/wp-content/uploads/333.png

[](https://www.notion.so/91a45c3f27114af592c9323159e67aaf?pvs=21)
*** Program for FCFS CPU Scheduling

Given
 n processes with their burst times, the task is to find average waiting
 time and average turn around time using FCFS scheduling algorithm. First
 in, first out (FIFO), also known as first come, first served (FCFS), is
 the simplest scheduling algorithm. FIFO simply queues processes in the 
order that they arrive in the ready queue. In this, the process that
 comes first will be executed first and next process starts only after 
the previous gets fully executed. Here we are considering that arrival time for all processes is 0.**How to compute below times in Round Robin using a program?** 

1. Completion Time: Time at which process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time. Waiting Time = Turn Around Time – Burst Time

***In this post, we have assumed arrival times as 0, so turn around and completion times are same.***

!https://media.geeksforgeeks.org/wp-content/uploads/FCFS.png

**Implementation:** 

```
1-  Input the processes along with their burst time (bt).
2-  Find waiting time (wt) for all processes.
3-  As first process that comes need not to wait so
    waiting time for process 1 will be 0 i.e. wt[0] = 0.
4-  Findwaiting time for all other processes i.e. for
     process i ->
       wt[i] = bt[i-1] + wt[i-1] .
5-  Findturnaround time = waiting_time + burst_time
    for all processes.
6-  Findaverage waiting time =
                 total_waiting_time / no_of_processes.
7-  Similarly, findaverage turnaround time =
                 total_turn_around_time / no_of_processes.
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ program for implementation of FCFS`

`// scheduling`

`#include<iostream>`

`using` `namespace` `std;`

`// Function to find the waiting time for all`

`// processes`

`void` `findWaitingTime(int` `processes[], int` `n,`

`int` `bt[], int` `wt[])`

`{`

`// waiting time for first process is 0`

`wt[0] = 0;`

`// calculating waiting time`

`for` `(int`  `i = 1; i < n ; i++ )`

`wt[i] =  bt[i-1] + wt[i-1] ;`

`}`

`// Function to calculate turn around time`

`void` `findTurnAroundTime( int` `processes[], int` `n,`

`int` `bt[], int` `wt[], int` `tat[])`

`{`

`// calculating turnaround time by adding`

`// bt[i] + wt[i]`

`for` `(int`  `i = 0; i < n ; i++)`

`tat[i] = bt[i] + wt[i];`

`}`

`//Function to calculate average time`

`void` `findavgTime( int` `processes[], int` `n, int` `bt[])`

`{`

`int` `wt[n], tat[n], total_wt = 0, total_tat = 0;`

`//Function to find waiting time of all processes`

`findWaitingTime(processes, n, bt, wt);`

`//Function to find turn around time for all processes`

`findTurnAroundTime(processes, n, bt, wt, tat);`

`//Display processes along with all details`

`cout << "Processes  "<< " Burst time  "`

`<< " Waiting time  "` `<< " Turn around time\n";`

`// Calculate total waiting time and total turn`

`// around time`

`for` `(int`  `i=0; i<n; i++)`

`{`

`total_wt = total_wt + wt[i];`

`total_tat = total_tat + tat[i];`

`cout << "   "` `<< i+1 << "\t\t"` `<< bt[i] <<"\t    "`

`<< wt[i] <<"\t\t  "` `<< tat[i] <<endl;`

`}`

`cout << "Average waiting time = "`

`<< (float)total_wt / (float)n;`

`cout << "\nAverage turn around time = "`

`<< (float)total_tat / (float)n;`

`}`

`// Driver code`

`int` `main()`

`{`

`//process id's`

`int` `processes[] = { 1, 2, 3};`

`int` `n = sizeof` `processes / sizeof` `processes[0];`

`//Burst time of all processes`

`int`  `burst_time[] = {10, 5, 8};`

`findavgTime(processes, n,  burst_time);`

`return` `0;`

`}`

**Output:**

```
Processes  Burst time  Waiting time  Turn around time
 1        10     0         10
 2        5     10         15
 3        8     15         23
Average waiting time = 8.33333
Average turn around time = 16
```

**Important Points:** 

1. Non-preemptive
2. Average Waiting Time is not optimal
3. Cannot utilize resources in parallel : Results in Convoy effect (Consider a
situation when many IO bound processes are there and one CPU bound
process. The IO bound processes have to wait for CPU bound process when
CPU bound process acquires CPU. The IO bound process could have better
taken CPU for some time, then used IO devices).

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

*** Program for FCFS CPU Scheduling | Set 2 (Processes with different arrival times)

We have already discussed [FCFS Scheduling of processes with same arrival time](https://www.geeksforgeeks.org/program-fcfs-scheduling-set-1/).
 In this post, scenarios, when processes have different arrival times, 
are discussed. Given n processes with their burst times and arrival 
times, the task is to find average waiting time and an average turn 
around time using FCFS scheduling algorithm. FIFO simply queues 
processes in the order they arrive in the ready queue. Here, the process
 that comes first will be executed first and next process will start 
only after the previous gets fully executed.

1. Completion Time: Time at which the process completes its execution.
2. Turn Around Time: Time Difference between completion time and arrival time. Turn Around Time = Completion Time – Arrival Time
3. Waiting Time(W.T): Time Difference between turn around time and burst time. Waiting Time = Turn Around Time – Burst Time.

!https://media.geeksforgeeks.org/wp-content/uploads/os1.png

!https://media.geeksforgeeks.org/wp-content/uploads/ghant-chart.png

```
Process     Wait Time : Service Time - Arrival Time
   P0                        0 - 0   = 0
   P1                        5 - 1   = 4
   P2                        8 - 2   = 6
   P3                        16 - 3  = 13

Average Wait Time: (0 + 4 + 6 + 13) / 4 = 5.75
```

**Service Time:**
 Service time means amount of time after which a process can start 
execution. It is summation of burst time of previous processes 
(Processes that came before)

**Changes in code as compare to [code of FCFS with same arrival time](https://www.geeksforgeeks.org/program-fcfs-scheduling-set-1/):** To
 find waiting time: Time taken by all processes before the current 
process to be started (i.e. burst time of all previous processes) – 
arrival time of current process **wait_time[i] = (bt[0] + bt[1] +…… bt[i-1] ) – arrival_time[i]**

**Implementation:**

```
1- Input the processes along with their burst time(bt)
   and arrival time(at)
2- Find waiting time for all other processes i.e. for
   a given process  i:
       wt[i] = (bt[0] + bt[1] +...... bt[i-1]) - at[i]
3- Now findturn around time
          = waiting_time + burst_time for all processes
4-Average waiting time =
                    total_waiting_time / no_of_processes
5-Average turn around time =
                 total_turn_around_time / no_of_processes
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ program for implementation of FCFS`

`// scheduling with different arrival time`

`#include<iostream>`

`using` `namespace` `std;`

`// Function to find the waiting time for all`

`// processes`

`void` `findWaitingTime(int` `processes[], int` `n, int` `bt[],`

`int` `wt[], int` `at[])`

`{`

`int` `service_time[n];`

`service_time[0] = at[0];`

`wt[0] = 0;`

`// calculating waiting time`

`for` `(int` `i = 1; i < n ; i++)`

`{`

`// Add burst time of previous processes`

`service_time[i] = service_time[i-1] + bt[i-1];`

`// Find waiting time for current process =`

`// sum - at[i]`

`wt[i] = service_time[i] - at[i];`

`// If waiting time for a process is in negative`

`// that means it is already in the ready queue`

`// before CPU becomes idle so its waiting time is 0`

`if` `(wt[i] < 0)`

`wt[i] = 0;`

`}`

`}`

`// Function to calculate turn around time`

`void` `findTurnAroundTime(int` `processes[], int` `n, int` `bt[],`

`int` `wt[], int` `tat[])`

`{`

`// Calculating turnaround time by adding bt[i] + wt[i]`

`for` `(int` `i = 0; i < n ; i++)`

`tat[i] = bt[i] + wt[i];`

`}`

`// Function to calculate average waiting and turn-around`

`// times.`

`void` `findavgTime(int` `processes[], int` `n, int` `bt[], int` `at[])`

`{`

`int` `wt[n], tat[n];`

`// Function to find waiting time of all processes`

`findWaitingTime(processes, n, bt, wt, at);`

`// Function to find turn around time for all processes`

`findTurnAroundTime(processes, n, bt, wt, tat);`

`// Display processes along with all details`

`cout << "Processes "` `<< " Burst Time "` `<< " Arrival Time "`

`<< " Waiting Time "` `<< " Turn-Around Time "`

`<< " Completion Time \n";`

`int` `total_wt = 0, total_tat = 0;`

`for` `(int` `i = 0 ; i < n ; i++)`

`{`

`total_wt = total_wt + wt[i];`

`total_tat = total_tat + tat[i];`

`int` `compl_time = tat[i] + at[i];`

`cout << " "` `<< i+1 << "\t\t"` `<< bt[i] << "\t\t"`

`<< at[i] << "\t\t"` `<< wt[i] << "\t\t "`

`<< tat[i]  <<  "\t\t "` `<< compl_time << endl;`

`}`

`cout << "Average waiting time = "`

`<< (float)total_wt / (float)n;`

`cout << "\nAverage turn around time = "`

`<< (float)total_tat / (float)n;`

`}`

`// Driver code`

`int` `main()`

`{`

`// Process id's`

`int` `processes[] = {1, 2, 3};`

`int` `n = sizeof` `processes / sizeof` `processes[0];`

`// Burst time of all processes`

`int` `burst_time[] = {5, 9, 6};`

`// Arrival time of all processes`

`int` `arrival_time[] = {0, 3, 6};`

`findavgTime(processes, n, burst_time, arrival_time);`

`return` `0;`

`}`

**Output:**

```
Processes  Burst Time  Arrival Time  Waiting Time  Turn-Around Time  Completion Time
 1        5        0        0         5         5
 2        9        3        2         11         14
 3        6        6        8         14         20
Average waiting time = 3.33333
Average turn around time = 10.0
```
*** Convoy Effect in Operating Systems

**Prerequisites :** Basics of FCFS Scheduling ([Program for FCFS Scheduling | Set 1](https://www.geeksforgeeks.org/program-fcfs-scheduling-set-1/), [Program for FCFS Scheduling | Set 2](https://www.geeksforgeeks.org/program-fcfs-scheduling-set-2-processes-different-arrival-time/) )

Convoy
 Effect is phenomenon associated with the First Come First Serve (FCFS) 
algorithm, in which the whole Operating System slows down due to few 
slow processes.

Video Player is loading.

Remaining Time -0:00

1x

!https://media.geeksforgeeks.org/wp-content/uploads/222-2.png

FCFS
 algorithm is non-preemptive in nature, that is, once CPU time has been 
allocated to a process, other processes can get CPU time only after the 
current process has finished. This property of FCFS scheduling leads to 
the situation called Convoy Effect.

Suppose there is one CPU 
intensive (large burst time) process in the ready queue, and several 
other processes with relatively less burst times but are Input/Output 
(I/O) bound (Need I/O operations frequently).

Steps are as following below:

- The I/O bound processes are first allocated CPU time. As they are less CPU
intensive, they quickly get executed and goto I/O queues.
- Now, the CPU intensive process is allocated CPU time. As its burst time is high, it takes time to complete.
- While the CPU intensive process is being executed, the I/O bound processes
complete their I/O operations and are moved back to ready queue.
- However, the I/O bound processes are made to wait as the CPU intensive process still hasn’t finished. **This leads to I/O devices being idle.**
- When the CPU intensive process gets over, it is sent to the I/O queue so that it can access an I/O device.
- Meanwhile, the I/O bound processes get their required CPU time and move back to I/O queue.
- However, they are made to wait because the CPU intensive process is still accessing an I/O device. As a result, **the CPU is sitting idle now**.

Hence
 in Convoy Effect, one slow process slows down the performance of the 
entire set of processes, and leads to wastage of CPU time and other 
devices.

To avoid Convoy Effect, preemptive scheduling algorithms 
like Round Robin Scheduling can be used – as the smaller processes don’t
 have to wait much for CPU time – making their execution faster and 
leading to less resources sitting idle.

References –

- A. Silberschatz, P. Galvin, G. Gagne, “Operating Systems Concepts (8th Edition)”, Wiley India Pvt. Ltd.

This article is contributed by **[Sanchit Agarwal](http://www.facebook.com/sanchit3008)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Belady’s Anomaly in Page Replacement Algorithms

Prerequisite – [Page Replacement Algorithms](https://www.geeksforgeeks.org/operating-system-page-replacement-algorithm/) In
 Operating System, process data is loaded in fixed-sized chunks and each
 chunk is referred to as a page. The processor loads these pages in the 
fixed-sized chunks of memory called frames. Typically the size of each 
page is always equal to the frame size.

A
 page fault occurs when a page is not found in the memory and needs to 
be loaded from the disk. If a page fault occurs and all memory frames 
have been already allocated, then replacement of a page in memory is 
required on the request of a new page. This is referred to as 
demand-paging. The choice of which page to replace is specified by page 
replacement algorithms. The commonly used page replacement algorithms 
are FIFO, LRU, optimal page replacement algorithms, etc.

Generally,
 on increasing the number of frames to a process’ virtual memory, its 
execution becomes faster as fewer page faults occur. Sometimes the 
reverse happens, i.e. more page faults occur when more frames are 
allocated to a process. This most unexpected result is termed **Belady’s Anomaly**.

**Bélády’s anomaly**
 is the name given to the phenomenon where increasing the number of page
 frames results in an increase in the number of page faults for a given 
memory access pattern.

This phenomenon is commonly experienced in the following page replacement algorithms:

1. First in first out (FIFO)
2. Second chance algorithm
3. Random page replacement algorithm

**Reason for Belady’s Anomaly –** The
 other two commonly used page replacement algorithms are Optimal and 
LRU, but Belady’s Anomaly can never occur in these algorithms for any 
reference string as they belong to a class of stack-based page 
replacement algorithms.

A **stack-based algorithm** is one for which it can be shown that the set of pages in memory for *N* frames is always a subset of the set of pages that would be in memory with *N + 1* frames. For LRU replacement, the set of pages in memory would be the *n* most recently referenced pages. If the number of frames increases then these *n* pages will still be the most recently referenced and so, will still be in the memory. While in FIFO, if a page named *b* came into physical memory before a page – *a* then priority of replacement of *b* is greater than that of *a*,
 but this is not independent of the number of page frames and hence, 
FIFO does not follow a stack page replacement policy and therefore 
suffers Belady’s Anomaly.

**Example:** Consider the following diagram to understand the behavior of a stack-based page replacement algorithm

!https://media.geeksforgeeks.org/wp-content/uploads/stackbased.png

The
 diagram illustrates that given the set of pages i.e. {0, 1, 2} in 3 
frames of memory is not a subset of the pages in memory – {0, 1, 4, 5} 
with 4 frames and it is a violation in the property of stack based 
algorithms. This situation can be frequently seen in FIFO algorithm.

**Belady’s Anomaly in FIFO –** Assuming
 a system that has no pages loaded in the memory and uses the FIFO Page 
replacement algorithm. Consider the following reference string:

```
1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5
```

**Case-1:**
 If the system has 3 frames, the given reference string the using FIFO 
page replacement algorithm yields a total of 9 page faults. The diagram 
below illustrates the pattern of the page faults occurring in the 
example.

!https://media.geeksforgeeks.org/wp-content/uploads/fifo3.png

**Case-2:**
 If the system has 4 frames, the given reference string using the FIFO 
page replacement algorithm yields a total of 10 page faults. The diagram
 below illustrates the pattern of the page faults occurring in the 
example.

!https://media.geeksforgeeks.org/wp-content/uploads/fifo4.png

It
 can be seen from the above example that on increasing the number of 
frames while using the FIFO page replacement algorithm, the number of **page faults increased** from 9 to 10.

**Note –**
 It is not necessary that every string reference pattern cause Belady 
anomaly in FIFO but there is certain kind of string references that 
worsen the FIFO performance on increasing the number of frames.

**Why Stack based algorithms do not suffer Anomaly –** All
 the stack based algorithms never suffer Belady Anomaly because these 
type of algorithms assigns a priority to a page (for replacement) that 
is independent of the number of page frames. Examples of such policies 
are Optimal, LRU and LFU. Additionally these algorithms also have a good
 property for simulation, i.e. the miss (or hit) ratio can be computed 
for any number of page frames with a single pass through the reference 
string.

In LRU algorithm every time a page is referenced it is moved at the top of the stack, so, the top *n* pages of the stack are the *n* most recently used pages. Even if the number of frames is incremented to *n+1*, top of the stack will have *n+1* most recently used pages.

Similar
 example can be used to calculate the number of page faults in LRU 
algorithm. Assuming a system that has no pages loaded in the memory and 
uses the LRU Page replacement algorithm. Consider the following 
reference string:

```
1, 2, 3, 4, 1, 2, 5, 1, 2, 3, 4, 5
```

**Case-1:**
 If the system has 3 frames, the given reference string using the LRU 
page replacement algorithm yields a total of 10 page faults. The diagram
 below illustrates the pattern of the page faults occurring in the 
example.

!https://media.geeksforgeeks.org/wp-content/uploads/LRU33.png

**Case-2:**
 If the system has 4 frames, the given reference string on using LRU 
page replacement algorithm, then total 8 page faults occur. The diagram 
shows the pattern of the page faults in the example.

!https://media.geeksforgeeks.org/wp-content/uploads/LRU44-1.png

**Conclusion –** Various
 factors substantially affect the number of page faults, such as 
reference string length and the number of free page frames available. 
Anomalies also occur due to the small cache size as well as the reckless
 rate of change of the contents of the cache. Also, the situation of a 
fixed number of page faults even after increasing the number of frames 
can also be seen as an anomaly. Often algorithms like **Random page replacement algorithm** are also susceptible to Belady’s Anomaly, because it **may behave like first in first out (FIFO)**
 page replacement algorithm. But Stack based algorithms are generally 
immune to all such situations as they are guaranteed to give better page
 hits when the frames are incremented.
 
*** Program for Shortest Job First (or SJF) CPU Scheduling | Set 1 (Non- preemptive)

https://www.geeksforgeeks.org/program-for-shortest-job-first-or-sjf-cpu-scheduling-set-1-non-preemptive/
*** Shortest Remaining Time First (Preemptive SJF) Scheduling Algorithm
https://www.geeksforgeeks.org/shortest-remaining-time-first-preemptive-sjf-scheduling-algorithm/
*** Shortest Job First CPU Scheduling with predicted burst time

**Prerequisite –** [CPU Scheduling](https://www.geeksforgeeks.org/gate-notes-operating-system-process-scheduling/), [SJF – Set 1 (Non- preemptive)](https://www.geeksforgeeks.org/program-shortest-job-first-sjf-scheduling-set-1-non-preemptive/), [Set 2 (Preemptive)](https://www.geeksforgeeks.org/program-shortest-job-first-scheduling-set-2srtf-make-changesdoneplease-review/)

**Shortest Job First (SJF)**
 is an optimal scheduling algorithm as it gives maximum Throughput and 
minimum average waiting time(WT) and turn around time (TAT) but it is 
not practically implementable because Burst-Time of a process can’t be 
predicted in advance.

We may not know the length of the next CPU 
burst, but we may be able to predict its value. We expect the next CPU 
burst will be similar in length to the previous ones. By computing an 
approximation of the length of the next CPU burst, we can pick the 
process with the shortest predicted CPU burst.

There are two methods by which we can predict the burst time of the process :

**1. Static method –** We can predict the Burst-Time by two factors :

- **Process size –**Let say we have Process P having size 200 KB which is already executed and its Burst-time is 20 Units of time, now lets say we have a New Process P having size 201 KB which is yet to be executed.We take Burst-Time of already executed process P which is almost of same size as that of New process as Burst-Time of New Process P.
    
    old
    
    new
    
    old
    
    new
    
- **Process type –**We can predict Burst-Time depending on the Type of Process. Operating
System process(like scheduler, dispatcher, segmentation, fragmentation)
are faster than User process( Gaming, application softwares ).
Burst-Time for any New O.S process can be predicted from any old O.S
process of similar type and same for User process.

**Note –** Static method for burst time prediction is not reliable as it is always not predicted correctly.

**2. Dynamic method –** Let ti be the actual Burst-Time of ith process and Τn+1 be the predicted Burst-time for n+1th process.

- **Simple average –** Given n processes ( P, P… P)
    
    1
    
    2
    
    n
    
    ```
    Τn+1 = 1/n(Σi=1 to n ti)
    ```
    
- **Exponential average (Aging) –**
    
    ```
    Τn+1 = αtn + (1 - α)Τn
    ```
    
    where α = is smoothing factor and 0 <= α <= 1 ,
    
    tn = actual burst time of nth process,Τn = predicted burst time of nth process.
    
    General term,
    
    ```
    αtn + (1 - α)αtn-1 + (1 - α)2αtn-2...+ (1 - α)jαtn-j...+ (1 - α)n+1Τ0
    ```
    
    **Τ0** is a constant or overall system average.
    

**Smoothening factor (α) –** It controls the relative weight of recent and past history in our prediction.

- If α = 0, Τ = Τ i.e. no change in value of initial predicted burst time.
    
    n+1
    
    n
    
- If α = 1, Τ = t i.e. predicted Burst-Time of new process will always change according to actual Burst-time of n process.
    
    n+1
    
    n
    
    th
    
- If α = 1/2, recent and past history are equally weighted.

**Example –**Calculate the exponential averaging with T1 = 10, α = 0.5 and the algorithm is SJF with previous runs as 8, 7, 4, 16.(a) 9(b) 8(c) 7.5(d) None

**Explanation :**Initially T1 = 10 and α = 0.5 and the run times given are 8, 7, 4, 16 as it is shortest job first,So the possible order in which these processes would serve will be 4, 7, 8, 16 since SJF is a non-preemptive technique.So, using formula: T2 = α*t1 + (1-α)T1so we have,T2 = 0.5*4 + 0.5*10 = 7, here t1 = 4 and T1 = 10T3 = 0.5*7 + 0.5*7 = 7, here t2 = 7 and T2 = 7T4 = 0.5*8 + 0.5*7 = 7.5, here t3 = 8 and T3 = 7So the future prediction for 4th process will be T4 = 7.5 which is the option(c).

This article is contributed by **Yash Singla**.
 If you like GeeksforGeeks and would like to contribute, you can also 
write an article using contribute.geeksforgeeks.org or mail your article
 to contribute@geeksforgeeks.org. See your article appearing on the 
GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Longest Remaining Time First (LRTF) CPU Scheduling Program

Prerequisite – [CPU Scheduling | Longest Remaining Time First (LRTF) algorithm](https://www.geeksforgeeks.org/cpu-scheduling-longest-remaining-time-first-lrtf-algorithm/) We
 have given some process with arrival time and Burst Time and we have to
 find the completion time (CT), Turn Around Time(TAT), Average Turn 
Around Time (Avg TAT), Waiting Time(WT), Average Waiting Time (AWT) for 
the given processes.

**Example:** Consider the following table of arrival time and burst time for four processes P1, P2, P3 and P4.

```
Process   Arrival time   Burst Time
P1            1 ms          2 ms
P2            2 ms          4 ms
P3            3 ms          6 ms
p4            4 ms          8 ms
```

Gantt chart will be as following below,

!https://media.geeksforgeeks.org/wp-content/uploads/GANT.png

Since, completion time (CT) can be directly determined by Gantt chart, and

```
Turn Around Time (TAT)
= (Completion Time) - (Arrival Time)

Also, Waiting Time (WT)
= (Turn Around Time) - (Burst Time)
```

Therefore,

**Output:**

```
Total Turn Around Time = 68 ms
So, Average Turn Around Time = 68/4 = 17.00 ms

And, Total Waiting Time = 48 ms
So, Average Waiting Time = 12.00 ms
```

**Algorithm –**

- **Step-1:** Create a structure of process containing all necessary fields like AT
(Arrival Time), BT(Burst Time), CT(Completion Time), TAT(Turn Around
Time), WT(Waiting Time).
- **Step-2:** Sort according to the AT;
- **Step-3:** Find the process having Largest Burst Time and execute for each single
unit. Increase the total time by 1 and reduce the Burst Time of that
process with 1.
- **Step-4:** When any process have 0 BT left, then update the CT(Completion Time of that process CT will be Total Time at that time).
- **Step-2:** After calculating the CT for each process, find TAT and WT.

```
(TAT = CT - AT)
(WT  = TAT - BT)
```

**Implementation of Algorithm –**

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// creating a structure of a process`

`struct` `process {`

`int` `processno;`

`int` `AT;`

`int` `BT;`

`// for backup purpose to print in last`

`int` `BTbackup;`

`int` `WT;`

`int` `TAT;`

`int` `CT;`

`};`

`// creating a structure of 4 processes`

`struct` `process p[4];`

`// variable to find the total time`

`int` `totaltime = 0;`

`int` `prefinaltotal = 0;`

`// comparator function for sort()`

`bool` `compare(process p1, process p2)`

`{`

`// compare the Arrival time of two processes`

`return` `p1.AT < p2.AT;`

`}`

`// finding the largest Arrival Time among all the available`

v`// process at that time`

`int` `findlargest(int` `at)`

`{`

`int` `max = 0, i;`

`for` `(i = 0; i < 4; i++) {`

`if` `(p[i].AT <= at) {`

`if` `(p[i].BT > p[max].BT)`

`max = i;`

`}`

`}`

`// returning the index of the process having the largest BT`

`return` `max;`

`}`

`// function to find the completion time of each process`

`int` `findCT()`

`{`

`int` `index;`

`int` `flag = 0;`

`int` `i = p[0].AT;`

`while` `(1) {`

`if` `(i <= 4) {`

`index = findlargest(i);`

`}`

`else`

`index = findlargest(4);`

`cout << "Process executing at time "` `<< totaltime`

`<< " is: P"` `<< index + 1 << "\t";`

`p[index].BT -= 1;`

`totaltime += 1;`

`i++;`

`if` `(p[index].BT == 0) {`

`p[index].CT = totaltime;`

`cout << " Process P"` `<< p[index].processno`

`<< " is completed at "` `<< totaltime;`

`}`

`cout << endl;`

`// loop termination condition`

`if` `(totaltime == prefinaltotal)`

`break;`

`}`

`}`

`int` `main()`

`{`

`int` `i;`

`// initializing the process number`

`for` `(i = 0; i < 4; i++) {`

`p[i].processno = i + 1;`

`}`

`// cout<<"arrival time of 4 processes : ";`

`for` `(i = 0; i < 4; i++) // taking AT`

`{`

`p[i].AT = i + 1;`

`}`

`// cout<<" Burst time of 4 processes : ";`

`for` `(i = 0; i < 4; i++) {`

`// assigning {2, 4, 6, 8} as Burst Time to the processes`

`// backup for displaying the output in last`

`// calculating total required time for terminating`

`// the function().`

`p[i].BT = 2 * (i + 1);`

`p[i].BTbackup = p[i].BT;`

`prefinaltotal += p[i].BT;`

`}`

`// displaying the process before executing`

`cout << "PNo\tAT\tBT\n";`

`for` `(i = 0; i < 4; i++) {`

`cout << p[i].processno << "\t";`

`cout << p[i].AT << "\t";`

`cout << p[i].BT << "\t";`

`cout << endl;`

`}`

`cout << endl;`

`// sorting process according to Arrival Time`

`sort(p, p + 4, compare);`

`// calculating initial time when execution starts`

`totaltime += p[0].AT;`

`// calculating to terminate loop`

`prefinaltotal += p[0].AT;`

`findCT();`

`int` `totalWT = 0;`

`int` `totalTAT = 0;`

`for` `(i = 0; i < 4; i++) {`

`// since, TAT = CT - AT`

`p[i].TAT = p[i].CT - p[i].AT;`

`p[i].WT = p[i].TAT - p[i].BTbackup;`

`// finding total waiting time`

`totalWT += p[i].WT;`

`// finding total turn around time`

`totalTAT += p[i].TAT;`

`}`

`cout << "After execution of all processes ... \n";`

`// after all process executes`

`cout << "PNo\tAT\tBT\tCT\tTAT\tWT\n";`

`for` `(i = 0; i < 4; i++) {`

`cout << p[i].processno << "\t";`

`cout << p[i].AT << "\t";`

`cout << p[i].BTbackup << "\t";`

`cout << p[i].CT << "\t";`

`cout << p[i].TAT << "\t";`

`cout << p[i].WT << "\t";`

`cout << endl;`

`}`

`cout << endl;`

`cout << "Total TAT = "` `<< totalTAT << endl;`

`cout << "Average TAT = "` `<< totalTAT / 4.0 << endl;`

`cout << "Total WT = "` `<< totalWT << endl;`

`cout << "Average WT = "` `<< totalWT / 4.0 << endl;`

`return` `0;`

`}`

**Output:**

```
PNo    AT    BT
1    1    2
2    2    4
3    3    6
4    4    8

Process executing at time 1 is: P1
Process executing at time 2 is: P2
Process executing at time 3 is: P3
Process executing at time 4 is: P4
Process executing at time 5 is: P4
Process executing at time 6 is: P4
Process executing at time 7 is: P3
Process executing at time 8 is: P4
Process executing at time 9 is: P3
Process executing at time 10 is: P4
Process executing at time 11 is: P2
Process executing at time 12 is: P3
Process executing at time 13 is: P4
Process executing at time 14 is: P2
Process executing at time 15 is: P3
Process executing at time 16 is: P4
Process executing at time 17 is: P1     Process P1 is completed at 18
Process executing at time 18 is: P2     Process P2 is completed at 19
Process executing at time 19 is: P3     Process P3 is completed at 20
Process executing at time 20 is: P4     Process P4 is completed at 21
After execution of all processes ...
PNo    AT    BT    CT    TAT    WT
1    1    2    18    17    15
2    2    4    19    17    13
3    3    6    20    17    11
4    4    8    21    17    9

Total TAT = 68
Average TAT = 17
Total WT = 48
Average WT = 12
```

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Longest Remaining Time First (LRTF) or Preemptive Longest Job First CPU Scheduling Algorithm
https://www.geeksforgeeks.org/longest-remaining-time-first-lrtf-cpu-scheduling-algorithm/
*** Program for Round Robin scheduling
https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/
*** Selfish Round Robin CPU Scheduling
Prerequisite – [Program for Round Robin scheduling](https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/)

In the traditional Round Robin scheduling algorithm, all processes were treated equally for processing. The objective of the **Selfish Round Robin**
 is to give better service to processes that have been executing for a 
while than to newcomers. It’s a more logical and superior implementation
 compared to the normal Round Robin algorithm.

**Implementation:-** 

- Processes in the ready list are partitioned into two lists: NEW and ACCEPTED.
- The New processes wait while Accepted processes are serviced by the Round Robin.
- Priority of a new process increases at rate ‘a’ while the priority of an accepted process increases at rate ‘b’.
- When the priority of a new process reaches the priority of an accepted process, that new process becomes accepted.
- If all accepted processes finish, the highest priority new process is accepted.

Let’s trace out the general working of this algorithm:-

**STEP 1:** Assume
 that initially there are no ready processes, when the first one, A, 
arrives. It has priority 0, to begin with. Since there are no other 
accepted processes, A is accepted immediately. **STEP 2:** After
 a while, another process, B, arrives. As long as b / a < 1, B’s 
priority will eventually catch up to A’s, so it is accepted; now both A 
and B have the same priority. **STEP 3:** All accepted 
processes share a common priority (which rises at rate b ); that makes 
this policy easy to implement i.e any new process’s priority is bound to
 get accepted at some point. So no process has to experience 
starvation. **STEP 4:** Even if b / a > 1, A will eventually finish, and then B can be accepted.

```
Adjusting the parameters a and b :
          -> If b / a >= 1, a new process is not accepted
                 until all the accepted processes have finished, so SRR becomes FCFS.
          -> If b / a = 0, all processes are accepted immediately, so SRR becomes RR.
          -> If 0 < b / a < 1, accepted processes are selfish, but not completely.
```

**Example on Selfish Round Robin –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/1-10.jpg

**Solution (where a = 2 and b = 1) –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/2-12.png

**Explanation –**

Process
 A gets accepted as soon as it comes at time t = 0. So its priority is 
increased only by ‘b’ i.e ‘1’ after each second. B enters at time t = 1 
and goes to the waiting queue. So its priority gets increased by ‘a’ 
i.e. ‘2’ at time t = 2. At this point priority of A = priority of B = 
2.

So now both processes A & B are in the accepted queue and 
are executed in a round-robin fashion. At time t = 3 process C enters 
the waiting queue. At time t = 6 the priority of process C catches up to
 the priority of process B and then they start executing in a Round 
Robin manner. When B finishes execution at time t = 10, D is 
automatically promoted to the accepted queue.

Similarly, when D finishes execution at time t = 15, E is automatically promoted to the accepted queue.
*** Round Robin Scheduling with different arrival times
**Prerequisite:** [Round Robin Scheduling with arrival time as 0](https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/)

A round-robin **scheduling algorithm**
 is used to schedule the process fairly for each job a time slot or 
quantum and the interrupting the job if it is not completed by then the 
job come after the other job which is arrived in the quantum time that 
makes these scheduling fairly.

**Note:**

- Round-robin is cyclic in nature, so starvation doesn’t occur
- Round-robin is a variant of first come, first served scheduling
- No priority, special importance is given to any process or task
- RR scheduling is also known as Time slicing scheduling

**Advantages:**

- Each process is served by CPU for a fixed time, so priority is the same for each one
- Starvation does not occur because of its cyclic nature.

**Disadvantages:**

- Throughput depends on quantum time.
- If we want to give some process priority, we cannot.

[](https://www.notion.so/8cd1b308632b407cb56c09535992fed5?pvs=21)

**Quantum time is 2** this means each process is only executing for 2 units of time at a time.**How to compute these process requests:-**

1. Take the process which occurs first and start executing the process(for quantum time only).
2. Check if any other process request has arrived. If a process request arrives
during the quantum time in which another process is executing, then add
the new process to the Ready queue
3. After the quantum time has passed, check for any processes in the Ready
queue. If the ready queue is empty then continue the current process. If the queue not empty and the current process is not complete, then add
the current process to the end of the ready queue.
4. Take the first process from the Ready queue and start executing it (same rules)
5. Repeat all steps above from 2-4
6. If the process is complete and the ready queue is empty then the task is complete

After all these we get the three times which are:

1. **Completion Time:** the time taken for a process to complete.
2. **Turn Around Time:** total time the process exists in the system. (completion time – arrival time).
3. **Waiting Time:** total time waiting for their complete execution. (turn around time – burst time ).

**How to implement in a** **programming language**

```
1. Declare arrival[], burst[], wait[], turn[] arrays and initialize them. Also declare a timer
   variable and initialize it to zero. To sustain the original burst array create another
   array (temp_burst[]) and copy all the values of burst array in it.

2. To keep a check we create another array of bool type which keeps the record of whether a
   process is completed or not. we also need to maintain a queue array which contains the process
   indices (initially the array is filled with 0).

3. Now we increment the timer variable until the first process arrives and when it does, we add the
   process index to the queue array

4. Now we execute the first process until the time quanta and during that time quanta, we check
   whether any other process has arrived or not and if it has then we add the index in the queue
   (by calling the fxn. queueUpdation()).

5. Now, after doing the above steps if a process has finished, we store its exit time and
   execute the next process in the queue array. Else, we move the currently executed process at
   the end of the queue (by calling another fxn. queueMaintainence()) when the time slice expires.

6. The above steps are then repeated until all the processes have been completely executed. If a
   scenario arises where there are some processes left but they have not arrived yet, then we
   shall wait and the CPU will remain idle during this interval.
```

Below is the implementation of the above approach:

(For the sake of simplicity, we assume that the arrival times are entered in a sorted way) C++

`//C++ Program for implementing`

`//Round Robin Algorithm`

`//code by sparsh_cbs`

`#include <iostream>`

`using` `namespace` `std;`

`void` `queueUpdation(int` `queue[],int` `timer,int` `arrival[],int` `n, int` `maxProccessIndex){`

`int` `zeroIndex;`

`for(int` `i = 0; i < n; i++){`

`if(queue[i] == 0){`

`zeroIndex = i;`

`break;`

`}`

`}`

`queue[zeroIndex] = maxProccessIndex + 1;`

`}`

`void` `queueMaintainence(int` `queue[], int` `n){`

`for(int` `i = 0; (i < n-1) && (queue[i+1] != 0) ; i++){`

`int` `temp = queue[i];`

`queue[i] = queue[i+1];`

`queue[i+1] = temp;`

`}`

`}`

`void` `checkNewArrival(int` `timer, int` `arrival[], int` `n, int` `maxProccessIndex,int` `queue[]){`

`if(timer <= arrival[n-1]){`

`bool` `newArrival = false;`

`for(int` `j = (maxProccessIndex+1); j < n; j++){`

`if(arrival[j] <= timer){`

`if(maxProccessIndex < j){`

`maxProccessIndex = j;`

`newArrival = true;`

`}`

`}`

`}`

`//adds the incoming process to the ready queue`

`//(if any arrives)`

`if(newArrival)`

`queueUpdation(queue,timer,arrival,n, maxProccessIndex);`

`}`

`}`

`//Driver Code`

`int` `main(){`

`int` `n,tq, timer = 0, maxProccessIndex = 0;`

`float` `avgWait = 0, avgTT = 0;`

`cout << "\nEnter the time quanta : ";`

`cin>>tq;`

`cout << "\nEnter the number of processes : ";`

`cin>>n;`

`int` `arrival[n], burst[n], wait[n], turn[n], queue[n], temp_burst[n];`

`bool` `complete[n];`

`cout << "\nEnter the arrival time of the processes : ";`

`for(int` `i = 0; i < n; i++)`

`cin>>arrival[i];`

`cout << "\nEnter the burst time of the processes : ";`

`for(int` `i = 0; i < n; i++){`

`cin>>burst[i];`

`temp_burst[i] = burst[i];`

`}`

`for(int` `i = 0; i < n; i++){    //Initializing the queue and complete array`

`complete[i] = false;`

`queue[i] = 0;`

`}`

`while(timer < arrival[0])    //Incrementing Timer until the first process arrives`

`timer++;`

`queue[0] = 1;`

`while(true){`

`bool` `flag = true;`

`for(int` `i = 0; i < n; i++){`

`if(temp_burst[i] != 0){`

`flag = false;`

`break;`

`}`

`}`

`if(flag)`

`break;`

`for(int` `i = 0; (i < n) && (queue[i] != 0); i++){`

`int` `ctr = 0;`

`while((ctr < tq) && (temp_burst[queue[0]-1] > 0)){`

`temp_burst[queue[0]-1] -= 1;`

`timer += 1;`

`ctr++;`

`//Checking and Updating the ready queue until all the processes arrive`

`checkNewArrival(timer, arrival, n, maxProccessIndex, queue);`

`}`

`//If a process is completed then store its exit time`

`//and mark it as completed`

`if((temp_burst[queue[0]-1] == 0) && (complete[queue[0]-1] == false)){`

`//turn array currently stores the completion time`

`turn[queue[0]-1] = timer;`

`complete[queue[0]-1] = true;`

`}`

`//checks whether or not CPU is idle`

`bool` `idle = true;`

`if(queue[n-1] == 0){`

`for(int` `i = 0; i < n && queue[i] != 0; i++){`

`if(complete[queue[i]-1] == false){`

`idle = false;`

`}`

`}`

`}`

`else`

`idle = false;`

`if(idle){`

`timer++;`

`checkNewArrival(timer, arrival, n, maxProccessIndex, queue);`

`}`

`//Maintaining the entries of processes`

`//after each premption in the ready Queue`

`queueMaintainence(queue,n);`

`}`

`}`

`for(int` `i = 0; i < n; i++){`

`turn[i] = turn[i] - arrival[i];`

`wait[i] = turn[i] - burst[i];`

`}`

`cout << "\nProgram No.\tArrival Time\tBurst Time\tWait Time\tTurnAround Time"`

`<< endl;`

`for(int` `i = 0; i < n; i++){`

`cout<<i+1<<"\t\t"<<arrival[i]<<"\t\t"`

`<<burst[i]<<"\t\t"<<wait[i]<<"\t\t"<<turn[i]<<endl;`

`}`

`for(int` `i =0; i< n; i++){`

`avgWait += wait[i];`

`avgTT += turn[i];`

`}`

`cout<<"\nAverage wait time : "<<(avgWait/n)`

`<<"\nAverage Turn Around Time : "<<(avgTT/n);`

`return` `0;`

`}`

**Output:**

```
Enter the time quanta : 2

Enter the number of processes : 4

Enter the arrival time of the processes : 0 1 2 3

Enter the burst time of the processes : 5 4 2 1

Program No.     Arrival Time    Burst Time      Wait Time       TurnAround Time
1               0               5               7               12
2               1               4               6               10
3               2               2               2               4
4               3               1               5               6

Average wait time : 5
Average Turn Around Time : 8
```

In case of any queries or a problem with the code, please write it in the comment section.

**Note:** A slightly optimized version of the above-implemented code could be done by using Queue data structure as follows:

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`struct` `Process`

`{`

`int` `pid;`

`int` `arrivalTime;`

`int` `burstTime;`

`int` `burstTimeRemaining; // the amount of CPU time remaining after each execution`

`int` `completionTime;`

`int` `turnaroundTime;`

`int` `waitingTime;`

`bool` `isComplete;`

`bool` `inQueue;`

`};`

`/*`

`* At every time quantum or when a process has been executed before the time quantum,`

`* check for any new arrivals and push them into the queue`

- `/`

`void` `checkForNewArrivals(Process processes[], const` `int` `n, const` `int` `currentTime, queue<int> &readyQueue)`

`{`

`for` `(int` `i = 0; i < n; i++)`

`{`

`Process p = processes[i];`

`// checking if any processes has arrived`

`// if so, push them in the ready Queue.`

`if` `(p.arrivalTime <= currentTime && !p.inQueue && !p.isComplete)`

`{`

`processes[i].inQueue = true;`

`readyQueue.push(i);`

`}`

`}`

`}`

`/*`

`* Context switching takes place at every time quantum`

`* At every iteration, the burst time of the processes in the queue are handled using this method`

- `/`

`void` `updateQueue(Process processes[], const` `int` `n, const` `int` `quantum, queue<int> &readyQueue, int` `¤tTime, int` `&programsExecuted)`

`{`

`int` `i = readyQueue.front();`

`readyQueue.pop();`

`// if the process is going to be finished executing,`

`// ie, when it's remaining burst time is less than time quantum`

`// mark it completed and increment the current time`

`// and calculate its waiting time and turnaround time`

`if` `(processes[i].burstTimeRemaining <= quantum)`

`{`

`processes[i].isComplete = true;`

`currentTime += processes[i].burstTimeRemaining;`

`processes[i].completionTime = currentTime;`

`processes[i].waitingTime = processes[i].completionTime - processes[i].arrivalTime - processes[i].burstTime;`

`processes[i].turnaroundTime = processes[i].waitingTime + processes[i].burstTime;`

`if` `(processes[i].waitingTime < 0)`

`processes[i].waitingTime = 0;`

`processes[i].burstTimeRemaining = 0;`

`// if all the processes are not yet inserted in the queue,`

`// then check for new arrivals`

`if` `(programsExecuted != n)`

`{`

`checkForNewArrivals(processes, n, currentTime, readyQueue);`

`}`

`}`

`else`

`{`

`// the process is not done yet. But it's going to be pre-empted`

`// since one quantum is used`

`// but first subtract the time the process used so far`

`processes[i].burstTimeRemaining -= quantum;`

`currentTime += quantum;`

`// if all the processes are not yet inserted in the queue,`

`// then check for new arrivals`

`if` `(programsExecuted != n)`

`{`

`checkForNewArrivals(processes, n, currentTime, readyQueue);`

`}`

`// insert the incomplete process back into the queue`

`readyQueue.push(i);`

`}`

`}`

`/*`

`* Just a function that outputs the result in terms of their PID.`

- `/`

`void` `output(Process processes[], const` `int` `n)`

`{`

`double` `avgWaitingTime = 0;`

`double` `avgTurntaroundTime = 0;`

`// sort the processes array by processes.PID`

`sort(processes, processes + n, [](const` `Process &p1, const` `Process &p2)`

`{ return` `p1.pid < p2.pid; });`

`for` `(int` `i = 0; i < n; i++)`

`{`

`cout << "Process "` `<< processes[i].pid << ": Waiting Time: "` `<< processes[i].waitingTime << " Turnaround Time: "` `<< processes[i].turnaroundTime << endl;`

`avgWaitingTime += processes[i].waitingTime;`

`avgTurntaroundTime += processes[i].turnaroundTime;`

`}`

`cout << "Average Waiting Time: "` `<< avgWaitingTime / n << endl;`

`cout << "Average Turnaround Time: "` `<< avgTurntaroundTime / n << endl;`

`}`

`/*`

`* This function assumes that the processes are already sorted according to their arrival time`

`*/`

`void` `roundRobin(Process processes[], int` `n, int` `quantum)`

`{`

`queue<int> readyQueue;`

`readyQueue.push(0); // initially, pushing the first process which arrived first`

`processes[0].inQueue = true;`

`int` `currentTime = 0; // holds the current time after each process has been executed`

`int` `programsExecuted = 0; // holds the number of programs executed so far`

`while` `(!readyQueue.empty())`

`{`

`updateQueue(processes, n, quantum, readyQueue, currentTime, programsExecuted);`

`}`

`}`

`int` `main()`

`{`

`int` `n, quantum;`

`cout << "Enter the number of processes: ";`

`cin >> n;`

`cout << "Enter time quantum: ";`

`cin >> quantum;`

`Process processes[n + 1];`

`for` `(int` `i = 0; i < n; i++)`

`{`

`cout << "Enter arrival time and burst time of each process "` `<< i + 1 << ": ";`

`cin >> processes[i].arrivalTime;`

`cin >> processes[i].burstTime;`

`processes[i].burstTimeRemaining = processes[i].burstTime;`

`processes[i].pid = i + 1;`

`cout << endl;`

`}`

`// stl sort in terms of arrival time`

`sort(processes, processes + n, [](const` `Process &p1, const` `Process &p2)`

`{ return` `p1.arrivalTime < p2.arrivalTime; });`

`roundRobin(processes, n, quantum);`

`output(processes, n);`

`return` `0;`

`}`

```
Enter the number of processes: 4

Enter time quantum: 2

Enter the arrival time and burst time of each process:
0 5
1 4
2 2
3 1

Process 1: Waiting Time: 7 Turnaround Time: 12
Process 2: Waiting Time: 6 Turnaround Time: 10
Process 3: Waiting Time: 2 Turnaround Time: 4
Process 4: Waiting Time: 5 Turnaround Time: 6

Average Waiting Time: 5
Average Turnaround Time: 8
```
*** Program for Priority CPU Scheduling
Priority
 scheduling is one of the most common scheduling algorithms in batch 
systems. Each process is assigned a priority. Process with the highest 
priority is to be executed first and so on. Processes with the same 
priority are executed on first come first served basis. Priority can be 
decided based on memory requirements, time requirements or any other 
resource requirement.**Implementation :** 

```
1- First input the processes with their burst time
   and priority.
2- Sort the processes, burst time and priority
   according to the priority.
3- Now simply applyFCFS algorithm.
```

!https://media.geeksforgeeks.org/wp-content/uploads/PRIORITYsCHEDULING.jpg

Note:
 A major problem with priority scheduling is indefinite blocking or 
starvation. A solution to the problem of indefinite blockage of the 
low-priority process is aging. Aging is a technique of gradually 
increasing the priority of processes that wait in the system for a long 
period of time.

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ program for implementation of FCFS`

`// scheduling`

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`struct` `Process`

`{`

`int` `pid;  // Process ID`

`int` `bt;   // CPU Burst time required`

`int` `priority; // Priority of this process`

`};`

`// Function to sort the Process acc. to priority`

`bool` `comparison(Process a, Process b)`

`{`

`return` `(a.priority > b.priority);`

`}`

`// Function to find the waiting time for all`

`// processes`

`void` `findWaitingTime(Process proc[], int` `n,`

`int` `wt[])`

`{`

`// waiting time for first process is 0`

`wt[0] = 0;`

`// calculating waiting time`

`for` `(int`  `i = 1; i < n ; i++ )`

`wt[i] =  proc[i-1].bt + wt[i-1] ;`

`}`

`// Function to calculate turn around time`

`void` `findTurnAroundTime( Process proc[], int` `n,`

`int` `wt[], int` `tat[])`

`{`

`// calculating turnaround time by adding`

`// bt[i] + wt[i]`

`for` `(int`  `i = 0; i < n ; i++)`

`tat[i] = proc[i].bt + wt[i];`

`}`

`//Function to calculate average time`

`void` `findavgTime(Process proc[], int` `n)`

`{`

`int` `wt[n], tat[n], total_wt = 0, total_tat = 0;`

`//Function to find waiting time of all processes`

`findWaitingTime(proc, n, wt);`

`//Function to find turn around time for all processes`

`findTurnAroundTime(proc, n, wt, tat);`

`//Display processes along with all details`

`cout << "\nProcesses  "<< " Burst time  "`

`<< " Waiting time  "` `<< " Turn around time\n";`

`// Calculate total waiting time and total turn`

`// around time`

`for` `(int`  `i=0; i<n; i++)`

`{`

`total_wt = total_wt + wt[i];`

`total_tat = total_tat + tat[i];`

`cout << "   "` `<< proc[i].pid << "\t\t"`

`<< proc[i].bt << "\t    "` `<< wt[i]`

`<< "\t\t  "` `<< tat[i] <<endl;`

`}`

`cout << "\nAverage waiting time = "`

`<< (float)total_wt / (float)n;`

`cout << "\nAverage turn around time = "`

`<< (float)total_tat / (float)n;`

`}`

`void` `priorityScheduling(Process proc[], int` `n)`

`{`

`// Sort processes by priority`

`sort(proc, proc + n, comparison);`

`cout<< "Order in which processes gets executed \n";`

`for` `(int`  `i = 0 ; i <  n; i++)`

`cout << proc[i].pid <<" "` `;`

`findavgTime(proc, n);`

`}`

`// Driver code`

`int` `main()`

`{`

`Process proc[] = {{1, 10, 2}, {2, 5, 0}, {3, 8, 1}};`

`int` `n = sizeof` `proc / sizeof` `proc[0];`

`priorityScheduling(proc, n);`

`return` `0;`

`}`

**Output:** 

```
Order in which processes gets executed
1 3 2
Processes  Burst time  Waiting time  Turn around time
 1        10     0         10
 3        8     10         18
 2        5     18         23

Average waiting time = 9.33333
Average turn around time = 17
```

In this post, the processes with 
arrival time 0 are discussed. In next set, we will be considering 
different arrival times to evaluate waiting times.This article is contributed by **[Sahil Chhabra (akku)](https://www.facebook.com/sahil.chhabra.965)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Program for Preemptive Priority CPU Scheduling
Implementing priority CPU scheduling. In this problem, we are using [Min Heap](https://www.geeksforgeeks.org/binary-heap/) as the data structure for implementing priority scheduling. **In this problem smaller numbers denote higher priority.** The following functions are used in the given code below:

```
struct process {
   processID,
   burst time,
   response time,
   priority,
   arrival time.
}
```

**void quicksort(process array[], low, high)**– This function is used to arrange the processes in ascending order according to their arrival time. **int partition(process array[], int low, int high)**– This function is used to partition the array for sorting. **void insert(process Heap[], process value, int *heapsize, int *currentTime)**–
 It is used to include all the valid and eligible processes in the heap 
for execution. heapsize defines the number of processes in execution 
depending on the current time currentTime keeps record of the current 
CPU time. **void order(process Heap[], int *heapsize, int start)**– It is used to reorder the heap according to priority if the processes after insertion of new process. **void extractminimum(process Heap[], int *heapsize, int *currentTime)**–
 This function is used to find the process with highest priority from 
the heap. It also reorders the heap after extracting the highest 
priority process. **void scheduling(process Heap[], process array[], int n, int *heapsize, int *currentTime)**– This function is responsible for executing the highest priority extracted from Heap[]. **void process(process array[], int n)**–
 This function is responsible for managing the entire execution of the 
processes as they arrive in the CPU according to their arrival time.

`// CPP program to implement preemptive priority scheduling`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`struct` `Process {`

`int` `processID;`

`int` `burstTime;`

`int` `tempburstTime;`

`int` `responsetime;`

`int` `arrivalTime;`

`int` `priority;`

`int` `outtime;`

`int` `intime;`

`};`

`// It is used to include all the valid and eligible`

`// processes in the heap for execution. heapsize defines`

`// the number of processes in execution depending on`

`// the current time currentTime keeps a record of`

`// the current CPU time.`

`void` `insert(Process Heap[], Process value, int* heapsize,`

`int* currentTime)`

`{`

`int` `start = *heapsize, i;`

`Heap[*heapsize] = value;`

`if` `(Heap[*heapsize].intime == -1)`

`Heap[*heapsize].intime = *currentTime;`

`++(*heapsize);`

`// Ordering the Heap`

`while` `(start != 0`

`&& Heap[(start - 1) / 2].priority`

`> Heap[start].priority) {`

`Process temp = Heap[(start - 1) / 2];`

`Heap[(start - 1) / 2] = Heap[start];`

`Heap[start] = temp;`

`start = (start - 1) / 2;`

`}`

`}`

`// It is used to reorder the heap according to`

`// priority if the processes after insertion`

`// of new process.`

`void` `order(Process Heap[], int* heapsize, int` `start)`

`{`

`int` `smallest = start;`

`int` `left = 2 * start + 1;`

`int` `right = 2 * start + 2;`

`if` `(left < *heapsize`

`&& Heap[left].priority`

`< Heap[smallest].priority)`

`smallest = left;`

`if` `(right < *heapsize`

`&& Heap[right].priority`

`< Heap[smallest].priority)`

`smallest = right;`

`// Ordering the Heap`

`if` `(smallest != start) {`

`Process temp = Heap[smallest];`

`Heap[smallest] = Heap[start];`

`Heap[start] = temp;`

`order(Heap, heapsize, smallest);`

`}`

`}`

`// This function is used to find the process with`

`// highest priority from the heap. It also reorders`

`// the heap after extracting the highest priority process.`

`Process extractminimum(Process Heap[], int* heapsize,`

`int* currentTime)`

`{`

`Process min = Heap[0];`

`if` `(min.responsetime == -1)`

`min.responsetime`

`= *currentTime - min.arrivalTime;`

`--(*heapsize);`

`if` `(*heapsize >= 1) {`

`Heap[0] = Heap[*heapsize];`

`order(Heap, heapsize, 0);`

`}`

`return` `min;`

`}`

`// Compares two intervals`

`// according to starting times.`

`bool` `compare(Process p1, Process p2)`

`{`

`return` `(p1.arrivalTime < p2.arrivalTime);`

`}`

`// This function is responsible for executing`

`// the highest priority extracted from Heap[].`

`void` `scheduling(Process Heap[], Process array[], int` `n,`

`int* heapsize, int* currentTime)`

`{`

`if` `(heapsize == 0)`

`return;`

`Process min = extractminimum(`

`Heap, heapsize, currentTime);`

`min.outtime = *currentTime + 1;`

`--min.burstTime;`

`printf("process id = %d current time = %d\n",`

`min.processID, *currentTime);`

`// If the process is not yet finished`

`// insert it back into the Heap*/`

`if` `(min.burstTime > 0) {`

`insert(Heap, min, heapsize, currentTime);`

`return;`

`}`

`for` `(int` `i = 0; i < n; i++)`

`if` `(array[i].processID == min.processID) {`

`array[i] = min;`

`break;`

`}`

`}`

`// This function is responsible for`

`// managing the entire execution of the`

`// processes as they arrive in the CPU`

`// according to their arrival time.`

`void` `priority(Process array[], int` `n)`

`{`

`sort(array, array + n, compare);`

`int` `totalwaitingtime = 0, totalbursttime = 0,`

`totalturnaroundtime = 0, i, insertedprocess = 0,`

`heapsize = 0, currentTime = array[0].arrivalTime,`

`totalresponsetime = 0;`

`Process Heap[4 * n];`

`// Calculating the total burst time`

`// of the processes`

`for` `(int` `i = 0; i < n; i++) {`

`totalbursttime += array[i].burstTime;`

`array[i].tempburstTime = array[i].burstTime;`

`}`

`// Inserting the processes in Heap`

`// according to arrival time`

`do` `{`

`if` `(insertedprocess != n) {`

`for` `(i = 0; i < n; i++) {`

`if` `(array[i].arrivalTime == currentTime) {`

`++insertedprocess;`

`array[i].intime = -1;`

`array[i].responsetime = -1;`

`insert(Heap, array[i],`

`&heapsize, ¤tTime);`

`}`

`}`

`}`

`scheduling(Heap, array, n,`

`&heapsize, ¤tTime);`

`++currentTime;`

`if` `(heapsize == 0`

`&& insertedprocess == n)`

`break;`

`} while` `(1);`

`for` `(int` `i = 0; i < n; i++) {`

`totalresponsetime`

`+= array[i].responsetime;`

`totalwaitingtime`

`+= (array[i].outtime`

`- array[i].intime`

`- array[i].tempburstTime);`

`totalbursttime += array[i].burstTime;`

`}`

`printf("Average waiting time = %f\n",`

`((float)totalwaitingtime / (float)n));`

`printf("Average response time =%f\n",`

`((float)totalresponsetime / (float)n));`

`printf("Average turn around time = %f\n",`

`((float)(totalwaitingtime`

`+ totalbursttime)`

`/ (float)n));`

`}`

`// Driver code`

`int` `main()`

`{`

`int` `n, i;`

`Process a[5];`

`a[0].processID = 1;`

`a[0].arrivalTime = 4;`

`a[0].priority = 2;`

`a[0].burstTime = 6;`

`a[1].processID = 4;`

`a[1].arrivalTime = 5;`

`a[1].priority = 1;`

`a[1].burstTime = 3;`

`a[2].processID = 2;`

`a[2].arrivalTime = 5;`

`a[2].priority = 3;`

`a[2].burstTime = 1;`

`a[3].processID = 3;`

`a[3].arrivalTime = 1;`

`a[3].priority = 4;`

`a[3].burstTime = 2;`

`a[4].processID = 5;`

`a[4].arrivalTime = 3;`

`a[4].priority = 5;`

`a[4].burstTime = 4;`

`priority(a, 5);`

`return` `0;`

`}`

**Output:**

`process id = 3 current time = 1
process id = 3 current time = 2
process id = 5 current time = 3
process id = 1 current time = 4
process id = 4 current time = 5
process id = 4 current time = 6
process id = 4 current time = 7
process id = 1 current time = 8
process id = 1 current time = 9
process id = 1 current time = 10
process id = 1 current time = 11
process id = 1 current time = 12
process id = 2 current time = 13
process id = 5 current time = 14
process id = 5 current time = 15
process id = 5 current time = 16
Average waiting time = 4.200000
Average response time =1.600000
Average turn around time = 7.400000`

The output displays the order in which the processes are 
executed in the memory and also shows the average waiting time, average 
response time and average turn around time for each process.This article is contributed by **[Hardik Gaur](https://www.linkedin.com/in/hardik-gaur-135891122/)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Priority CPU Scheduling with different arrival time
**Prerequisite –** [Program for Priority Scheduling – Set 1](https://www.geeksforgeeks.org/program-priority-scheduling-set-1/)Priority
 scheduling is a non-preemptive algorithm and one of the most common 
scheduling algorithms in batch systems. Each process is assigned first 
arrival time (less arrival time process first) if two processes have 
same arrival time, then compare to priorities (highest process first). 
Also, if two processes have same priority then compare to process number
 (less process number first). This process is repeated while all process
 get executed.

**Implementation –**

1. First input the processes with their arrival time, burst time and priority.
2. First process will schedule, which have the lowest arrival time, if two or
more processes will have lowest arrival time, then whoever has higher
priority will schedule first.
3. Now further processes will be
schedule according to the arrival time and priority of the process.
(Here we are assuming that lower the priority number having higher
priority). If two process priority are same then sort according to process number.**Note:** In the question, They will clearly mention, which number will have higher priority and which number will have lower priority.
4. Once all the processes have been arrived, we can schedule them based on their priority.

!https://media.geeksforgeeks.org/wp-content/uploads/opSystemScheduling.png

**Gantt Chart –**

!https://media.geeksforgeeks.org/wp-content/uploads/20210420124146/priorityscheduling.jpg

**Examples –**

```
Input :
process no-> 1 2 3 4 5
arrival time-> 0 1 3 2 4
burst time-> 3 6 1 2 4
priority-> 3 4 9 7 8
Output :
Process_no   arrival_time   Burst_time   Complete_time    Turn_Around_Time       Waiting_Time
1             0               3                3                   3               0
2             1               6                9                   8               2
3             3               1                16                  13              12
4             2               2                11                  9               7
5             4               4                15                  11              7
Average Waiting Time is : 5.6
Average Turn Around time is : 8.8
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ implementation for Priority Scheduling with`

`//Different Arrival Time priority scheduling`

`/*1. sort the processes according to arrival time`

`2. if arrival time is same the acc to priority`

`3. apply fcfs`

- `/`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`#define totalprocess 5`

`// Making a struct to hold the given input`

`struct` `process`

`{`

`int` `at,bt,pr,pno;`

`};`

`process proc[50];`

`/*`

`Writing comparator function to sort according to priority if`

`arrival time is same`

- `/`

`bool` `comp(process a,process b)`

`{`

`if(a.at == b.at)`

`{`

`return` `a.pr<b.pr;`

`}`

`else`

`{`

`return` `a.at<b.at;`

`}`

`}`

`// Using FCFS Algorithm to find Waiting time`

`void` `get_wt_time(int` `wt[])`

`{`

`// declaring service array that stores cumulative burst time`

`int` `service[50];`

`// Initialising initial elements of the arrays`

`service[0] = proc[0].at;`

`wt[0]=0;`

`for(int` `i=1;i<totalprocess;i++)`

`{`

`service[i]=proc[i-1].bt+service[i-1];`

`wt[i]=service[i]-proc[i].at;`

`// If waiting time is negative, change it into zero`

`if(wt[i]<0)`

`{`

`wt[i]=0;`

`}`

`}`

`}`

`void` `get_tat_time(int` `tat[],int` `wt[])`

`{`

`// Filling turnaroundtime array`

`for(int` `i=0;i<totalprocess;i++)`

`{`

`tat[i]=proc[i].bt+wt[i];`

`}`

`}`

`void` `findgc()`

`{`

`//Declare waiting time and turnaround time array`

`int` `wt[50],tat[50];`

`double` `wavg=0,tavg=0;`

`// Function call to find waiting time array`

`get_wt_time(wt);`

`//Function call to find turnaround time`

`get_tat_time(tat,wt);`

`int` `stime[50],ctime[50];`

`stime[0] = proc[0].at;`

`ctime[0]=stime[0]+tat[0];`

`// calculating starting and ending time`

`for(int` `i=1;i<totalprocess;i++)`

`{`

`stime[i]=ctime[i-1];`

`ctime[i]=stime[i]+tat[i]-wt[i];`

`}`

`cout<<"Process_no\tStart_time\tComplete_time\tTurn_Around_Time\tWaiting_Time"<<endl;`

`// display the process details`

`for(int` `i=0;i<totalprocess;i++)`

`{`

`wavg += wt[i];`

`tavg += tat[i];`

`cout<<proc[i].pno<<"\t\t"<<`

`stime[i]<<"\t\t"<<ctime[i]<<"\t\t"<<`

`tat[i]<<"\t\t\t"<<wt[i]<<endl;`

`}`

`// display the average waiting time`

`//and average turn around time`

`cout<<"Average waiting time is : ";`

`cout<<wavg/(float)totalprocess<<endl;`

`cout<<"average turnaround time : ";`

`cout<<tavg/(float)totalprocess<<endl;`

`}`

`int` `main()`

`{`

`int` `arrivaltime[] = { 1, 2, 3, 4, 5 };`

`int` `bursttime[] = { 3, 5, 1, 7, 4 };`

`int` `priority[] = { 3, 4, 1, 7, 8 };`

`for(int` `i=0;i<totalprocess;i++)`

`{`

`proc[i].at=arrivaltime[i];`

`proc[i].bt=bursttime[i];`

`proc[i].pr=priority[i];`

`proc[i].pno=i+1;`

`}`

`//Using inbuilt sort function`

`sort(proc,proc+totalprocess,comp);`

`//Calling function findgc for finding Gantt Chart`

`findgc();`

`return` `0;`

`}`

`// This code is contributed by Anukul Chand.`

**Output:**

```
Process_no Start_time Complete_time Turn_Around_Time Waiting_Time
1           1           4              3              0
2           5           10             8              3
3           4           5              2              1
4          10           17             13             6
5          17           21             16             12
Average Waiting Time is : 4.4
Average Turn Around time is : 8.4
```

**Time Complexity:** O(N * logN), where N is the total number of processes. **Auxiliary Space:** O(N)

This article is contributed by **[Amit Verma](https://auth.geeksforgeeks.org/profile.php?user=amit%20verma%204&list=practice)** . If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Starvation and Aging in Operating Systems
We have already discussed priority scheduling in [this](https://www.geeksforgeeks.org/program-priority-scheduling-set-1/)
 post. It is one of the most common scheduling algorithms in batch 
systems. Each process is assigned a priority. The process with the 
highest priority is to be executed first and so on. Here we will be 
discussing a major problem related to priority scheduling and its 
solution.

> Prerequisites: Priority Scheduling
> 

**Starvation**
 or indefinite blocking is a phenomenon associated with the Priority 
scheduling algorithms, in which a process ready for the CPU (resources) 
can wait to run indefinitely because of low priority. In a heavily 
loaded computer system, a steady stream of higher-priority processes can
 prevent a low-priority process from ever getting the CPU. There have 
been rumors that in 1967 Priority Scheduling was used in IBM 7094 at 
MIT, and they found a low-priority process that had not been submitted 
till 1973.

!https://media.geeksforgeeks.org/wp-content/uploads/starvationAndAging.jpg

As
 we see in the above example process having higher priority than other 
processes getting CPU earlier. We can think of a scenario in which only 
one process is having very low-priority (for example 127) and we are 
giving other process with high-priority, this can lead indefinitely 
waiting for the process for CPU which is having low-priority, this leads
 to **Starvation**. Further we have also discuss about the solution of starvation.

Differences between [Deadlock](https://www.geeksforgeeks.org/operating-system-process-management-deadlock-introduction/) and Starvation in OS are as follows:****

1. Deadlock occurs when none of the processes in the set is able to move ahead due
to occupancy of the required resources by some other process as shown in the figure below, on the other hand, Starvation occurs when a process
waits for an indefinite period of time to get the resource it requires.
2. Another name for deadlock is **Circular Waiting**. Another name for starvation is **Lived lock**.
3. When deadlock occurs no process can make progress, while in starvation apart from the victim process other processes can progress or proceed.

**Solution to Starvation:** Aging

Aging
 is a technique of gradually increasing the priority of processes that 
wait in the system for a long time. For example, if priority range from 
127(low) to 0(high), we could increase the priority of a waiting process
 by 1 Every 15 minutes. Eventually, even a process with an initial 
priority of 127 would take no more than 32 hours for the priority 127 
process to age to a priority-0 process.

!https://media.geeksforgeeks.org/wp-content/uploads/startvationAndAging.jpg

*** Highest Response Ratio Next (HRRN) CPU Scheduling
https://www.geeksforgeeks.org/highest-response-ratio-next-hrrn-cpu-scheduling/
*** Multilevel Queue (MLQ) CPU Scheduling
It 
may happen that processes in the ready queue can be divided into 
different classes where each class has its own scheduling needs. For 
example, a common division is a **foreground (interactive)** process and a **background (batch)** process. These two classes have different scheduling needs. For this kind of situation **Multilevel Queue Scheduling** is used.

Now, let us see how it works.

Video Player is loading.

Remaining Time -0:00

1x

## **Advantages of Multilevel Queue CPU Scheduling:**

- The processes are permanently assigned to the queue, so it has advantage of low scheduling overhead.

## **Disadvantages of Multilevel Queue CPU Scheduling:**

- Some processes may starve for CPU if some higher priority queues are never becoming empty.
- It is inflexible in nature.

**Ready Queue**
 is divided into separate queues for each class of processes. For 
example, let us take three different types of processes System 
processes, Interactive processes, and Batch Processes. All three 
processes have their own queue. Now, look at the below figure.

!https://media.geeksforgeeks.org/wp-content/uploads/multilevel-queue-schedueling-1-300x217.png

The Description of the processes in the above diagram is as follows:

- **System Processes:** The CPU itself has its own process to run which is generally termed as System Process.
- **Interactive Processes:** An Interactive Process is a type of process in which there should be same type of interaction.
- **Batch Processes:** Batch processing is generally a technique in the Operating system that
collects the programs and data together in the form of the **batch** before the **processing** starts.

All
 three different type of processes has their own queue. Each queue has 
its own Scheduling algorithm. For example, queue 1 and queue 2 uses **Round Robin** while queue 3 can use **FCFS** to schedule their processes.

**Scheduling among the queues:**
 What will happen if all the queues have some processes? Which process 
should get the CPU? To determine this Scheduling among the queues is 
necessary. There are two ways to do so –

1. **Fixed priority preemptive scheduling method –** Each queue has absolute priority over the lower priority queue. Let us consider following priority order **queue 1 > queue 2 > queue 3**.According to this algorithm, no process in the batch queue(queue 3) can run
unless queues 1 and 2 are empty. If any batch process (queue 3) is
running and any system (queue 1) or Interactive process(queue 2) entered the ready queue the batch process is preempted.
2. **Time slicing** – In this method, each queue gets a certain portion of CPU time and can use it to schedule its own processes. For instance, queue 1 takes 50
percent of CPU time queue 2 takes 30 percent and queue 3 gets 20 percent of CPU time.

**Example Problem :** Consider below table of four processes under Multilevel queue scheduling. Queue number denotes the queue of the process. 

!https://media.geeksforgeeks.org/wp-content/uploads/Multilevel-Process-Queue-1.png

Priority of queue 1 is greater than queue 2. queue 1 uses Round Robin (Time Quantum = 2) and queue 2 uses FCFS.

Below is the **gantt chart** of the problem :

!https://media.geeksforgeeks.org/wp-content/uploads/Gantt-Chart-Multilevel-Queue.png

**Working:**

- At starting, both queues have process so process in queue 1 (P1, P2) runs
first (because of higher priority) in the round robin fashion and
completes after 7 units
- Then process in queue 2 (P3) starts
running (as there is no process in queue 1) but while it is running P4
comes in queue 1 and interrupts P3 and start running for 5 second and
- After its completion P3 takes the CPU and completes its execution.

This article is contributed by **Ashish Sharma**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.
 
*** Multilevel Feedback Queue Scheduling (MLFQ) CPU Scheduling
**Multilevel Feedback Queue Scheduling (MLFQ)** CPU Scheduling is like [Multilevel Queue(MLQ) Scheduling](https://www.geeksforgeeks.org/multilevel-queue-mlq-cpu-scheduling/) but in this processes can move between the queues. And thus, much more efficient than multilevel queue scheduling.

**Characteristics of Multilevel Feedback Queue Scheduling:**

!https://videocdn.geeksforgeeks.org/geeksforgeeks/MultilevelFeedbackQueueSchedulingOperatingSystem/MultilevelFeedbackQueueSchedulingOperatingSystem20220707133112.jpg

- In a [multilevel queue-scheduling](https://www.geeksforgeeks.org/multilevel-queue-mlq-cpu-scheduling/) algorithm, processes are permanently assigned to a queue on entry to
the system and processes are not allowed to move between queues.
- As the processes are permanently assigned to the queue, this setup has the advantage of low scheduling overhead,
- But on the other hand disadvantage of being inflexible.

**Advantages of Multilevel Feedback Queue Scheduling:**

- It is more flexible.
- It allows different processes to move between different queues.
- It prevents starvation by moving a process that waits too long for the lower priority queue to the higher priority queue.

**Disadvantages of Multilevel Feedback Queue Scheduling:**

- For the selection of the best scheduler, it requires some other means to select the values.
- It produces more CPU overheads.
- It is the most complex algorithm.

**Multilevel feedback queue scheduling**, however, allows a process to move between queues. Multilevel Feedback Queue ****Scheduling **(MLFQ)** keeps analyzing the behavior (time of execution) of processes and according to which it changes its priority.

Now, look at the diagram and explanation below to understand it properly.

!https://media.geeksforgeeks.org/wp-content/uploads/Multilevel-Feedback-Queue-Scheduling-300x269.png

Now let us suppose that queues 1 and 2 follow [round robin](https://www.geeksforgeeks.org/program-round-robin-scheduling-set-1/) with time quantum 4 and 8 respectively and queue 3 follow [FCFS](https://www.geeksforgeeks.org/program-for-fcfs-cpu-scheduling-set-1/).

Implementation of MFQS is given below –

- When a process starts executing the operating system can insert it into any of the above three queues depending upon its **priority**. For example, if it is some background process, then the operating
system would not like it to be given to higher priority queues such as
queue 1 and 2. It will directly assign it to a lower priority queue i.e. queue 3. Let’s say our current process for consideration is of
significant priority so it will be given **queue 1**.
- In queue 1 process executes for 4 units and if it completes in these 4
units or it gives CPU for I/O operation in these 4 units then the
priority of this process does not change and if it again comes in the
ready queue then it again starts its execution in Queue 1.
- If a process in queue 1 does not complete in 4 units then its priority gets reduced and it shifted to queue 2.
- Above points 2 and 3 are also true for queue 2 processes but the time quantum is 8 units. In a general case if a process does not complete in a time
quantum then it is shifted to the lower priority queue.
- In the last queue, processes are scheduled in an FCFS manner.
- A process in a lower priority queue can only execute only when higher priority queues are empty.
- A process running in the lower priority queue is interrupted by a process arriving in the higher priority queue.

> Well, the above implementation may differ for example the last queue can also follow Round-robin Scheduling.
> 
> 
> **Problems in the above implementation:** A process in the lower priority queue can suffer from starvation due to some short processes taking all the CPU time.
> 
> **Solution:** A
>  simple solution can be to boost the priority of all the processes after
>  regular intervals and place them all in the highest priority queue.
> 

### **What is the need for such complex Scheduling?**

- Firstly, it is more flexible than multilevel queue scheduling.
- To optimize turnaround time algorithms like SJF are needed which require
the running time of processes to schedule them. But the running time of
the process is not known in advance. MFQS runs a process for a time
quantum and then it can change its priority(if it is a long process).
Thus it learns from past behavior of the process and then predicts its
future behavior. This way it tries to run a shorter process first thus
optimizing turnaround time.
- MFQS also reduces the response time.

**Example:** Consider
 a system that has a CPU-bound process, which requires a burst time of 
40 seconds. The multilevel Feed Back Queue scheduling algorithm is used 
and the queue time quantum ‘2’ seconds and in each level it is 
incremented by ‘5’ seconds. Then how many times the process will be 
interrupted and in which queue the process will terminate the 
execution?

**Solution:**

- Process P needs 40 Seconds for total execution.
- At Queue 1 it is executed for 2 seconds and then interrupted and shifted to queue 2.
- At Queue 2 it is executed for 7 seconds and then interrupted and shifted to queue 3.
- At Queue 3 it is executed for 12 seconds and then interrupted and shifted to queue 4.
- At Queue 4 it is executed for 17 seconds and then interrupted and shifted to queue 5.
- At Queue 5 it executes for 2 seconds and then it completes.
- Hence the process is interrupted 4 times and completed on queue 5.
*** Lottery Process Scheduling in Operating System
**Prerequisite –** [CPU Scheduling](https://www.geeksforgeeks.org/gate-notes-operating-system-process-scheduling/), [Process Management](https://www.geeksforgeeks.org/gate-notes-operating-system-process-management-introduction/)**Lottery Scheduling**
 is type of process scheduling, somewhat different from other 
Scheduling. Processes are scheduled in a random manner. Lottery 
scheduling can be **preemptive or non-preemptive**. It also solves 
the problem of starvation. Giving each process at least one lottery 
ticket guarantees that it has non-zero probability of being selected at 
each scheduling operation.

In
 this scheduling every process have some tickets and scheduler picks a 
random ticket and process having that ticket is the winner and it is 
executed for a time slice and then another ticket is picked by the 
scheduler. These tickets represent the share of processes. A process 
having a higher number of tickets give it more chance to get chosen for 
execution.

**Example –** If we have two processes A and B 
having 60 and 40 tickets respectively out of total 100 tickets. CPU 
share of A is 60% and that of B is 40%.These shares are calculated 
probabilistically and not deterministically.

**Explanation –**

1. We have two processes A and B. A has 60 tickets (ticket number 1 to 60) and B have 40 tickets (ticket no. 61 to 100).
2. Scheduler picks a random number from 1 to 100. If the picked no. is from 1 to 60 then A is executed otherwise B is executed.
3. An example of 10 tickets picked by Scheduler may look like this –
    
    ```
    Ticket number -  73 82 23 45 32 87 49 39 12 09.
    Resulting Schedule -  B  B  A  A  A  B  A  A  A  A.
    
    ```
    
4. A is executed 7 times and B is executed 3 times. As you
can see that A takes 70% of CPU and B takes 30% which is not the same as what we need as we need A to have 60% of CPU and B should have 40% of
CPU.This happens because shares are calculated probabilistically but in a long run(i.e when no. of tickets picked is more than 100 or 1000) we
can achieve a share percentage of approx. 60 and 40 for A and B
respectively.

**Ways to manipulate tickets –**

- **Ticket Currency –**Scheduler give a certain number of tickets to different users in a currency and
users can give it to there processes in a different currency. E.g. Two
users A and B are given 100 and 200 tickets respectively. User A is
running two process and give 50 tickets to each in A’s own currency. B
is running 1 process and gives it all 200 tickets in B’s currency. Now
at the time of scheduling tickets of each process are converted into
global currency i.e A’s process will have 50 tickets each and B’s
process will have 200 tickets and scheduling is done on this basis.
- **Transfer Tickets –**A process can pass its tickets to another process.
- **Ticket inflation –**With this technique a process can temporarily raise or lower the number of tickets it own.

**References –**[Lottery scheduling – Wikipedia](https://en.wikipedia.org/wiki/Lottery_scheduling)
*** Multiple-Processor Scheduling in Operating System
In multiple-processor scheduling **multiple CPU’s** are available and hence **Load Sharing** becomes possible. However multiple processor scheduling is more **complex**
 as compared to single processor scheduling. In multiple processor 
scheduling there are cases when the processors are identical i.e. 
HOMOGENEOUS, in terms of their functionality, we can use any processor 
available to run any process in the queue.

### Approaches to Multiple-Processor Scheduling –

One approach is when all the scheduling decisions and I/O processing are handled by a single processor which is called the **Master Server** and the other processors executes only the **user code**. This is simple and reduces the need of data sharing. This entire scenario is called **Asymmetric Multiprocessing**.

A second approach uses **Symmetric Multiprocessing** where each processor is **self scheduling**.
 All processes may be in a common ready queue or each processor may have
 its own private queue for ready processes. The scheduling proceeds 
further by having the scheduler for each processor examine the ready 
queue and select a process to execute.

### Processor Affinity –

Processor Affinity means a processes has an **affinity** for the processor on which it is currently running.When
 a process runs on a specific processor there are certain effects on the
 cache memory. The data most recently accessed by the process populate 
the cache for the processor and as a result successive memory access by 
the process are often satisfied in the cache memory. Now if the process 
migrates to another processor, the contents of the cache memory must be 
invalidated for the first processor and the cache for the second 
processor must be repopulated. Because of the high cost of invalidating 
and repopulating caches, most of the SMP(symmetric multiprocessing) 
systems try to avoid migration of processes from one processor to 
another and try to keep a process running on the same processor. This is
 known as **PROCESSOR AFFINITY**.

There are two types of processor affinity:

1. **Soft Affinity –** When an operating system has a policy of attempting to keep a process
running on the same processor but not guaranteeing it will do so, this
situation is called soft affinity.
2. **Hard Affinity –** Hard
Affinity allows a process to specify a subset of processors on which it
may run. Some systems such as Linux implements soft affinity but also
provide some system calls like *sched_setaffinity()* that supports hard affinity.

### Load Balancing –

Load Balancing is the **phenomena** which keeps the **workload** evenly **distributed**
 across all processors in an SMP system. Load balancing is necessary 
only on systems where each processor has its own private queue of 
process which are eligible to execute. Load balancing is unnecessary 
because once a processor becomes idle it immediately extracts a runnable
 process from the common run queue. On SMP(symmetric multiprocessing), 
it is important to keep the workload balanced among all processors to 
fully utilize the benefits of having more than one processor else one or
 more processor will sit idle while other processors have high workloads
 along with lists of processors awaiting the CPU.

There are two general approaches to load balancing :

1. **Push Migration –** In push migration a task routinely checks the load on each processor
and if it finds an imbalance then it evenly distributes load on each
processors by moving the processes from overloaded to idle or less busy
processors.
2. **Pull Migration –** Pull Migration occurs when an idle processor pulls a waiting task from a busy processor for its execution.

### Multicore Processors –

In multicore processors **multiple processor**
 cores are places on the same physical chip. Each core has a register 
set to maintain its architectural state and thus appears to the 
operating system as a separate physical processor. **SMP systems** that use multicore processors are faster and consume **less power** than systems in which each processor has its own physical chip.

However multicore processors may **complicate**
 the scheduling problems. When processor accesses memory then it spends a
 significant amount of time waiting for the data to become available. 
This situation is called **MEMORY STALL**. It occurs for various 
reasons such as cache miss, which is accessing the data that is not in 
the cache memory. In such cases the processor can spend upto fifty 
percent of its time waiting for data to become available from the 
memory. To solve this problem recent hardware designs have implemented 
multithreaded processor cores in which two or more hardware threads are 
assigned to each core. Therefore if one thread stalls while waiting for 
the memory, core can switch to another thread.

There are two ways to multithread a processor :

1. **Coarse-Grained Multithreading –** In coarse grained multithreading a thread executes on a processor until a long latency event such as a memory stall occurs, because of the
delay caused by the long latency event, the processor must switch to
another thread to begin execution. The cost of switching between threads is high as the instruction pipeline must be terminated before the other thread can begin execution on the processor core. Once this new thread
begins execution it begins filling the pipeline with its instructions.
2. **Fine-Grained Multithreading –** This multithreading switches between threads at a much finer level
mainly at the boundary of an instruction cycle. The architectural design of fine grained systems include logic for thread switching and as a
result the cost of switching between threads is small.

### Virtualization and Threading –

In this type of **multiple-processor**
 scheduling even a single CPU system acts like a multiple-processor 
system. In a system with Virtualization, the virtualization presents one
 or more virtual CPU to each of virtual machines running on the system 
and then schedules the use of physical CPU among the virtual machines. 
Most virtualized environments have one host operating system and many 
guest operating systems. The host operating system creates and manages 
the virtual machines. Each virtual machine has a guest operating system 
installed and applications run within that guest.Each guest operating 
system may be assigned for specific use cases,applications or users 
including time sharing or even real-time operation. Any guest 
operating-system scheduling algorithm that assumes a certain amount of 
progress in a given amount of time will be negatively impacted by the 
virtualization. A time sharing operating system tries to allot 100 
milliseconds to each time slice to give users a reasonable response 
time. A given 100 millisecond time slice may take much more than 100 
milliseconds of virtual CPU time. Depending on how busy the system is, 
the time slice may take a second or more which results in a very poor 
response time for users logged into that virtual machine. The net effect
 of such scheduling layering is that individual virtualized operating 
systems receive only a portion of the available CPU cycles, even though 
they believe they are receiving all cycles and that they are scheduling 
all of those cycles.Commonly, the time-of-day clocks in virtual machines
 are incorrect because timers take no longer to trigger than they would 
on dedicated CPU’s.

**Virtualizations** can thus undo the good scheduling-algorithm efforts of the operating systems within virtual machines.

**Reference –**[Operating System Principles – Galvin](https://www.amazon.in/Operating-System-Principles-Silberschatz/dp/8126509627)

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
** Process Synchronization
*** Introduction of Process Synchronization
On the basis of synchronization, processes are categorized as one of the following two types:

- **Independent Process**: The execution of one process does not affect the execution of other processes.
- **Cooperative Process**: A process that can affect or be affected by other processes executing in the system.

Process
 synchronization problem arises in the case of Cooperative process also 
because resources are shared in Cooperative processes.

### Race Condition:

When
 more than one process is executing the same code or accessing the same 
memory or any shared variable in that condition there is a possibility 
that the output or the value of the shared variable is wrong so for that
 all the processes doing the race to say that my output is correct this 
condition known as a race condition. Several processes access and 
process the manipulations over the same data concurrently, then the 
outcome depends on the particular order in which the access takes place.
 A race condition is a situation that may occur inside a critical 
section. This happens when the result of multiple thread execution in 
the critical section differs according to the order in which the threads
 execute. Race conditions in critical sections can be avoided if the 
critical section is treated as an atomic instruction. Also, proper 
thread synchronization using locks or atomic variables can prevent race 
conditions.

### Critical Section Problem:

A critical 
section is a code segment that can be accessed by only one process at a 
time. The critical section contains shared variables that need to be 
synchronized to maintain the consistency of data variables. So the 
critical section problem means designing a way for cooperative processes
 to access shared resources without creating data inconsistencies.

!https://www.geeksforgeeks.org/wp-content/uploads/gq/2015/06/critical-section-problem.png

In the entry section, the process requests for entry in the **Critical Section.**

Any solution to the critical section problem must satisfy three requirements:

- **Mutual Exclusion**: If a process is executing in its critical section, then no other process is allowed to execute in the critical section.
- **Progress**: If no process is executing in the critical section and other processes
are waiting outside the critical section, then only those processes that are not executing in their remainder section can participate in
deciding which will enter in the critical section next, and the
selection can not be postponed indefinitely.
- **Bounded Waiting**: A bound must exist on the number of times that other processes are
allowed to enter their critical sections after a process has made a
request to enter its critical section and before that request is
granted.

### Peterson’s Solution:

Peterson’s Solution 
is a classical software-based solution to the critical section problem. 
In Peterson’s solution, we have two shared variables:

- boolean flag[i]: Initialized to FALSE, initially no one is interested in entering the critical section
- int turn: The process whose turn is to enter the critical section.

!https://www.geeksforgeeks.org/wp-content/uploads/gq/2015/06/peterson.png

**Peterson’s Solution preserves all three conditions:**

- Mutual Exclusion is assured as only one process can access the critical section at any time.
- Progress is also assured, as a process outside the critical section does not
block other processes from entering the critical section.
- Bounded Waiting is preserved as every process gets a fair chance.

**Disadvantages of Peterson’s solution:**

- It involves busy waiting.(In the Peterson’s solution, the code statement-
“while(flag[j] && turn == j);” is responsible for this. Busy
waiting is not favored because it wastes CPU cycles that could be used
to perform other tasks.)
- It is limited to 2 processes.
- Peterson’s solution cannot be used in modern CPU architectures.

### Semaphores:

A
 semaphore is a signaling mechanism and a thread that is waiting on a 
semaphore can be signaled by another thread. This is different than a 
mutex as the mutex can be signaled only by the thread that is called the
 wait function.

A semaphore uses two atomic operations, wait and signal for process synchronization.A Semaphore is an integer variable, which can be accessed only through two operations wait() and signal().There are two types of semaphores: Binary Semaphores and Counting Semaphores.

- **Binary Semaphores:** They can only be either 0 or 1. They are also known as mutex locks, as the
locks can provide mutual exclusion. All the processes can share the same mutex semaphore that is initialized to 1. Then, a process has to wait
until the lock becomes 0. Then, the process can make the mutex semaphore 1 and start its critical section. When it completes its critical
section, it can reset the value of the mutex semaphore to 0 and some
other process can enter its critical section.
- **Counting Semaphores:** They can have any value and are not restricted over a certain domain. They
can be used to control access to a resource that has a limitation on the number of simultaneous accesses. The semaphore can be initialized to
the number of instances of the resource. Whenever a process wants to use that resource, it checks if the number of remaining instances is more
than zero, i.e., the process has an instance available. Then, the
process can enter its critical section thereby decreasing the value of
the counting semaphore by 1. After the process is over with the use of
the instance of the resource, it can leave the critical section thereby
adding 1 to the number of available instances of the resource.

*** Process Synchronization | Set 2
Prerequisite – [Process Synchronization | Introduction](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Critical Section](https://www.geeksforgeeks.org/g-fact-70/), [Semaphores](https://www.geeksforgeeks.org/semaphores-operating-system/)**Process Synchronization**
 is a technique which is used to coordinate the process that use shared 
Data. There are two types of Processes in an Operating Systems:-

1. **Independent Process –**The process that does not affect or is affected by the other process while
its execution then the process is called Independent Process. Example
The process that does not share any shared variable, database, files,
etc.
2. **Cooperating Process –**The process that
affect or is affected by the other process while execution, is called a
Cooperating Process. Example The process that share file, variable,
database, etc are the Cooperating Process.

Process Synchronization is mainly used for Cooperating Process that shares the resources.Let us consider an example of//racing condition image

!https://media.geeksforgeeks.org/wp-content/uploads/Race-condition.png

It
 is the condition where several processes tries to access the resources 
and modify the shared data concurrently and outcome of the process 
depends on the particular order of execution that leads to data 
inconsistency, this condition is called **Race Condition**.This 
condition can be avoided using the technique called Synchronization or 
Process Synchronization, in which we allow only one process to enter and
 manipulates the shared data in Critical Section.//diagram of the view of CS

!https://media.geeksforgeeks.org/wp-content/uploads/CR_repr.png

This setup can be defined in various regions like:

- **Entry Section –**It is part of the process which decide the entry of a particular process in the Critical Section, out of many other processes.
- **Critical Section –**It is the part in which only one process is allowed to enter and modify
the shared variable.This part of the process ensures that only no other
process can access the resource of shared data.
- **Exit Section –**This process allows the other process that are waiting in the Entry Section, to enter into the Critical Sections. It checks that a process that
after a process has finished execution in Critical Section can be
removed through this Exit Section.
- **Remainder Section –**The other parts of the Code other than Entry Section, Critical Section and Exit Section are known as Remainder Section.

Critical Section problems must satisfy these three requirements:

1. **Mutual Exclusion –**It states that no other process is allowed to execute in the critical section if a process is executing in critical section.
2. **Progress –**When no process is in the critical section, then any process from outside
that request for execution can enter in the critical section without any delay. Only those process can enter that have requested and have finite time to enter the process.
3. **Bounded Waiting –**An upper bound must exist on the number of times a process enters so that other
processes are allowed to enter their critical sections after a process
has made a request to enter its critical section and before that request is granted.

Process Synchronization are handled by two approaches:

1. **Software Approach –**In Software Approach, Some specific Algorithm approach is used to maintain synchronization of the data. Like in Approach One or Approach Two, for a number of two process, a temporary variable like (turn) or boolean
variable (flag) value is used to store the data. When the condition is
True then the process in waiting State, known as Busy Waiting State.
This does not satisfy all the Critical Section requirements.
    
    Another 
    Software approach known as Peterson’s Solution is best for 
    Synchronization. It uses two variables in the Entry Section so as to 
    maintain consistency, like Flag (boolean variable) and Turn 
    variable(storing the process states). It satisfy all the three Critical 
    Section requirements.
    
    //Image of Peterson’s Algorithm
    
    !https://media.geeksforgeeks.org/wp-content/uploads/peterson-1.png
    
2. **Hardware Approach –**The Hardware Approach of synchronization can be done through Lock &
Unlock technique.Locking part is done in the Entry Section, so that only one process is allowed to enter into the Critical Section, after it
complete its execution, the process is moved to the Exit Section, where
Unlock Operation is done so that another process in the Lock Section can repeat this process of Execution.This process is designed in such a way that all the three conditions of the Critical Sections are satisfied.
    
    //Image of Lock
    
    !https://media.geeksforgeeks.org/wp-content/uploads/lock-2.png
    

**Using Interrupts –**These
 are easy to implement.When Interrupt are disabled then no other process
 is allowed to perform Context Switch operation that would allow only 
one process to enter into the Critical State.

//Image of Interrupts

!https://media.geeksforgeeks.org/wp-content/uploads/interrupt-1.png

**Test_and_Set Operation –**This
 allows boolean value (True/False) as a hardware Synchronization, which 
is atomic in nature i.e no other interrupt is allowed to access.This is 
mainly used in Mutual Exclusion Application. Similar type operation can 
be achieved through Compare and Swap function. In this process, a 
variable is allowed to accessed in Critical Section while its lock 
operation is ON.Till then, the other process is in Busy Waiting State. 
Hence Critical Section Requirements are achieved.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
**Critical Section in Synchronization
**[Critical Section](http://en.wikipedia.org/wiki/Critical_section):**
 When more than one processes access the same code segment that segment 
is known as the critical section. The critical section contains shared 
variables or resources which are needed to be synchronized to maintain 
the consistency of data variables. In simple terms, a critical section 
is a group of instructions/statements or region of code that need to be 
executed atomically ([read this post](https://www.geeksforgeeks.org/g-fact-57/)
 for atomicity), such as accessing a resource (file, input or output 
port, global data, etc.). In concurrent programming, if one thread tries
 to change the value of shared data at the same time as another thread 
tries to read the value (i.e. data race across threads), the result is 
unpredictable. The access to such shared variables (shared memory, 
shared files, shared port, etc…) is to be synchronized. Few programming 
languages have built-in support for synchronization. It is critical to 
understand the importance of race conditions while writing kernel-mode 
programming (a device driver, kernel thread, etc.). since the programmer
 can directly access and modify kernel data structures.

!https://media.geeksforgeeks.org/wp-content/uploads/20190409154423/os2.png

It could be visualized using the pseudo-code below:-

```
do{
    flag=1;
    while(flag); // (entry section)
        // critical section
    if (!flag)
        // remainder section
} while(true);
```

A simple solution to the critical section can be thought of as shown below,

```
acquireLock();
Process Critical Section
releaseLock();
```

A thread must acquire a
 lock prior to executing a critical section. The lock can be acquired by
 only one thread. There are various ways to implement locks in the 
above pseudo-code. Let us discuss them in future articles. Please write 
comments if you find anything incorrect, or you want to share more 
information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Inter Process Communication (IPC)
A process can be of two types:

- Independent process.
- Co-operating process.

An
 independent process is not affected by the execution of other processes
 while a co-operating process can be affected by other executing 
processes. Though one can think that those processes, which are running 
independently, will execute very efficiently, in reality, there are many
 situations when co-operative nature can be utilized for increasing 
computational speed, convenience, and modularity. Inter-process 
communication (IPC) is a mechanism that allows processes to communicate 
with each other and synchronize their actions. The communication between
 these processes can be seen as a method of co-operation between them. 
Processes can communicate with each other through both:

1. Shared Memory
2. Message passing

Figure
 1 below shows a basic structure of communication between processes via 
the shared memory method and via the message passing method. 

An
 operating system can implement both methods of communication. First, we
 will discuss the shared memory methods of communication and then 
message passing. Communication between processes using shared memory 
requires processes to share some variable, and it completely depends on 
how the programmer will implement it. One way of communication using 
shared memory can be imagined like this: Suppose process1 and process2 
are executing simultaneously, and they share some resources or use some 
information from another process. Process1 generates information about 
certain computations or resources being used and keeps it as a record in
 shared memory. When process2 needs to use the shared information, it 
will check in the record stored in shared memory and take note of the 
information generated by process1 and act accordingly. Processes can use
 shared memory for extracting information as a record from another 
process as well as for delivering any specific information to other 
processes. Let’s discuss an example of communication between processes using the shared memory method.

!https://media.geeksforgeeks.org/wp-content/uploads/1-76.png

**i) Shared Memory Method**

**Ex: Producer-Consumer problem** There
 are two processes: Producer and Consumer. The producer produces some 
items and the Consumer consumes that item. The two processes share a 
common space or memory location known as a buffer where the item 
produced by the Producer is stored and from which the Consumer consumes 
the item if needed. There are two versions of this problem: the first 
one is known as the unbounded buffer problem in which the Producer can 
keep on producing items and there is no limit on the size of the buffer,
 the second one is known as the bounded buffer problem in which the 
Producer can produce up to a certain number of items before it starts 
waiting for Consumer to consume it. We will discuss the bounded buffer 
problem. First, the Producer and the Consumer will share some common 
memory, then the producer will start producing items. If the total 
produced item is equal to the size of the buffer, the producer will wait
 to get it consumed by the Consumer. Similarly, the consumer will first 
check for the availability of the item. If no item is available, the 
Consumer will wait for the Producer to produce it. If there are items 
available, Consumer will consume them. The pseudo-code to demonstrate is
 provided below:**Shared Data between the two Processes** 

`#define buff_max 25`

`#define mod %`

`struct` `item{`

`// different member of the produced data`

`// or consumed data`

`---------`

`}`

`// An array is needed for holding the items.`

`// This is the shared place which will be`

`// access by both process`

`// item shared_buff [ buff_max ];`

`// Two variables which will keep track of`

`// the indexes of the items produced by producer`

`// and consumer The free index points to`

`// the next free index. The full index points to`

`// the first full index.`

`int` `free_index = 0;`

`int` `full_index = 0;`

**Producer Process Code** 

`item nextProduced;`

`while(1){`

`// check if there is no space`

`// for production.`

`// if so keep waiting.`

`while((free_index+1) mod buff_max == full_index);`

`shared_buff[free_index] = nextProduced;`

`free_index = (free_index + 1) mod buff_max;`

`}`

**Consumer Process Code** 

`item nextConsumed;`

`while(1){`

`// check if there is an available`

`// item  for consumption.`

`// if not keep on waiting for`

`// get them produced.`

`while((free_index == full_index);`

`nextConsumed = shared_buff[full_index];`

`full_index = (full_index + 1) mod buff_max;`

`}`

In
 the above code, the Producer will start producing again when the 
(free_index+1) mod buff max will be free because if it it not free, this
 implies that there are still items that can be consumed by the Consumer
 so there is no need to produce more. Similarly, if free index and full 
index point to the same index, this implies that there are no items to 
consume.

**ii) Messaging Passing Method**

Now,
 We will start our discussion of the communication between processes via
 message passing. In this method, processes communicate with each other 
without using any kind of shared memory. If two processes p1 and p2 want
 to communicate with each other, they proceed as follows:

- Establish a communication link (if a link already exists, no need to establish it again.)
- Start exchanging messages using basic primitives.We need at least two primitives: – **send**(message, destination) or **send**(message) – **receive**(message, host) or **receive**(message)

!https://media.geeksforgeeks.org/wp-content/uploads/2-50.png

The
 message size can be of fixed size or of variable size. If it is of 
fixed size, it is easy for an OS designer but complicated for a 
programmer and if it is of variable size then it is easy for a 
programmer but complicated for the OS designer. A standard message can 
have two parts: **header and body.** The **header part**
 is used for storing message type, destination id, source id, message 
length, and control information. The control information contains 
information like what to do if runs out of buffer space, sequence 
number, priority. Generally, message is sent using FIFO style.

**Message Passing through Communication Link.Direct and Indirect Communication link** Now,
 We will start our discussion about the methods of implementing 
communication links. While implementing the link, there are some 
questions that need to be kept in mind like : 

1. How are links established?
2. Can a link be associated with more than two processes?
3. How many links can there be between every pair of communicating processes?
4. What is the capacity of a link? Is the size of a message that the link can accommodate fixed or variable?
5. Is a link unidirectional or bi-directional?

A
 link has some capacity that determines the number of messages that can 
reside in it temporarily for which every link has a queue associated 
with it which can be of zero capacity, bounded capacity, or unbounded 
capacity. In zero capacity, the sender waits until the receiver informs 
the sender that it has received the message. In non-zero capacity cases,
 a process does not know whether a message has been received or not 
after the send operation. For this, the sender must communicate with the
 receiver explicitly. Implementation of the link depends on the 
situation, it can be either a direct communication link or an 
in-directed communication link. **Direct Communication links**
 are implemented when the processes use a specific process identifier 
for the communication, but it is hard to identify the sender ahead of 
time. **For example the print server.In-direct Communication**
 is done via a shared mailbox (port), which consists of a queue of 
messages. The sender keeps the message in mailbox and the receiver picks
 them up.

**Message Passing through Exchanging the Messages.**

**Synchronous and Asynchronous Message Passing:** A
 process that is blocked is one that is waiting for some event, such as a
 resource becoming available or the completion of an I/O operation. IPC 
is possible between the processes on same computer as well as on the 
processes running on different computer i.e. in networked/distributed 
system. In both cases, the process may or may not be blocked while 
sending a message or attempting to receive a message so message passing 
may be blocking or non-blocking. Blocking is considered **synchronous** and **blocking send** means the sender will be blocked until the message is received by receiver. Similarly, **blocking receive** has the receiver block until a message is available. Non-blocking is considered **asynchronous**
 and Non-blocking send has the sender sends the message and continue. 
Similarly, Non-blocking receive has the receiver receive a valid message
 or null. After a careful analysis, we can come to a conclusion that for
 a sender it is more natural to be non-blocking after message passing as
 there may be a need to send the message to different processes. 
However, the sender expects acknowledgment from the receiver in case the
 send fails. Similarly, it is more natural for a receiver to be blocking
 after issuing the receive as the information from the received message 
may be used for further execution. At the same time, if the message send
 keep on failing, the receiver will have to wait indefinitely. That is 
why we also consider the other possibility of message passing. There are
 basically three preferred combinations:

- Blocking send and blocking receive
- Non-blocking send and Non-blocking receive
- Non-blocking send and Blocking receive (Mostly used)

**In Direct message passing**, The process which wants to communicate must explicitly name the recipient or sender of the communication. e.g. **send(p1, message)** means send the message to p1. Similarly, **receive(p2, message)** means to receive the message from p2. In
 this method of communication, the communication link gets established 
automatically, which can be either unidirectional or bidirectional, but 
one link can be used between one pair of the sender and receiver and one
 pair of sender and receiver should not possess more than one pair of 
links. Symmetry and asymmetry between sending and receiving can also be 
implemented i.e. either both processes will name each other for sending 
and receiving the messages or only the sender will name the receiver for
 sending the message and there is no need for the receiver for naming 
the sender for receiving the message. The problem with this method of 
communication is that if the name of one process changes, this method 
will not work.**In Indirect message passing**, 
processes use mailboxes (also referred to as ports) for sending and 
receiving messages. Each mailbox has a unique id and processes can 
communicate only if they share a mailbox. Link established only if 
processes share a common mailbox and a single link can be associated 
with many processes. Each pair of processes can share several 
communication links and these links may be unidirectional or 
bi-directional. Suppose two processes want to communicate through 
Indirect message passing, the required operations are: create a mailbox,
 use this mailbox for sending and receiving messages, then destroy the 
mailbox. The standard primitives used are: **send(A, message)** which means send the message to mailbox A. The primitive for the receiving the message also works in the same way e.g. **received (A, message)**.
 There is a problem with this mailbox implementation. Suppose there are 
more than two processes sharing the same mailbox and suppose the process
 p1 sends a message to the mailbox, which process will be the receiver? 
This can be solved by either enforcing that only two processes can share
 a single mailbox or enforcing that only one process is allowed to 
execute the receive at a given time or select any process randomly and 
notify the sender about the receiver. A mailbox can be made private to a
 single sender/receiver pair and can also be shared between multiple 
sender/receiver pairs. Port is an implementation of such mailbox that 
can have multiple senders and a single receiver. It is used in 
client/server applications (in this case the server is the receiver). 
The port is owned by the receiving process and created by OS on the 
request of the receiver process and can be destroyed either on request 
of the same receiver processor when the receiver terminates itself. 
Enforcing that only one process is allowed to execute the receive can be
 done using the concept of mutual exclusion. **Mutex mailbox**
 is created which is shared by n process. The sender is non-blocking and
 sends the message. The first process which executes the receive will 
enter in the critical section and all other processes will be blocking 
and will wait.Now, let’s discuss the Producer-Consumer problem using
 the message passing concept. The producer places items (inside 
messages) in the mailbox and the consumer can consume an item when at 
least one message present in the mailbox. The code is given below:**Producer Code** 

`void` `Producer(void){`

`int` `item;`

`Message m;`

`while(1){`

`receive(Consumer, &m);`

`item = produce();`

`build_message(&m , item ) ;`

`send(Consumer, &m);`

`}`

`}`

**Consumer Code** 

`void` `Consumer(void){`

`int` `item;`

`Message m;`

`while(1){`

`receive(Producer, &m);`

`item = extracted_item();`

`send(Producer, &m);`

`consume_item(item);`

`}`

`}`

**Examples of IPC systems** 

1. Posix : uses shared memory method.
2. Mach : uses message passing
3. Windows XP : uses message passing using local procedural calls

**Communication in client/server Architecture:**There are various mechanism: 

- Pipe
- Socket
- Remote Procedural calls (RPCs)

The
 above three methods will be discussed in later articles as all of them 
are quite conceptual and deserve their own separate articles.**References:** 

1. Operating System Concepts by Galvin et al.
2. Lecture notes/ppt of Ariel J. Frank, Bar-Ilan University

More Reference: http://nptel.ac.in/courses/106108101/pdf/Lecture_Notes/Mod%207_LN.pdf https://www.youtube.com/watch?v=lcRqHwIn5DkThis article is contributed by **Durgesh Pandey**.
 Please write comments if you find anything incorrect, or you want to 
share more information about the topic discussed above. If you like 
GeeksforGeeks and would like to contribute, you can also write an 
article and mail your article to contribute@geeksforgeeks.org. See your 
article appearing on the GeeksforGeeks main page and help other Geeks.
*** Methods in Interprocess Communication
Prerequisite – [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/),**Inter-process communication (IPC)**
 is set of interfaces, which is usually programmed in order for the 
programs to communicate between series of processes. This allows running
 programs concurrently in an Operating System. These are the methods in 
IPC:

1. **Pipes (Same Process) –**This allows flow of data in one direction only. Analogous to simplex systems (Keyboard). Data from the output is usually buffered until input
process receives it which must have a common origin.
2. **Names Pipes (Different Processes) –**This is a pipe with a specific name it can be used in processes that don’t
have a shared common process origin. E.g. is FIFO where the details
written to a pipe is first named.
3. **Message Queuing –**This allows messages to be passed between processes using either a single
queue or several message queue. This is managed by system kernel these
messages are coordinated using an API.
4. **Semaphores –**This is used in solving problems associated with synchronization and to
avoid race condition. These are integer values which are greater than or equal to 0.
5. **Shared memory –**This allows the
interchange of data through a defined area of memory. Semaphore values
have to be obtained before data can get access to shared memory.
6. **Sockets –**This method is mostly used to communicate over a network between a client
and a server. It allows for a standard connection which is computer and
OS independent.
*** IPC through shared memory
[Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/)
 through shared memory is a concept where two or more process can access
 the common memory. And communication is done via this shared memory 
where changes made by one process can be viewed by another process.

The
 problem with pipes, fifo and message queue – is that for two process to
 exchange information. The information has to go through the kernel.

- Server reads from the input file.
- The server writes this data in a message using either a pipe, fifo or message queue.
- The client reads the data from the IPC channel,again requiring the data to
be copied from kernel’s IPC buffer to the client’s buffer.
- Finally the data is copied from the client’s buffer.

A
 total of four copies of data are required (2 read and 2 write). So, 
shared memory provides a way by letting two or more processes share a 
memory segment. With Shared Memory the data is only copied twice – from 
input file into shared memory and from shared memory to the output file.

SYSTEM CALLS USED ARE:

> ftok(): is use to generate a unique key.
> 
> 
> **shmget()**:
>  int shmget(key_t,size_tsize,intshmflg); upon successful completion, 
> shmget() returns an identifier for the shared memory segment.
> 
> **shmat()**: Before you can use a shared memory segment, you have to attach yourselfto it using shmat(). void *shmat(int shmid ,void *shmaddr ,int shmflg);shmid is shared memory id. shmaddr specifies specific address to use but we should setit to zero and OS will automatically choose the address.
> 
> **shmdt()**: When you’re done with the shared memory segment, your program shoulddetach itself from it using shmdt(). int shmdt(void *shmaddr);
> 
> **shmctl()**: when you detach from shared memory,it is not destroyed. So, to destroyshmctl() is used. shmctl(int shmid,IPC_RMID,NULL);
> 

SHARED MEMORY FOR WRITER PROCESS

`#include <iostream>`

`#include <sys/ipc.h>`

`#include <sys/shm.h>`

`#include <stdio.h>`

`using` `namespace` `std;`

`int` `main()`

`{`

`// ftok to generate unique key`

`key_t key = ftok("shmfile",65);`

`// shmget returns an identifier in shmid`

`int` `shmid = shmget(key,1024,0666|IPC_CREAT);`

`// shmat to attach to shared memory`

`char` `*str = (char*) shmat(shmid,(void*)0,0);`

`cout<<"Write Data : ";`

`gets(str);`

`printf("Data written in memory: %s\n",str);`

`//detach from shared memory`

`shmdt(str);`

`return` `0;`

`}`

SHARED MEMORY FOR READER PROCESS

`#include <iostream>`

`#include <sys/ipc.h>`

`#include <sys/shm.h>`

`#include <stdio.h>`

`using` `namespace` `std;`

`int` `main()`

`{`

`// ftok to generate unique key`

`key_t key = ftok("shmfile",65);`

`// shmget returns an identifier in shmid`

`int` `shmid = shmget(key,1024,0666|IPC_CREAT);`

`// shmat to attach to shared memory`

`char` `*str = (char*) shmat(shmid,(void*)0,0);`

`printf("Data read from memory: %s\n",str);`

`//detach from shared memory`

`shmdt(str);`

`// destroy the shared memory`

`shmctl(shmid,IPC_RMID,NULL);`

`return` `0;`

`}`

!https://media.geeksforgeeks.org/wp-content/uploads/shared-memory.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** IPC using Message Queues
Prerequisite : [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/) A
 message queue is a linked list of messages stored within the kernel and
 identified by a message queue identifier. A new queue is created or an 
existing queue opened by **msgget()**. New messages are added to the end of a queue by **msgsnd()**.
 Every message has a positive long integer type field, a non-negative 
length, and the actual data bytes (corresponding to the length), all of 
which are specified to msgsnd() when the message is added to a queue. 
Messages are fetched from a queue by **msgrcv()**. We don’t have to fetch the messages in a first-in, first-out order. Instead, we can fetch messages based on their type field.All
 processes can exchange information through access to a common system 
message queue. The sending process places a message (via some (OS) 
message-passing module) onto a queue which can be read by another 
process. Each message is given an identification or type so that 
processes can select the appropriate message. Process must share a 
common key in order to gain access to the queue in the first place.

!https://media.geeksforgeeks.org/wp-content/uploads/message-queue1.png

System calls used for message queues: 

- **ftok()**: is use to generate a unique key.
- **msgget()**: either returns the message queue identifier for a newly created message queue or returns the identifiers for a queue which exists with the same key value.
- **msgsnd()**: Data is placed on to a message queue by calling msgsnd().
- **msgrcv()**: messages are retrieved from a queue.
- **msgctl()**: It performs various operations on a queue. Generally it is use to destroy message queue.

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

MESSAGE QUEUE FOR WRITER PROCESS 

`// C Program for Message Queue (Writer Process)`

`#include <stdio.h>`

`#include <sys/ipc.h>`

`#include <sys/msg.h>`

`#define MAX 10`

`// structure for message queue`

`struct` `mesg_buffer {`

`long` `mesg_type;`

`char` `mesg_text[100];`

`} message;`

`int` `main()`

`{`

`key_t key;`

`int` `msgid;`

`// ftok to generate unique key`

`key = ftok("progfile", 65);`

`// msgget creates a message queue`

`// and returns identifier`

`msgid = msgget(key, 0666 | IPC_CREAT);`

`message.mesg_type = 1;`

`printf("Write Data : ");`

`fgets(message.mesg_text,MAX,stdin);`

`// msgsnd to send message`

`msgsnd(msgid, &message, sizeof(message), 0);`

`// display the message`

`printf("Data send is : %s \n", message.mesg_text);`

`return` `0;`

`}`

MESSAGE QUEUE FOR READER PROCESS 

`// C Program for Message Queue (Reader Process)`

`#include <stdio.h>`

`#include <sys/ipc.h>`

`#include <sys/msg.h>`

`// structure for message queue`

`struct` `mesg_buffer {`

`long` `mesg_type;`

`char` `mesg_text[100];`

`} message;`

`int` `main()`

`{`

`key_t key;`

`int` `msgid;`

`// ftok to generate unique key`

`key = ftok("progfile", 65);`

`// msgget creates a message queue`

`// and returns identifier`

`msgid = msgget(key, 0666 | IPC_CREAT);`

`// msgrcv to receive message`

`msgrcv(msgid, &message, sizeof(message), 1, 0);`

`// display the message`

`printf("Data Received is : %s \n",`

`message.mesg_text);`

`// to destroy the message queue`

`msgctl(msgid, IPC_RMID, NULL);`

`return` `0;`

`}`

Output: 

!https://media.geeksforgeeks.org/wp-content/uploads/message-queue.png
*** Message based Communication in IPC (inter process communication)
Prerequisites – [Cloud computing](https://www.geeksforgeeks.org/cloud-computing/), [Load balancing in Cloud Computing](https://www.geeksforgeeks.org/load-balancing-in-cloud-computing/), [Inter-process Communication](https://www.geeksforgeeks.org/inter-process-communication/)In
 the development of models and technologies, message abstraction is a 
necessary aspect that enables distributed computing. Distributed system 
is defined as a system in which components reside at networked 
communication and synchronise its functions only by movement of 
messages. In this, message recognizes any discrete data that is moved 
from one entity to another. It includes any kind of data representation 
having restriction of size and time, whereas it invokes a remote 
procedure or a sequence of object instance or a common message. This is 
the reason that “*message-based communication model*” can be beneficial to refer various model for inter-process communication, which is based on the data streaming abstraction.

Various
 distributed programming model use this type of communication despite of
 the abstraction which is shown to developers for programming the 
co-ordination of shared components. Below are some major distributed 
programming models that uses “*message-based communication model*”.

- **Message Passing –**In this model, the concept of message as the major abstraction of model is introduced. The units which inter-change the data and information that
is explicitly encode, in the form of message. According to then model,
the schema and content of message changes or varies. Message Passing
Interface and OpenMP are major example of this type of model.
- **[Remote Procedure Call](https://www.geeksforgeeks.org/operating-system-remote-procedure-call-rpc/) –**This model explores the keys of procedure call beyond the restrictions of a
single process, thus pointing the execution of program in remote
processes. In this, primary client-server is implied. A remote process
maintains a server component, thus enabling client processes to invoke
the approaches and returns the output of the execution. Messages,
created by the Remote Procedure Call (RPC) implementation, retrieve the
information of the procedure itself and that procedure is to execute
having necessary arguments and also returns the values. The use of
messages regarding this referred as marshal-ling of the arguments and
return values.
- **Distributed Objects –**It is
an implementation of Remote Procedure Call (RPC) model for the
Object-oriented model, and contextualizes this for the remote invocation of methods extended by objects. Every process assigns a set of
interfaces which are accessible remotely. Client process can demand a
reference to these interfaces and invoke the methods available through
them. The basic runtime infrastructure is in transformation the local
method calls into a request to a remote process and collecting the
result of the execution. The interaction within the caller and the
remote process is done trough messages. This model is stateless by
design, distributed object models introduce the complexity of object
state management and lifetime. Common Object Request Broker Architecture (CORBA), Component Object Model (COM, DCOM and COM+), Java Remote
Method Invocation (RMI), and .NET Remoting are some major examples which falls under Distributed object infrastructure.
- **Active objects –**Programming models based on active objects comprise by definition the presence of
instances, whether they are agent of objects, despite the availability
of requests. It means, that objects having particular control thread,
which enables them to convey their activity. These models sometime make
manual use of messages to encounter the execution of functions and a
more complex and a more complex semantics is attached to the messages.
- **Web Services –**Web service technology delivers an approach of the RPC concept over the
HTTP, thus enabling the communication of components that are evolved
with numerous technologies. A web service is revealed as a remote object maintained on a Web server, and method invocations are transformed in
HTTP requests wrapped with the help of specific protocol. It is
necessary to observe that the concept of message is a basic abstraction
of inter-process communication and it is utilized either implicitly or
explicitly.
*** Communication between two process using signals in C
**Prerequisite :** **[C signal handling](https://www.geeksforgeeks.org/signals-c-language/)**

In this post, the communication between child and parent processes is done using kill() and signal(), fork() system call.

- **[fork()](https://www.geeksforgeeks.org/fork-system-call/)** creates the child process from the parent. The pid can be checked to
decide whether it is the child (if pid == 0) or the parent (pid = child
process id).
- The parent can then send messages to child using the pid and kill().
- The child picks up these signals with signal() and calls appropriate functions.

**Example of how 2 processes can talk to each other using kill() and signal():**

`// C program to implement sighup(), sigint()`

`// and sigquit() signal functions`

`#include <signal.h>`

`#include <stdio.h>`

`#include <stdlib.h>`

`#include <sys/types.h>`

`#include <unistd.h>`

`// function declaration`

`void` `sighup();`

`void` `sigint();`

`void` `sigquit();`

`// driver code`

`void` `main()`

`{`

`int` `pid;`

`/* get child process */`

`if` `((pid = fork()) < 0) {`

`perror("fork");`

`exit(1);`

`}`

`if` `(pid == 0) { /* child */`

`signal(SIGHUP, sighup);`

`signal(SIGINT, sigint);`

`signal(SIGQUIT, sigquit);`

`for` `(;;)`

`; /* loop for ever */`

`}`

`else` `/* parent */`

`{ /* pid hold id of child */`

`printf("\nPARENT: sending SIGHUP\n\n");`

`kill(pid, SIGHUP);`

`sleep(3); /* pause for 3 secs */`

`printf("\nPARENT: sending SIGINT\n\n");`

`kill(pid, SIGINT);`

`sleep(3); /* pause for 3 secs */`

`printf("\nPARENT: sending SIGQUIT\n\n");`

`kill(pid, SIGQUIT);`

`sleep(3);`

`}`

`}`

`// sighup() function definition`

`void` `sighup()`

`{`

`signal(SIGHUP, sighup); /* reset signal */`

`printf("CHILD: I have received a SIGHUP\n");`

`}`

`// sigint() function definition`

`void` `sigint()`

`{`

`signal(SIGINT, sigint); /* reset signal */`

`printf("CHILD: I have received a SIGINT\n");`

`}`

`// sigquit() function definition`

`void` `sigquit()`

`{`

`printf("My DADDY has Killed me!!!\n");`

`exit(0);`

`}`

**Output:**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Screenshot-from-2018-08-01-18-31-03.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Semaphores in Process Synchronization
Semaphore
 was proposed by Dijkstra in 1965 which is a very significant technique 
to manage concurrent processes by using a simple integer value, which is
 known as a semaphore. Semaphore is simply an integer variable that is 
shared between threads. This variable is used to solve the critical 
section problem and to achieve process synchronization in the 
multiprocessing environment. Semaphores are of two types:

1. **Binary Semaphore –** This is also known as mutex lock. It can have only two values – 0 and 1. Its value is initialized to 1. It is used to implement the solution of
critical section problems with multiple processes.
2. **Counting Semaphore –** Its value can range over an unrestricted domain. It is used to control access to a resource that has multiple instances.

Now let us see how it does so.

First, look at two operations that can be used to access and change the value of the semaphore variable.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Semaphores_1.png

**Some point regarding P and V operation :**

1. P operation is also called wait, sleep, or down operation, and V operation is also called signal, wake-up, or up operation.
2. Both operations are atomic and semaphore(s) is always initialized to one.
Here atomic means that variable on which read, modify and update happens at the same time/moment with no pre-emption i.e. in-between read,
modify and update no other operation is performed that may change the
variable.
3. A critical section is surrounded by both operations to implement process synchronization. See the below image. The critical
section of Process P is in between P and V operation.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Semaphores_2.png

Now,
 let us see how it implements mutual exclusion. Let there be two 
processes P1 and P2 and a semaphore s is initialized as 1. Now if 
suppose P1 enters in its critical section then the value of semaphore s 
becomes 0. Now if P2 wants to enter its critical section then it will 
wait until s > 0, this can only happen when P1 finishes its critical 
section and calls V operation on semaphore s.

This way mutual exclusion is achieved. Look at the below image for details which is Binary semaphore.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Semaphores_3.jpg

**Implementation of binary semaphores :**

`struct` `semaphore {`

`enum` `value(0, 1);`

`// q contains all Process Control Blocks (PCBs)`

`// corresponding to processes got blocked`

`// while performing down operation.`

`Queue<process> q;`

`} P(semaphore s)`

`{`

`if` `(s.value == 1) {`

`s.value = 0;`

`}`

`else` `{`

`// add the process to the waiting queue`

`q.push(P)`

`sleep();`

`}`

`}`

`V(Semaphore s)`

`{`

`if` `(s.q is empty) {`

`s.value = 1;`

`}`

`else` `{`

`// select a process from waiting queue`

`Process p=q.front();`

`// remove the process from wating as it has been sent for CS`

`q.pop();`

`wakeup(p);`

`}`

`}`

The
 description above is for binary semaphore which can take only two 
values 0 and 1 and ensure mutual exclusion. There is one other type of 
semaphore called counting semaphore which can take values greater than 
one.

Now suppose there is a resource whose number of instances is 
4. Now we initialize S = 4 and the rest is the same as for binary 
semaphore. Whenever the process wants that resource it calls P or waits 
for function and when it is done it calls V or signal function. If the 
value of S becomes zero then a process has to wait until S becomes 
positive. For example, Suppose there are 4 processes P1, P2, P3, P4, and
 they all call wait operation on S(initialized with 4). If another 
process P5 wants the resource then it should wait until one of the four 
processes calls the signal function and the value of semaphore becomes 
positive.

**Limitations :**

1. One of the biggest limitations of semaphore is priority inversion.
2. Deadlock, suppose a process is trying to wake up another process which is not in a sleep state. Therefore, a deadlock may block indefinitely.
3. The operating system has to keep track of all calls to wait and to signal the semaphore.

**Problem in this implementation of semaphore :**The
 main problem with semaphores is that they require busy waiting, If a 
process is in the critical section, then other processes trying to enter
 critical section will be waiting until the critical section is not 
occupied by any process.Whenever any process waits then it 
continuously checks for semaphore value (look at this line while (s==0);
 in P operation) and waste CPU cycle.

There is also a chance of “spinlock” as the processes keep on spins while waiting for the lock.

To avoid this another implementation is provided below.

**Implementation of counting semaphore :**

`struct` `Semaphore {`

`int` `value;`

`// q contains all Process Control Blocks(PCBs)`

`// corresponding to processes got blocked`

`// while performing down operation.`

`Queue<process> q;`

`} P(Semaphore s)`

`{`

`s.value = s.value - 1;`

`if` `(s.value < 0) {`

`// add process to queue`

`// here p is a process which is currently executing`

`q.push(p);`

`block();`

`}`

`else`

`return;`

`}`

`V(Semaphore s)`

`{`

`s.value = s.value + 1;`

`if` `(s.value >= 0) {`

`// remove process p from queue`

`Process p=q.pop();`

`wakeup(p);`

`}`

`else`

`return;`

`}`

In
 this implementation whenever the process waits it is added to a waiting
 queue of processes associated with that semaphore. This is done through
 system call block() on that process. When a process is completed it 
calls the signal function and one process in the queue is resumed. It 
uses wakeup() system call.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

[Previous](https://www.geeksforgeeks.org/inter-process-communication-ipc/)

[Inter Process Communication (IPC)](https://www.geeksforgeeks.org/inter-process-communication-ipc/)

[Next](https://www.geeksforgeeks.org/mutex-vs-semaphore/)

[Mutex vs Semaphore](https://www.geeksforgeeks.org/mutex-vs-semaphore/)

Recommended Articles

Page :

1

2

3

[Difference Between Process, Parent Process, and Child Process01, May 21](https://www.geeksforgeeks.org/difference-between-process-parent-process-and-child-process/?ref=rp)

[Monitors in Process Synchronization01, Jul 15](https://www.geeksforgeeks.org/monitors-in-process-synchronization/?ref=rp)

[Peterson's Algorithm in Process Synchronizatio](https://www.geeksforgeeks.org/petersons-algorithm-in-process-synchronization/?ref=rp)
*** Mutex vs Semaphore
What is the difference between a mutex and a semaphore? When should you use a mutex and when should you use a semaphore?

A
 concrete understanding of Operating System concepts is required to 
design/develop smart applications. Our objective is to educate the 
reader on these concepts and learn from other expert geeks.

As 
per operating system terminology, mutexes and semaphores are kernel 
resources that provide synchronization services (also called *synchronization primitives*). *Why do we need such synchronization primitives? Won’t only one be sufficient?* To answer these questions, we need to understand a few keywords. Please read the posts on [atomicity](https://www.geeksforgeeks.org/g-fact-57/) and [critical section](https://www.geeksforgeeks.org/g-fact-70/). We will illustrate with examples to understand these concepts well, rather than following the usual OS textual description.

**The [producer-consumer](http://en.wikipedia.org/wiki/Producer-consumer_problem) problem:**

*Note that the content is a generalized explanation. Practical details vary with implementation.*

Consider
 the standard producer-consumer problem. Assume, we have a buffer of 
4096-byte length. A producer thread collects the data and writes it to 
the buffer. A consumer thread processes the collected data from the 
buffer. The objective is, both the threads should not run at the same 
time.

**Using Mutex:**

A mutex provides 
mutual exclusion, either producer or consumer can have the key (mutex) 
and proceed with their work. As long as the buffer is filled by the 
producer, the consumer needs to wait, and vice versa.

At any point of time, only one thread can work with the *entire* buffer. The concept can be generalized using semaphore.

**Using Semaphore:**

A
 semaphore is a generalized mutex. In lieu of a single buffer, we can 
split the 4 KB buffer into four 1 KB buffers (identical resources). A 
semaphore can be associated with these four buffers. The consumer and 
producer can work on different buffers at the same time.

**Misconception:**

There is an ambiguity between *binary semaphore* and *mutex*. We might have come across that a mutex is a binary semaphore. *But it is not*!
 The purpose of mutex and semaphore are different. Maybe, due to 
similarity in their implementation a mutex would be referred to as a 
binary semaphore.

Strictly speaking, a mutex is a **locking mechanism**
 used to synchronize access to a resource. Only one task (can be a 
thread or process based on OS abstraction) can acquire the mutex. It 
means there is ownership associated with a mutex, and only the owner can
 release the lock (mutex).

Semaphore is **signaling mechanism**
 (“I am done, you can carry on” kind of signal). For example, if you are
 listening to songs (assume it as one task) on your mobile phone and at 
the same time, your friend calls you, an interrupt is triggered upon 
which an interrupt service routine (ISR) signals the call processing 
task to wakeup.

**General Questions:**

*1. Can a thread acquire more than one lock (Mutex)?*

Yes,
 it is possible that a thread is in need of more than one resource, 
hence the locks. If any lock is not available the thread will wait 
(block) on the lock.

*2. Can a mutex be locked more than once?*

A mutex is a lock. Only one state (locked/unlocked) is associated with it. However, a *recursive mutex*
 can be locked more than once (POSIX compliant systems), in which a 
count is associated with it, yet retains only one state 
(locked/unlocked). The programmer must unlock the mutex as many number 
times as it was locked.

*3. What happens if a non-recursive mutex is locked more than once.*

Deadlock.
 If a thread that had already locked a mutex, tries to lock the mutex 
again, it will enter into the waiting list of that mutex, which results 
in a deadlock. It is because no other thread can unlock the mutex. An 
operating system implementer can exercise care in identifying the owner 
of the mutex and return if it is already locked by a same thread to 
prevent deadlocks.

*4. Are binary semaphore and mutex same?*

No.
 We suggest treating them separately, as it is explained in signaling vs
 locking mechanisms. But a binary semaphore may experience the same 
critical issues (e.g. priority inversion) associated with a mutex. We 
will cover these in a later article.

A programmer can prefer mutex rather than creating a semaphore with count 1.

*5. What is a mutex and critical section?*

Some operating systems use the same word *critical section*
 in the API. Usually a mutex is a costly operation due to protection 
protocols associated with it. At last, the objective of mutex is atomic 
access. There are other ways to achieve atomic access like disabling 
interrupts which can be much faster but ruins responsiveness. The 
alternate API makes use of disabling interrupts.

*6. What are events?*

The
 semantics of mutex, semaphore, event, critical section, etc… are same. 
All are synchronization primitives. Based on their cost in using them 
they are different. We should consult the OS documentation for exact 
details.

*7. Can we acquire mutex/semaphore in an Interrupt Service Routine?*

An ISR will run asynchronously in the context of current running thread. It is **not recommended**
 to query (blocking call) the availability of synchronization primitives
 in an ISR. The ISR are meant be short, the call to mutex/semaphore may 
block the current running thread. However, an ISR can signal a semaphore
 or unlock a mutex.

*8. What we mean by “thread blocking on mutex/semaphore” when they are not available?*

Every
 synchronization primitive has a waiting list associated with it. When 
the resource is not available, the requesting thread will be moved from 
the running list of processors to the waiting list of the 
synchronization primitive. When the resource is available, the higher 
priority thread on the waiting list gets the resource (more precisely, 
it depends on the scheduling policies).

*9. Is it necessary that a thread must block always when resource is not available?*

Not necessary. If the design is sure ‘*what has to be done when resource is not available*‘,
 the thread can take up that work (a different code branch). To support 
application requirements the OS provides non-blocking API.

For 
example POSIX pthread_mutex_trylock() API. When the mutex is not 
available the function returns immediately whereas the API 
pthread_mutex_lock() blocks the thread till the resource is available.
*** Monitors in Process Synchronization
The 
monitor is one of the ways to achieve Process synchronization. The 
monitor is supported by programming languages to achieve mutual 
exclusion between processes. For example Java Synchronized methods. Java
 provides wait() and notify() constructs.

1. It is the collection of condition variables and procedures combined together in a special kind of module or a package.
2. The processes running outside the monitor can’t access the internal
variable of the monitor but can call procedures of the monitor.
3. Only one process at a time can execute code inside monitors.

**Syntax:**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/monitors-300x255.png

**Condition Variables:**Two different operations are performed on the condition variables of the monitor.

```
Wait.
signal.

```

let say we have 2 condition variables**condition x, y; // Declaring variable**

**Wait operation**x.wait()
 : Process performing wait operation on any condition variable are 
suspended. The suspended processes are placed in block queue of that 
condition variable.

**Note:** Each condition variable has its unique block queue.

**Signal operation**x.signal(): When a process performs signal operation on condition variable, one of the blocked processes is given chance.

```
If (x block queue empty)
  // Ignore signal
else
  // Resume a process from block queue.
```

**Advantages of Monitor:**Monitors have the advantage of making parallel programming easier and less error prone than using techniques such as semaphore.

**Disadvantages of Monitor:**Monitors
 have to be implemented as part of the programming language . The 
compiler must generate code for them. This gives the compiler the 
additional burden of having to know what operating system facilities are
 available to control access to critical sections in concurrent 
processes. Some languages that do support monitors are Java,C#,Visual 
Basic,Ada and concurrent Euclid.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Peterson’s Algorithm for Mutual Exclusion | Set 1 (Basic C implementation)
**Problem:**
 Given 2 processes i and j, you need to write a program that can 
guarantee mutual exclusion between the two without any additional 
hardware support.

**Solution:**
 There can be multiple ways to solve this problem, but most of them 
require additional hardware support. The simplest and the most popular 
way to do this is by using Peterson’s Algorithm for mutual Exclusion. It
 was developed by Peterson in 1981 though the initial work in this 
direction was done by Theodorus Jozef Dekker who came up with **Dekker’s algorithm** in 1960, which was later refined by Peterson and came to be known as **Peterson’s Algorithm**.

Basically,
 Peterson’s algorithm provides guaranteed mutual exclusion by using only
 the shared memory. It uses two ideas in the algorithm:

1. Willingness to acquire lock.
2. Turn to acquire lock.

Prerequisite: [Multithreading in C](https://www.geeksforgeeks.org/multithreading-c-2/)

### Explanation:

The idea is that first a thread expresses its desire to acquire a lock and sets **flag[self] = 1**
 and then gives the other thread a chance to acquire the lock. If the 
thread desires to acquire the lock, then, it gets the lock and passes 
the chance to the 1st thread. If it does not desire to get the lock then
 the while loop breaks and the 1st thread gets the chance.

**Implementation in C language**

`// Filename: peterson_spinlock.c`

`// Use below command to compile:`

`// gcc -pthread peterson_spinlock.c -o peterson_spinlock`

`#include <stdio.h>`

`#include <pthread.h>`

`#include"mythreads.h"`

`int` `flag[2];`

`int` `turn;`

`const` `int` `MAX = 1e9;`

`int` `ans = 0;`

`void` `lock_init()`

`{`

`// Initialize lock by resetting the desire of`

`// both the threads to acquire the locks.`

`// And, giving turn to one of them.`

`flag[0] = flag[1] = 0;`

`turn = 0;`

`}`

`// Executed before entering critical section`

`void` `lock(int` `self)`

`{`

`// Set flag[self] = 1 saying you want to acquire lock`

`flag[self] = 1;`

`// But, first give the other thread the chance to`

`// acquire lock`

`turn = 1-self;`

`// Wait until the other thread looses the desire`

`// to acquire lock or it is your turn to get the lock.`

`while` `(flag[1-self]==1 && turn==1-self) ;`

`}`

`// Executed after leaving critical section`

`void` `unlock(int` `self)`

`{`

`// You do not desire to acquire lock in future.`

`// This will allow the other thread to acquire`

`// the lock.`

`flag[self] = 0;`

`}`

`// A Sample function run by two threads created`

`// in main()`

`void* func(void` `*s)`

`{`

`int` `i = 0;`

`int` `self = (int` `*)s;`

`printf("Thread Entered: %d\n", self);`

`lock(self);`

`// Critical section (Only one thread`

`// can enter here at a time)`

`for` `(i=0; i<MAX; i++)`

`ans++;`

`unlock(self);`

`}`

`// Driver code`

`int` `main()`

`{`

`// Initialized the lock then fork 2 threads`

`pthread_t p1, p2;`

`lock_init();`

`// Create two threads (both run func)`

`pthread_create(&p1, NULL, func, (void*)0);`

`pthread_create(&p2, NULL, func, (void*)1);`

`// Wait for the threads to end.`

`pthread_join(p1, NULL);`

`pthread_join(p2, NULL);`

`printf("Actual Count: %d | Expected Count: %d\n",`

`ans, MAX*2);`

`return` `0;`

`}`

`// mythread.h (A wrapper header file with assert`

`// statements)`

`#ifndef __MYTHREADS_h__`

`#define __MYTHREADS_h__`

`#include <pthread.h>`

`#include <assert.h>`

`#include <sched.h>`

`void` `Pthread_mutex_lock(pthread_mutex_t *m)`

`{`

`int` `rc = pthread_mutex_lock(m);`

`assert(rc == 0);`

`}`

`void` `Pthread_mutex_unlock(pthread_mutex_t *m)`

`{`

`int` `rc = pthread_mutex_unlock(m);`

`assert(rc == 0);`

`}`

`void` `Pthread_create(pthread_t *thread, const` `pthread_attr_t *attr,`

`void` `*(*start_routine)(void*), void` `*arg)`

`{`

`int` `rc = pthread_create(thread, attr, start_routine, arg);`

`assert(rc == 0);`

`}`

`void` `Pthread_join(pthread_t thread, void` `**value_ptr)`

`{`

`int` `rc = pthread_join(thread, value_ptr);`

`assert(rc == 0);`

`}`

`#endif // __MYTHREADS_h__`

**Output:**

```
Thread Entered: 1
Thread Entered: 0
Actual Count: 2000000000 | Expected Count: 2000000000
```

The produced output is 2*109 where 109 is incremented by both threads.

This article is contributed by **Pinkesh Badjatiya** . If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Peterson’s Algorithm for Mutual Exclusion | Set 2 (CPU Cycles and Memory Fence)
**Problem:**
 Given 2 process i and j, you need to write a program that can guarantee
 mutual exclusion between the two without any additional hardware 
support.

We strongly recommend to refer below basic solution discussed in previous article. [Peterson’s Algorithm for Mutual Exclusion | Set 1](https://www.geeksforgeeks.org/petersons-algorithm-for-mutual-exclusion-set-1/)We would be resolving 2 issues in the previous algorithm.

### Wastage of CPU clock cycles

In
 layman terms, when a thread was waiting for its turn, it ended in a 
long while loop which tested the condition millions of times per second 
thus doing unnecessary computation. There is a better way to wait, and 
it is known as *“yield”*.

To understand what it does, we need
 to dig deep into how the Process scheduler works in Linux. The idea 
mentioned here is a simplified version of the scheduler, the actual 
implementation has lots of complications.

Consider the following example, There
 are three processes, P1, P2 and P3. Process P3 is such that it has a 
while loop similar to the one in our code, doing not so useful 
computation, and it exists from the loop only when P2 finishes its 
execution. The scheduler puts all of them in a round robin queue. Now, 
say the clock speed of processor is 1000000/sec, and it allocates 100 
clocks to each process in each iteration. Then, first P1 will be run for
 100 clocks (0.0001 seconds), then P2(0.0001 seconds) followed by 
P3(0.0001 seconds), now since there are no more processes, this cycle 
repeats until P2 ends and then followed by P3’s execution and eventually
 its termination.

This is a complete waste of the 100 CPU clock 
cycles. To avoid this, we mutually give up the CPU time slice, i.e. 
yield, which essentially ends this time slice and the scheduler picks up
 the next process to run. Now, we test our condition once, then we give 
up the CPU. Considering our test takes 25 clock cycles, we save 75% of 
our computation in a time slice. To put this graphically,

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/peterson.png

Considering the processor clock speed as 1MHz this is a lot of saving!. Different distributions provide different function to achieve this functionality. Linux provides **sched_yield()**.

`void` `lock(int` `self)`

`{`

`flag[self] = 1;`

`turn = 1-self;`

`while` `(flag[1-self] == 1 &&`

`turn == 1-self)`

`// Only change is the addition of`

`// sched_yield() call`

`sched_yield();`

`}`

### Memory fence.

The
 code in earlier tutorial might have worked on most systems, but is was 
not 100% correct. The logic was perfect, but most modern CPUs employ 
performance optimizations that can result in out-of-order execution. 
This reordering of memory operations (loads and stores) normally goes 
unnoticed within a single thread of execution, but can cause 
unpredictable behaviour in concurrent programs.Consider this example,

`while` `(f == 0);`

`// Memory fence required here`

`print x;`

In
 the above example, the compiler considers the 2 statements as 
independent of each other and thus tries to increase the code efficiency
 by re-ordering them, which can lead to problems for concurrent 
programs. To avoid this we place a memory fence to give hint to the 
compiler about the possible relationship between the statements across 
the barrier.

So the order of statements,

> flag[self] = 1; turn = 1-self; while (turn condition check) yield();
> 

has to be exactly the same in order for the lock to work, otherwise it will end up in a deadlock condition.

To ensure this, compilers provide a instruction that prevent ordering of statements across this barrier. In case of gcc, its **__sync_synchronize()**.So the modified code becomes, **Full Implementation in C:**

`// Filename: peterson_yieldlock_memoryfence.c`

`// Use below command to compile:`

`// gcc -pthread peterson_yieldlock_memoryfence.c -o peterson_yieldlock_memoryfence`

`#include<stdio.h>`

`#include<pthread.h>`

`#include "mythreads.h"`

`int` `flag[2];`

`int` `turn;`

`const` `int` `MAX = 1e9;`

`int` `ans = 0;`

`void` `lock_init()`

`{`

`// Initialize lock by reseting the desire of`

`// both the threads to acquire the locks.`

`// And, giving turn to one of them.`

`flag[0] = flag[1] = 0;`

`turn = 0;`

`}`

`// Executed before entering critical section`

`void` `lock(int` `self)`

`{`

`// Set flag[self] = 1 saying you want`

`// to acquire lock`

`flag[self]=1;`

`// But, first give the other thread the`

`// chance to acquire lock`

`turn = 1-self;`

`// Memory fence to prevent the reordering`

`// of instructions beyond this barrier.`

`__sync_synchronize();`

`// Wait until the other thread loses the`

`// desire to acquire  lock or it is your`

`// turn to get the lock.`

`while` `(flag[1-self]==1 && turn==1-self)`

`// Yield to avoid wastage of resources.`

`sched_yield();`

`}`

`// Executed after leaving critical section`

`void` `unlock(int` `self)`

`{`

`// You do not desire to acquire lock in future.`

`// This will allow the other thread to acquire`

`// the lock.`

`flag[self]=0;`

`}`

`// A Sample function run by two threads created`

`// in main()`

`void* func(void` `*s)`

`{`

`int` `i = 0;`

`int` `self = (int` `*)s;`

`printf("Thread Entered: %d\n",self);`

`lock(self);`

`// Critical section (Only one thread`

`// can enter here at a time)`

`for` `(i=0; i<MAX; i++)`

`ans++;`

`unlock(self);`

`}`

`// Driver code`

`int` `main()`

`{`

`pthread_t p1, p2;`

`// Initialize the lock`

`lock_init();`

`// Create two threads (both run func)`

`Pthread_create(&p1, NULL, func, (void*)0);`

`Pthread_create(&p2, NULL, func, (void*)1);`

`// Wait for the threads to end.`

`Pthread_join(p1, NULL);`

`Pthread_join(p2, NULL);`

`printf("Actual Count: %d | Expected Count:"`

`" %d\n",ans,MAX*2);`

`return` `0;`

`}`

`// mythread.h (A wrapper header file with assert`

`// statements)`

`#ifndef __MYTHREADS_h__`

`#define __MYTHREADS_h__`

`#include <pthread.h>`

`#include <assert.h>`

`#include <sched.h>`

`void` `Pthread_mutex_lock(pthread_mutex_t *m)`

`{`

`int` `rc = pthread_mutex_lock(m);`

`assert(rc == 0);`

`}`

`void` `Pthread_mutex_unlock(pthread_mutex_t *m)`

`{`

`int` `rc = pthread_mutex_unlock(m);`

`assert(rc == 0);`

`}`

`void` `Pthread_create(pthread_t *thread, const` `pthread_attr_t *attr,`

`void` `*(*start_routine)(void*), void` `*arg)`

`{`

`int` `rc = pthread_create(thread, attr, start_routine, arg);`

`assert(rc == 0);`

`}`

`void` `Pthread_join(pthread_t thread, void` `**value_ptr)`

`{`

`int` `rc = pthread_join(thread, value_ptr);`

`assert(rc == 0);`

`}`

`#endif // __MYTHREADS_h__`

Output:

```
Thread Entered: 1
Thread Entered: 0
Actual Count: 2000000000 | Expected Count: 2000000000
```

This article is contributed by **Pinkesh Badjatiya** . If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](https://write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Peterson’s Algorithm in Process Synchronization
Prerequisite – [synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Critical Section](https://www.geeksforgeeks.org/g-fact-70/)

**Problem:**The
 producer consumer problem (or bounded buffer problem) describes two 
processes, the producer and the consumer, which share a common, 
fixed-size buffer used as a queue. Producer produce an item and put it 
into buffer. If buffer is already full then producer will have to wait 
for an empty block in buffer. Consumer consume an item from buffer. If 
buffer is already empty then consumer will have to wait for an item in 
buffer. Implement Peterson’s Algorithm for the two processes using 
shared memory such that there is mutual exclusion between them. The 
solution should have free from synchronization problems.

!https://media.geeksforgeeks.org/wp-content/uploads/producer-consumer.png

**Peterson’s algorithm –**

`// code for producer (j)`

`// producer j is ready`

`// to produce an item`

`flag[j] = true;`

`// but consumer (i) can consume an item`

`turn = i;`

`// if consumer is ready to consume an item`

`// and if its consumer's turn`

`while` `(flag[i] == true` `&& turn == i)`

`{ // then producer will wait }`

`// otherwise producer will produce`

`// an item and put it into buffer (critical Section)`

`// Now, producer is out of critical section`

`flag[j] = false;`

`// end of code for producer`

`//--------------------------------------------------------`

`// code for consumer i`

`// consumer i is ready`

`// to consume an item`

`flag[i] = true;`

`// but producer (j) can produce an item`

`turn = j;`

`// if producer is ready to produce an item`

`// and if its producer's turn`

`while` `(flag[j] == true` `&& turn == j)`

`{ // then consumer will wait }`

`// otherwise consumer will consume`

`// an item from buffer (critical Section)`

`// Now, consumer is out of critical section`

`flag[i] = false;`

`// end of code for consumer`

**Explanation of Peterson’s algorithm –**

Peterson’s Algorithm is used to synchronize two processes. It uses two variables, a bool array **flag** of size 2 and an int variable **turn** to accomplish it.In
 the solution i represents the Consumer and j represents the Producer. 
Initially the flags are false. When a process wants to execute it’s 
critical section, it sets it’s flag to true and turn as the index of the
 other process. This means that the process wants to execute but it will
 allow the other process to run first. The process performs busy waiting
 until the other process has finished it’s own critical section.After
 this the current process enters it’s critical section and adds or 
removes a random number from the shared buffer. After completing the 
critical section, it sets it’s own flag to false, indication it does not
 wish to execute anymore.

The program runs for a fixed amount of time before exiting. This time can be changed by changing value of the macro RT.

`// C program to implement Peterson’s Algorithm`

`// for producer-consumer problem.`

`#include <stdio.h>`

`#include <stdlib.h>`

`#include <unistd.h>`

`#include <time.h>`

`#include <sys/types.h>`

`#include <sys/ipc.h>`

`#include <sys/shm.h>`

`#include <stdbool.h>`

`#define _BSD_SOURCE`

`#include <sys/time.h>`

`#include <stdio.h>`

`#define BSIZE 8 // Buffer size`

`#define PWT 2 // Producer wait time limit`

`#define CWT 10 // Consumer wait time limit`

`#define RT 10 // Program run-time in seconds`

`int` `shmid1, shmid2, shmid3, shmid4;`

`key_t k1 = 5491, k2 = 5812, k3 = 4327, k4 = 3213;`

`bool* SHM1;`

`int* SHM2;`

`int* SHM3;`

`int` `myrand(int` `n) // Returns a random number between 1 and n`

`{`

`time_t` `t;`

`srand((unsigned)time(&t));`

`return` `(rand() % n + 1);`

`}`

`int` `main()`

`{`

`shmid1 = shmget(k1, sizeof(bool) * 2, IPC_CREAT | 0660); // flag`

`shmid2 = shmget(k2, sizeof(int) * 1, IPC_CREAT | 0660); // turn`

`shmid3 = shmget(k3, sizeof(int) * BSIZE, IPC_CREAT | 0660); // buffer`

`shmid4 = shmget(k4, sizeof(int) * 1, IPC_CREAT | 0660); // time stamp`

`if` `(shmid1 < 0 || shmid2 < 0 || shmid3 < 0 || shmid4 < 0) {`

`perror("Main shmget error: ");`

`exit(1);`

`}`

`SHM3 = (int*)shmat(shmid3, NULL, 0);`

`int` `ix = 0;`

`while` `(ix < BSIZE) // Initializing buffer`

`SHM3[ix++] = 0;`

`struct` `timeval t;`

`time_t` `t1, t2;`

`gettimeofday(&t, NULL);`

`t1 = t.tv_sec;`

`int* state = (int*)shmat(shmid4, NULL, 0);`

`*state = 1;`

`int` `wait_time;`

`int` `i = 0; // Consumer`

`int` `j = 1; // Producer`

`if` `(fork() == 0) // Producer code`

`{`

`SHM1 = (bool*)shmat(shmid1, NULL, 0);`

`SHM2 = (int*)shmat(shmid2, NULL, 0);`

`SHM3 = (int*)shmat(shmid3, NULL, 0);`

`if` `(SHM1 == (bool*)-1 || SHM2 == (int*)-1 || SHM3 == (int*)-1) {`

`perror("Producer shmat error: ");`

`exit(1);`

`}`

`bool* flag = SHM1;`

`int* turn = SHM2;`

`int* buf = SHM3;`

`int` `index = 0;`

`while` `(*state == 1) {`

`flag[j] = true;`

`printf("Producer is ready now.\n\n");`

`*turn = i;`

`while` `(flag[i] == true` `&& *turn == i)`

`;`

`// Critical Section Begin`

`index = 0;`

`while` `(index < BSIZE) {`

`if` `(buf[index] == 0) {`

`int` `tempo = myrand(BSIZE * 3);`

`printf("Job %d has been produced\n", tempo);`

`buf[index] = tempo;`

`break;`

`}`

`index++;`

`}`

`if` `(index == BSIZE)`

`printf("Buffer is full, nothing can be produced!!!\n");`

`printf("Buffer: ");`

`index = 0;`

`while` `(index < BSIZE)`

`printf("%d ", buf[index++]);`

`printf("\n");`

`// Critical Section End`

`flag[j] = false;`

`if` `(*state == 0)`

`break;`

`wait_time = myrand(PWT);`

`printf("Producer will wait for %d seconds\n\n", wait_time);`

`sleep(wait_time);`

`}`

`exit(0);`

`}`

`if` `(fork() == 0) // Consumer code`

`{`

`SHM1 = (bool*)shmat(shmid1, NULL, 0);`

`SHM2 = (int*)shmat(shmid2, NULL, 0);`

`SHM3 = (int*)shmat(shmid3, NULL, 0);`

`if` `(SHM1 == (bool*)-1 || SHM2 == (int*)-1 || SHM3 == (int*)-1) {`

`perror("Consumer shmat error:");`

`exit(1);`

`}`

`bool* flag = SHM1;`

`int* turn = SHM2;`

`int* buf = SHM3;`

`int` `index = 0;`

`flag[i] = false;`

`sleep(5);`

`while` `(*state == 1) {`

`flag[i] = true;`

`printf("Consumer is ready now.\n\n");`

`*turn = j;`

`while` `(flag[j] == true` `&& *turn == j)`

`;`

`// Critical Section Begin`

`if` `(buf[0] != 0) {`

`printf("Job %d has been consumed\n", buf[0]);`

`buf[0] = 0;`

`index = 1;`

`while` `(index < BSIZE) // Shifting remaining jobs forward`

`{`

`buf[index - 1] = buf[index];`

`index++;`

`}`

`buf[index - 1] = 0;`

`} else`

`printf("Buffer is empty, nothing can be consumed!!!\n");`

`printf("Buffer: ");`

`index = 0;`

`while` `(index < BSIZE)`

`printf("%d ", buf[index++]);`

`printf("\n");`

`// Critical Section End`

`flag[i] = false;`

`if` `(*state == 0)`

`break;`

`wait_time = myrand(CWT);`

`printf("Consumer will sleep for %d seconds\n\n", wait_time);`

`sleep(wait_time);`

`}`

`exit(0);`

`}`

`// Parent process will now for RT seconds before causing child to terminate`

`while` `(1) {`

`gettimeofday(&t, NULL);`

`t2 = t.tv_sec;`

`if` `(t2 - t1 > RT) // Program will exit after RT seconds`

`{`

`*state = 0;`

`break;`

`}`

`}`

`// Waiting for both processes to exit`

`wait();`

`wait();`

`printf("The clock ran out.\n");`

`return` `0;`

`}`

Output:

```
Producer is ready now.

Job 9 has been produced
Buffer: 9 0 0 0 0 0 0 0
Producer will wait for 1 seconds

Producer is ready now.

Job 8 has been produced
Buffer: 9 8 0 0 0 0 0 0
Producer will wait for 2 seconds

Producer is ready now.

Job 13 has been produced
Buffer: 9 8 13 0 0 0 0 0
Producer will wait for 1 seconds

Producer is ready now.

Job 23 has been produced
Buffer: 9 8 13 23 0 0 0 0
Producer will wait for 1 seconds

Consumer is ready now.

Job 9 has been consumed
Buffer: 8 13 23 0 0 0 0 0
Consumer will sleep for 9 seconds

Producer is ready now.

Job 15 has been produced
Buffer: 8 13 23 15 0 0 0 0
Producer will wait for 1 seconds

Producer is ready now.

Job 13 has been produced
Buffer: 8 13 23 15 13 0 0 0
Producer will wait for 1 seconds

Producer is ready now.

Job 11 has been produced
Buffer: 8 13 23 15 13 11 0 0
Producer will wait for 1 seconds

Producer is ready now.

Job 22 has been produced
Buffer: 8 13 23 15 13 11 22 0
Producer will wait for 2 seconds

Producer is ready now.

Job 23 has been produced
Buffer: 8 13 23 15 13 11 22 23
Producer will wait for 1 seconds

The clock ran out.

```

This article is contributed by **Nabaneet Roy**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

*** Dekker’s algorithm in Process Synchronization
Prerequisite – [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/) To
 obtain such a mutual exclusion, bounded waiting, and progress there 
have been several algorithms implemented, one of which is Dekker’s 
Algorithm. To understand the algorithm let’s understand the solution to 
the critical section problem first. A process is generally represented as : 

```
do {
    //entry section
        critical section
    //exit section
        remainder section
} while (TRUE);
```

The solution to the critical section problem must ensure the following three conditions: 

1. Mutual Exclusion
2. Progress
3. Bounded Waiting

One of the solutions for ensuring above all factors is [Peterson’s solution](https://www.geeksforgeeks.org/petersons-algorithm-for-mutual-exclusion-set-1/).Another one is **Dekker’s Solution**.
 Dekker’s algorithm was the first probably-correct solution to the 
critical section problem. It allows two threads to share a single-use 
resource without conflict, using only shared memory for communication. 
It avoids the strict alternation of a naïve turn-taking algorithm, and 
was one of the first mutual exclusion algorithms to be invented.Although
 there are many versions of Dekker’s Solution, the final or 5th version 
is the one that satisfies all of the above conditions and is the most 
efficient of them all. **Note –** Dekker’s Solution, 
mentioned here, ensures mutual exclusion between two processes only, it 
could be extended to more than two processes with the proper use of 
arrays and variables.**Algorithm –** It requires both an array of Boolean values and an integer variable:

```
var flag: array [0..1] of boolean;
turn: 0..1;
repeat

        flag[i] := true;
        while flag[j] do
                if turn = j then
                begin
                        flag[i] := false;
                        while turn = j do no-op;
                        flag[i] := true;
                end;

                critical section

        turn := j;
        flag[i] := false;

                remainder section

until false;
```

**First Version of Dekker’s Solution –**
 The idea is to use a common or shared thread number between processes 
and stop the other process from entering its critical section if the 
shared thread indicates the former one already running.

`Main()`

`{`

`int` `thread_number = 1;`

`startThreads();`

`}`

`Thread1()`

`{`

`do` `{`

`// entry section`

`// wait until threadnumber is 1`

`while` `(threadnumber == 2)`

`;`

`// critical section`

`// exit section`

`// give access to the other thread`

`threadnumber = 2;`

`// remainder section`

`} while` `(completed == false)`

`}`

`Thread2()`

`{`

`do` `{`

`// entry section`

`// wait until threadnumber is 2`

`while` `(threadnumber == 1)`

`;`

`// critical section`

`// exit section`

`// give access to the other thread`

`threadnumber = 1;`

`// remainder section`

`} while` `(completed == false)`

`}`

The
 problem arising in the above implementation is lockstep 
synchronization, i.e each thread depends on the other for its execution.
 If one of the processes completes, then the second process runs, gives 
access to the completed one, and waits for its turn, however, the former
 process is already completed and would never run to return the access 
back to the latter one. Hence, the second process waits infinitely then.**Second Version of Dekker’s Solution –**
 To remove lockstep synchronization, it uses two flags to indicate its 
current status and updates them accordingly at the entry and exit 
section.

`Main()`

`{`

`// flags to indicate if each thread is in`

`// its critical section or not.`

`boolean thread1 = false;`

`boolean thread2 = false;`

`startThreads();`

`}`

`Thread1()`

`{`

`do` `{`

`// entry section`

`// wait until thread2 is in its critical section`

`while` `(thread2 == true)`

`;`

`// indicate thread1 entering its critical section`

`thread1 = true;`

`// critical section`

`// exit section`

`// indicate thread1 exiting its critical section`

`thread1 = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

`Thread2()`

`{`

`do` `{`

`// entry section`

`// wait until thread1 is in its critical section`

`while` `(thread1 == true)`

`;`

`// indicate thread2 entering its critical section`

`thread2 = true;`

`// critical section`

`// exit section`

`// indicate thread2 exiting its critical section`

`thread2 = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

The
 problem arising in the above version is mutual exclusion itself. If 
threads are preempted (stopped) during flag updation ( i.e during 
current_thread = true ) then, both the threads enter their critical 
section once the preempted thread is restarted, also the same can be 
observed at the start itself, when both the flags are false.**Third Version of Dekker’s Solution –** To re-ensure mutual exclusion, it sets the flags before the entry section itself.

`Main()`

`{`

`// flags to indicate if each thread is in`

`// queue to enter its critical section`

`boolean thread1wantstoenter = false;`

`boolean thread2wantstoenter = false;`

`startThreads();`

`}`

`Thread1()`

`{`

`do` `{`

`thread1wantstoenter = true;`

`// entry section`

`// wait until thread2 wants to enter`

`// its critical section`

`while` `(thread2wantstoenter == true)`

`;`

`// critical section`

`// exit section`

`// indicate thread1 has completed`

`// its critical section`

`thread1wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

`Thread2()`

`{`

`do` `{`

`thread2wantstoenter = true;`

`// entry section`

`// wait until thread1 wants to enter`

`// its critical section`

`while` `(thread1wantstoenter == true)`

`;`

`// critical section`

`// exit section`

`// indicate thread2 has completed`

`// its critical section`

`thread2wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

The
 problem with this version is a deadlock possibility. Both threads could
 set their flag as true simultaneously and both will wait infinitely 
later on.**Fourth Version of Dekker’s Solution –** Uses small time interval to recheck the condition, eliminates deadlock, and ensures mutual exclusion as well.

`Main()`

`{`

`// flags to indicate if each thread is in`

`// queue to enter its critical section`

`boolean thread1wantstoenter = false;`

`boolean thread2wantstoenter = false;`

`startThreads();`

`}`

`Thread1()`

`{`

`do` `{`

`thread1wantstoenter = true;`

`while` `(thread2wantstoenter == true) {`

`// gives access to other thread`

`// wait for random amount of time`

`thread1wantstoenter = false;`

`thread1wantstoenter = true;`

`}`

`// entry section`

`// wait until thread2 wants to enter`

`// its critical section`

`// critical section`

`// exit section`

`// indicate thread1 has completed`

`// its critical section`

`thread1wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

`Thread2()`

`{`

`do` `{`

`thread2wantstoenter = true;`

`while` `(thread1wantstoenter == true) {`

`// gives access to other thread`

`// wait for random amount of time`

`thread2wantstoenter = false;`

`thread2wantstoenter = true;`

`}`

`// entry section`

`// wait until thread1 wants to enter`

`// its critical section`

`// critical section`

`// exit section`

`// indicate thread2 has completed`

`// its critical section`

`thread2wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

The
 problem with this version is the indefinite postponement. Also, a 
random amount of time is erratic depending upon the situation in which 
the algorithm is being implemented, hence not an acceptable solution in 
business critical systems.**Dekker’s Algorithm: Final and completed Solution –**
 -Idea is to use favoured thread notion to determine entry to the 
critical section. Favoured thread alternates between the thread 
providing mutual exclusion and avoiding deadlock, indefinite 
postponement, or lockstep synchronization.

`Main()`

`{`

`// to denote which thread will enter next`

`int` `favouredthread = 1;`

`// flags to indicate if each thread is in`

`// queue to enter its critical section`

`boolean thread1wantstoenter = false;`

`boolean thread2wantstoenter = false;`

`startThreads();`

`}`

`Thread1()`

`{`

`do` `{`

`thread1wantstoenter = true;`

`// entry section`

`// wait until thread2 wants to enter`

`// its critical section`

`while` `(thread2wantstoenter == true) {`

`// if 2nd thread is more favored`

`if` `(favaouredthread == 2) {`

`// gives access to other thread`

`thread1wantstoenter = false;`

`// wait until this thread is favored`

`while` `(favouredthread == 2)`

`;`

`thread1wantstoenter = true;`

`}`

`}`

`// critical section`

`// favor the 2nd thread`

`favouredthread = 2;`

`// exit section`

`// indicate thread1 has completed`

`// its critical section`

`thread1wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

`Thread2()`

`{`

`do` `{`

`thread2wantstoenter = true;`

`// entry section`

`// wait until thread1 wants to enter`

`// its critical section`

`while` `(thread1wantstoenter == true) {`

`// if 1st thread is more favored`

`if` `(favaouredthread == 1) {`

`// gives access to other thread`

`thread2wantstoenter = false;`

`// wait until this thread is favored`

`while` `(favouredthread == 1)`

`;`

`thread2wantstoenter = true;`

`}`

`}`

`// critical section`

`// favour the 1st thread`

`favouredthread = 1;`

`// exit section`

`// indicate thread2 has completed`

`// its critical section`

`thread2wantstoenter = false;`

`// remainder section`

`} while` `(completed == false)`

`}`

This version guarantees a complete solution to the critical solution problem.**References –**

[Dekker’s Algorithm -csisdmz.ul.ie](http://garryowen.csisdmz.ul.ie/~cs4023/resources/oth9.pdf)

[Dekker’s algorithm – Wikipedia](https://en.wikipedia.org/wiki/Dekker%27s_algorithm)
*** Bakery Algorithm in Process Synchronization
Prerequisite – [Critical Section](https://www.geeksforgeeks.org/g-fact-70/), [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/)

The **Bakery algorithm**
 is one of the simplest known solutions to the mutual exclusion problem 
for the general case of N process. Bakery Algorithm is a critical 
section solution for **N** processes. The algorithm preserves the first come first serve property.

- Before entering its critical section, the process receives a number. Holder of the smallest number enters the critical section.
- If processes Pi and Pj receive the same number,
    
    ```
    if i < j
    Pi is served first;
    else
    Pj is served first.
    ```
    
- The numbering scheme always generates numbers in increasing order of enumeration; i.e., 1, 2, 3, 3, 3, 3, 4, 5, …

**Notation –**
 lexicographical order (ticket #, process id #) – Firstly the ticket 
number is compared. If same then the process ID is compared next, i.e.-

```
– (a, b) < (c, d) if a < c or if a = c and b < d
– max(a [0], . . ., a [n-1]) is a number, k, such that k >= a[i]  for i = 0, . . ., n - 1
```

Shared
 data – choosing is an array [0..n – 1] of boolean values; & number 
is an array [0..n – 1] of integer values. Both are initialized to **False & Zero** respectively.

**Algorithm Pseudocode –**

```
repeat
    choosing[i] := true;
    number[i] := max(number[0], number[1], ..., number[n - 1])+1;
    choosing[i] := false;
    for j := 0 to n - 1
        do begin
            while choosing[j] do no-op;
            while number[j] != 0
                and (number[j], j) < (number[i], i) do no-op;
        end;

        critical section

    number[i] := 0;

        remainder section

until false;

```

**Explanation –**Firstly the process sets its 
“choosing” variable to be TRUE indicating its intent to enter critical 
section. Then it gets assigned the highest ticket number corresponding 
to other processes. Then the “choosing” variable is set to FALSE 
indicating that it now has a new ticket number. This is in-fact the most
 important and confusing part of the algorithm.

It is actually a 
small critical section in itself ! The very purpose of the first three 
lines is that if a process is modifying its TICKET value then at that 
time some other process should not be allowed to check its old ticket 
value which is now obsolete. This is why inside the for loop before 
checking ticket value we first make sure that all other processes have 
the “choosing” variable as FALSE.

After that we proceed to check 
the ticket values of processes where process with least ticket 
number/process id gets inside the critical section. The exit section 
just resets the ticket value to zero.

**Code –** Here’s the C code implementation of the Bakery Algorithm. Run the following in a **UNIX environment** –

## [Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// Importing the thread library`

`#include "pthread.h"`

`#include "stdio.h"`

`// Importing POSIX Operating System API library`

`#include "unistd.h"`

`#include "string.h"`

`// This is a memory barrier instruction.`

`// Causes compiler to enforce an ordering`

`// constraint on memory operations.`

`// This means that operations issued prior`

`// to the barrier will be performed`

`// before operations issued after the barrier.`

`#define MEMBAR __sync_synchronize()`

`#define THREAD_COUNT 8`

`volatile` `int` `tickets[THREAD_COUNT];`

`volatile` `int` `choosing[THREAD_COUNT];`

`// VOLATILE used to prevent the compiler`

`// from applying any optimizations.`

`volatile` `int` `resource;`

`void` `lock(int` `thread)`

`{`

`// Before getting the ticket number`

`//"choosing" variable is set to be true`

`choosing[thread] = 1;`

`MEMBAR;`

`// Memory barrier applied`

`int` `max_ticket = 0;`

`// Finding Maximum ticket value among current threads`

`for` `(int` `i = 0; i < THREAD_COUNT; ++i) {`

`int` `ticket = tickets[i];`

`max_ticket = ticket > max_ticket ? ticket : max_ticket;`

`}`

`// Allotting a new ticket value as MAXIMUM + 1`

`tickets[thread] = max_ticket + 1;`

`MEMBAR;`

`choosing[thread] = 0;`

`MEMBAR;`

`// The ENTRY Section starts from here`

`for` `(int` `other = 0; other < THREAD_COUNT; ++other) {`

`// Applying the bakery algorithm conditions`

`while` `(choosing[other]) {`

`}`

`MEMBAR;`

`while` `(tickets[other] != 0 && (tickets[other]`

`< tickets[thread]`

`|| (tickets[other]`

`== tickets[thread]`

`&& other < thread))) {`

`}`

`}`

`}`

`// EXIT Section`

`void` `unlock(int` `thread)`

`{`

`MEMBAR;`

`tickets[thread] = 0;`

`}`

`// The CRITICAL Section`

`void` `use_resource(int` `thread)`

`{`

`if` `(resource != 0) {`

`printf("Resource was acquired by %d, but is still in-use by %d!\n",`

`thread, resource);`

`}`

`resource = thread;`

`printf("%d using resource...\n", thread);`

`MEMBAR;`

`sleep(2);`

`resource = 0;`

`}`

`// A simplified function to show the implementation`

`void* thread_body(void* arg)`

`{`

`long` `thread` `= (long)arg;`

`lock(thread);`

`use_resource(thread);`

`unlock(thread);`

`return` `NULL;`

`}`

`int` `main(int` `argc, char** argv)`

`{`

`memset((void*)tickets, 0, sizeof(tickets));`

`memset((void*)choosing, 0, sizeof(choosing));`

`resource = 0;`

`// Declaring the thread variables`

`pthread_t threads[THREAD_COUNT];`

`for` `(int` `i = 0; i < THREAD_COUNT; ++i) {`

`// Creating a new thread with the function`

`//"thread_body" as its thread routine`

`pthread_create(&threads[i], NULL, &thread_body, (void*)((long)i));`

`}`

`for` `(int` `i = 0; i < THREAD_COUNT; ++i) {`

`// Reaping the resources used by`

`// all threads once their task is completed !`

`pthread_join(threads[i], NULL);`

`}`

`return` `0;`

`}`

Output:

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-12-23-04-15-36.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

*** Producer Consumer Problem using Semaphores | Set 1
**Prerequisite –** [Semaphores in operating system](https://www.geeksforgeeks.org/semaphores-operating-system/), [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/) Producer consumer problem is a classical synchronization problem. We can solve this problem by using semaphores.

A **[semaphore](https://www.geeksforgeeks.org/semaphores-in-process-synchronization/)** S is an integer variable that can be accessed only through two standard operations : wait() and signal(). The wait() operation reduces the value of semaphore by 1 and the signal() operation increases its value by 1.

```
wait(S){
while(S<=0);   // busy waiting
S--;
}

signal(S){
S++;
}
```

Semaphores are of two types:

1. **Binary Semaphore –** This is similar to mutex lock but not the same thing. It can have only
two values – 0 and 1. Its value is initialized to 1. It is used to
implement the solution of critical section problem with multiple
processes. 
2. **Counting Semaphore –** Its value can range over an unrestricted domain. It is used to control access to a resource that has multiple instances. 

**Problem Statement –**
 We have a buffer of fixed size. A producer can produce an item and can 
place in the buffer. A consumer can pick items and can consume them. We 
need to ensure that when a producer is placing an item in the buffer, 
then at the same time consumer should not consume any item. In this 
problem, buffer is the critical section.

To solve this problem, 
we need two counting semaphores – Full and Empty. “Full” keeps track of 
number of items in the buffer at any given time and “Empty” keeps track 
of number of unoccupied slots.

**Initialization of semaphores –** mutex = 1 Full = 0 // Initially, all slots are empty. Thus full slots are 0 Empty = n // All slots are empty initially

**Solution for Producer –**

```
do{

//produce an item

wait(empty);
wait(mutex);

//place in buffer

signal(mutex);
signal(full);

}while(true)
```

When producer produces an item then the value of 
“empty” is reduced by 1 because one slot will be filled now. The value 
of mutex is also reduced to prevent consumer to access the buffer. Now, 
the producer has placed the item and thus the value of “full” is 
increased by 1. The value of mutex is also increased by 1 because the 
task of producer has been completed and consumer can access the buffer.

**Solution for Consumer –**

```
do{

wait(full);
wait(mutex);

// remove item from buffer

signal(mutex);
signal(empty);

// consumes item

}while(true)
```

As the consumer is removing an item from buffer, 
therefore the value of “full” is reduced by 1 and the value is mutex is 
also reduced so that the producer cannot access the buffer at this 
moment. Now, the consumer has consumed the item, thus increasing the 
value of “empty” by 1. The value of mutex is also increased so that 
producer can access the buffer now.

See for implementation – [Producer-Consumer solution using Semaphores in Java | Set 2](https://www.geeksforgeeks.org/producer-consumer-solution-using-semaphores-java/)
*** Dining Philosopher Problem Using Semaphores
**Prerequisite –** [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Semaphores](https://www.geeksforgeeks.org/semaphores-operating-system/), [Dining-Philosophers Solution Using Monitors](https://www.geeksforgeeks.org/dining-philosophers-solution-using-monitors/)**The Dining Philosopher Problem –**
 The Dining Philosopher Problem states that K philosophers seated around
 a circular table with one chopstick between each pair of philosophers. 
There is one chopstick between each philosopher. A philosopher may eat 
if he can pick up the two chopsticks adjacent to him. One chopstick may 
be picked up by any one of its adjacent followers but not both. 

!https://media.geeksforgeeks.org/wp-content/uploads/dining_philosopher_problem.png

**Semaphore Solution to Dining Philosopher –**Each philosopher is represented by the following pseudocode: 

```
process P[i]
 while true do
   {  THINK;
      PICKUP(CHOPSTICK[i], CHOPSTICK[i+1 mod 5]);
      EAT;
      PUTDOWN(CHOPSTICK[i], CHOPSTICK[i+1 mod 5])
   }
```

There are three states of the philosopher: **THINKING, HUNGRY, and EATING**.
 Here there are two semaphores: Mutex and a semaphore array for the 
philosophers. Mutex is used such that no two philosophers may access the
 pickup or putdown at the same time. The array is used to control the 
behavior of each philosopher. But, semaphores can result in deadlock due
 to programming errors.**Code –** 

`#include <pthread.h>`

`#include <semaphore.h>`

`#include <stdio.h>`

`#define N 5`

`#define THINKING 2`

`#define HUNGRY 1`

`#define EATING 0`

`#define LEFT (phnum + 4) % N`

`#define RIGHT (phnum + 1) % N`

`int` `state[N];`

`int` `phil[N] = { 0, 1, 2, 3, 4 };`

`sem_t mutex;`

`sem_t S[N];`

`void` `test(int` `phnum)`

`{`

`if` `(state[phnum] == HUNGRY`

`&& state[LEFT] != EATING`

`&& state[RIGHT] != EATING) {`

`// state that eating`

`state[phnum] = EATING;`

`sleep(2);`

`printf("Philosopher %d takes fork %d and %d\n",`

`phnum + 1, LEFT + 1, phnum + 1);`

`printf("Philosopher %d is Eating\n", phnum + 1);`

`// sem_post(&S[phnum]) has no effect`

`// during takefork`

`// used to wake up hungry philosophers`

`// during putfork`

`sem_post(&S[phnum]);`

`}`

`}`

`// take up chopsticks`

`void` `take_fork(int` `phnum)`

`{`

`sem_wait(&mutex);`

`// state that hungry`

`state[phnum] = HUNGRY;`

`printf("Philosopher %d is Hungry\n", phnum + 1);`

`// eat if neighbours are not eating`

`test(phnum);`

`sem_post(&mutex);`

`// if unable to eat wait to be signalled`

`sem_wait(&S[phnum]);`

`sleep(1);`

`}`

`// put down chopsticks`

`void` `put_fork(int` `phnum)`

`{`

`sem_wait(&mutex);`

`// state that thinking`

`state[phnum] = THINKING;`

`printf("Philosopher %d putting fork %d and %d down\n",`

`phnum + 1, LEFT + 1, phnum + 1);`

`printf("Philosopher %d is thinking\n", phnum + 1);`

`test(LEFT);`

`test(RIGHT);`

`sem_post(&mutex);`

`}`

`void* philosopher(void* num)`

`{`

`while` `(1) {`

`int* i = num;`

`sleep(1);`

`take_fork(*i);`

`sleep(0);`

`put_fork(*i);`

`}`

`}`

`int` `main()`

`{`

`int` `i;`

`pthread_t thread_id[N];`

`// initialize the semaphores`

`sem_init(&mutex, 0, 1);`

`for` `(i = 0; i < N; i++)`

`sem_init(&S[i], 0, 0);`

`for` `(i = 0; i < N; i++) {`

`// create philosopher processes`

`pthread_create(&thread_id[i], NULL,`

`philosopher, &phil[i]);`

`printf("Philosopher %d is thinking\n", i + 1);`

`}`

`for` `(i = 0; i < N; i++)`

`pthread_join(thread_id[i], NULL);`

`}`

**Note –** The below program may compile only with C compilers with semaphore and pthread library.**References –** [Solution of Dining Philosophers – cs.gordon.edu](http://cs.gordon.edu/courses/cs322/lectures/transparencies/dining_phil.html) [Solution of Dining Philosophers – cs.indiana.edu](https://www.cs.indiana.edu/classes/p415-sjoh/hw/project/dining-philosophers/index.htm)
*** Dining-Philosophers Solution Using Monitors
Prerequisite: [Monitor](https://www.geeksforgeeks.org/monitors/), [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/)

**Dining-Philosophers Problem –** N philosophers seated around a circular table

!https://media.geeksforgeeks.org/wp-content/uploads/operating_system_din.png

- There is one chopstick between each philosopher
- A philosopher must pick up its two nearest chopsticks in order to eat
- A philosopher must pick up first one chopstick, then the second one, not both at once

We
 need an algorithm for allocating these limited resources(chopsticks) 
among several processes(philosophers) such that solution is free from 
deadlock and free from starvation.

There exist some algorithm to 
solve Dining – Philosopher Problem, but they may have deadlock 
situation. Also, a deadlock-free solution is not necessarily 
starvation-free. Semaphores can result in deadlock due to programming 
errors. Monitors alone are not sufficiency to solve this, we need 
monitors with *condition variables*

**Monitor-based Solution to Dining Philosophers**

We
 illustrate monitor concepts by presenting a deadlock-free solution to 
the dining-philosophers problem. Monitor is used to control access to 
state variables and condition variables. It only tells when to enter and
 exit the segment. This solution imposes the restriction that a 
philosopher may pick up her chopsticks only if both of them are 
available.

To code this solution, we need to distinguish among 
three states in which we may find a philosopher. For this purpose, we 
introduce the following data structure:

**THINKING –** When philosopher doesn’t want to gain access to either fork.

**HUNGRY –** When philosopher wants to enter the critical section.

**EATING –** When philosopher has got both the forks, i.e., he has entered the section.

Philosopher i can set the variable state[i] = EATING only if her two neighbors are not eating(state[(i+4) % 5] != EATING) and (state[(i+1) % 5] != EATING).

`// Dining-Philosophers Solution Using Monitors`

`monitor DP`

`{`

`status state[5];`

`condition self[5];`

`// Pickup chopsticks`

`Pickup(int` `i)`

`{`

`// indicate that I’m hungry`

`state[i] = hungry;`

`// set state to eating in test()`

`// only if my left and right neighbors`

`// are not eating`

`test(i);`

`// if unable to eat, wait to be signaled`

`if` `(state[i] != eating)`

`self[i].wait;`

`}`

`// Put down chopsticks`

`Putdown(int` `i)`

`{`

`// indicate that I’m thinking`

`state[i] = thinking;`

`// if right neighbor R=(i+1)%5 is hungry and`

`// both of R’s neighbors are not eating,`

`// set R’s state to eating and wake it up by`

`// signaling R’s CV`

`test((i + 1) % 5);`

`test((i + 4) % 5);`

`}`

`test(int` `i)`

`{`

`if` `(state[(i + 1) % 5] != eating`

`&& state[(i + 4) % 5] != eating`

`&& state[i] == hungry) {`

`// indicate that I’m eating`

`state[i] = eating;`

`// signal() has no effect during Pickup(),`

`// but is important to wake up waiting`

`// hungry philosophers during Putdown()`

`self[i].signal();`

`}`

`}`

`init()`

`{`

`// Execution of Pickup(), Putdown() and test()`

`// are all mutually exclusive,`

`// i.e. only one at a time can be executing`

`for`

`i = 0 to 4`

`// Verify that this monitor-based solution is`

`// deadlock free and mutually exclusive in that`

`// no 2 neighbors can eat simultaneously`

`state[i] = thinking;`

`}`

`} // end of monitor`

Above Program is a monitor solution to the dining-philosopher problem.

We also need to declare

```
condition self[5];
```

This
 allows philosopher i to delay herself when she is hungry but is unable 
to obtain the chopsticks she needs. We are now in a position to describe
 our solution to the dining-philosophers problem. The distribution of 
the chopsticks is controlled by the monitor Dining Philosophers. Each 
philosopher, before starting to eat, must invoke the operation pickup().
 This act may result in the suspension of the philosopher process. After
 the successful completion of the operation, the philosopher may eat. 
Following this, the philosopher invokes the putdown() operation. Thus, 
philosopher i must invoke the operations pickup() and putdown() in the 
following sequence:

```
DiningPhilosophers.pickup(i);
              ...
              eat
              ...
DiningPhilosophers.putdown(i);

```

It is easy to show that this solution ensures that **no two neighbors**
 are eating simultaneously and that no deadlocks will occur. We note, 
however, that it is possible for a philosopher to starve to death.
*** Readers-Writers Problem | Set 1 (Introduction and Readers Preference Solution)
Consider a situation where we have a file shared between many people. 

- If one of the people tries editing the file, no other person should be
reading or writing at the same time, otherwise changes will not be
visible to him/her.
- However if some person is reading the file, then others may read it at the same time.

Precisely in OS we call this situation as the **readers-writers problem**

Problem parameters: 

- One set of data is shared among a number of processes
- Once a writer is ready, it performs its write. Only one writer may write at a time
- If a process is writing, no other process can read it
- If at least one reader is reading, no other process can write
- Readers may not write and only read

**Solution when Reader has the Priority over Writer**

Here priority means, no reader should wait if the share is currently opened for reading.

Three variables are used: **mutex, wrt, readcnt** to implement solution 

1. **semaphore** mutex, wrt; // semaphore **mutex** is used to ensure mutual exclusion when **readcnt** is updated i.e. when any reader enters or exit from the critical section and semaphore **wrt** is used by both readers and writers
2. **int** readcnt; // **readcnt** tells the number of processes performing read in the critical section, initially 0

**Functions for semaphore :**

– wait() : decrements the semaphore value.

– signal() : increments the semaphore value.

**Writer process:** 

1. Writer requests the entry to critical section.
2. If allowed i.e. wait() gives a true value, it enters and performs the write. If not allowed, it keeps on waiting.
3. It exits the critical section.

```
do {
    // writer requests for critical section
    wait(wrt);

    // performs the write

    // leaves the critical section
    signal(wrt);

} while(true);
```

**Reader process:** 

1. Reader requests the entry to critical section.
2. If allowed:
    - it increments the count of number of readers inside the critical section.
    If this reader is the first reader entering, it locks the **wrt** semaphore to restrict the entry of writers if any reader is inside.
    - It then, signals mutex as any other reader is allowed to enter while others are already reading.
    - After performing reading, it exits the critical section. When exiting, it
    checks if no more reader is inside, it signals the semaphore “wrt” as
    now, writer can enter the critical section.
3. If not allowed, it keeps on waiting.

```
do {

   // Reader wants to enter the critical section
   wait(mutex);

   // The number of readers has now increased by 1
   readcnt++;

   // there is atleast one reader in the critical section
// this ensure no writer can enter if there is even one reader// thus we give preference to readers here
   if (readcnt==1)
      wait(wrt);

   // other readers can enter while this current reader is inside
   // the critical section
   signal(mutex);

   // current reader performs reading here
   wait(mutex);   // a reader wants to leave

   readcnt--;

   // that is, no reader is left in the critical section,
   if (readcnt == 0)
       signal(wrt);         // writers can enter

   signal(mutex); // reader leaves

} while(true);
```

Thus, the semaphore ‘**wrt**‘ is 
queued on both readers and writers in a manner such that preference is 
given to readers if writers are also there. Thus, no reader is waiting 
simply because a writer has requested to enter the critical section.

Article contributed by [Ekta Goel](https://www.linkedin.com/pub/ekta-goel/75/12a/3a6). Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Reader-Writers solution using Monitors ( set 2 )
Prerequisite – [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Monitors](https://www.geeksforgeeks.org/monitors/), [Readers-Writers Problem](https://www.geeksforgeeks.org/readers-writers-problem-set-1-introduction-and-readers-preference-solution/)Considering a shared Database our objectives are:

- Readers can access database only when there are no writers.
- Writers can access database only when there are no readers or writers.
- Only one thread can manipulate the state variables at a time.

**Basic structure of a solution –**

```
Reader()
   Wait until no writers
   Access database
   Check out – wake up a waiting writer
Writer()
   Wait until no active readers or writers
   Access database
   Check out – wake up waiting readers or writer

```

–Now let’s suppose that a writer is active and a mixture of readers and writers now show up.**Who should get in next?**–Or suppose that a writer is waiting and an endless of stream of readers keep showing up.**Would it be fair for them to become active?**So we’ll implement a kind of back-and-forth form of fairness:

- **Once a reader is waiting, readers will get in next.**
- **If a writer is waiting, one writer will get in next.**

***Implementation of the solution using monitors:-***

1. The methods should be executed with mutual exclusion i.e. At each point in
time, at most one thread may be executing any of its methods.
2. Monitors also provide a mechanism for threads to temporarily give up exclusive
access, in order to wait for some condition to be met, before regaining
exclusive access and resuming their task.
3. Monitors also have a mechanism for signaling other threads that such conditions have been met.
4. So in this implementation only mutual exclusion is not enough. Threads
attempting an operation may need to wait until some assertion P holds
true.
5. While a thread is waiting upon a condition variable, that
thread is not considered to occupy the monitor, and so other threads may enter the monitor to change the monitor’s state.

## [Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

**Code –**

`// STATE VARIABLES`

`// Number of active readers; initially = 0`

`int` `NReaders = 0;`

`// Number of waiting readers; initially = 0`

`int` `WaitingReaders = 0;`

`// Number of active writers; initially = 0`

`int` `NWriters = 0;`

`// Number of waiting writers; initially = 0`

`int` `WaitingWriters = 0;`

`Condition canRead = NULL;`

`Condition canWrite = NULL;`

`Void BeginWrite()`

`{`

`// A writer can enter if there are no other`

`// active writers and no readers are waiting`

`if` `(NWriters == 1 || NReaders > 0) {`

`++WaitingWriters;`

`wait(CanWrite);`

`--WaitingWriters;`

`}`

`NWriters = 1;`

`}`

`Void EndWrite()`

`{`

`NWriters = 0;`

`// Checks to see if any readers are waiting`

`if` `(WaitingReaders)`

`Signal(CanRead);`

`else`

`Signal(CanWrite);`

`}`

`Void BeginRead()`

`{`

`// A reader can enter if there are no writers`

`// active or waiting, so we can have`

`// many readers active all at once`

`if` `(NWriters == 1 || WaitingWriters > 0) {`

`++WaitingReaders;`

`// Otherwise, a reader waits (maybe many do)`

`Wait(CanRead);`

`--WaitingReaders;`

`}`

`++NReaders;`

`Signal(CanRead);`

`}`

`Void EndRead()`

`{`

`// When a reader finishes, if it was the last reader,`

`// it lets a writer in (if any is there).`

`if` `(--NReaders == 0)`

`Signal(CanWrite);`

`}`

***Understanding the solution:-***

**It wants to be fair.**

1. If a writer is waiting, readers queue up.
2. If a reader (or another writer) is active or waiting, writers queue up.
3. This is mostly fair, although once it lets a reader in, it lets ALL waiting
readers in all at once, even if some showed up “after” other waiting
writers.
- **The code is “simplified” because we know there can only be one writer at a time.**
- **It also takes advantage of the fact that signal is a no-op if nobody is waiting.**
    1. In the “EndWrite” code (it signals CanWrite without checking for waiting writers)
    2. In the EndRead code (same thing)
    3. In StartRead (signals CanRead at the end)

> With
 Semaphores we never did have a “fair” solution of this sort. In fact it
 can be done but the code is quite tricky. Here the straightforward 
solution works in the desired way! Monitors are less error-prone and 
also easier to understand.
>
*** Sleeping Barber problem in Process Synchronization
Prerequisite – [Inter Process Communication](https://www.geeksforgeeks.org/inter-process-communication/)**Problem :**
 The analogy is based upon a hypothetical barber shop with one barber. 
There is a barber shop which has one barber, one barber chair, and n 
chairs for waiting for customers if there are any to sit on the chair.

- If there is no customer, then the barber sleeps in his own chair.
- When a customer arrives, he has to wake up the barber.
- If there are many customers and the barber is cutting a customer’s hair,
then the remaining customers either wait if there are empty chairs in
the waiting room or they leave if no chairs are empty.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-Diagram-25.png

**Solution :** The solution to this problem includes three [semaphores](https://www.geeksforgeeks.org/semaphores-operating-system/).First
 is for the customer which counts the number of customers present in the
 waiting room (customer in the barber chair is not included because he 
is not waiting). Second, the barber 0 or 1 is used to tell whether the 
barber is idle or is working, And the third mutex is used to provide the
 mutual exclusion which is required for the process to execute. In the 
solution, the customer has the record of the number of customers waiting
 in the waiting room if the number of customers is equal to the number 
of chairs in the waiting room then the upcoming customer leaves the 
barbershop.

When the barber shows up in the morning, he executes 
the procedure barber, causing him to block on the semaphore customers 
because it is initially 0. Then the barber goes to sleep until the first
 customer comes up.

When a customer arrives, he executes customer 
procedure the customer acquires the mutex for entering the critical 
region, if another customer enters thereafter, the second one will not 
be able to anything until the first one has released the mutex. The 
customer then checks the chairs in the waiting room if waiting customers
 are less then the number of chairs then he sits otherwise he leaves and
 releases the mutex.

If the chair is available then customer sits 
in the waiting room and increments the variable waiting value and also 
increases the customer’s semaphore this wakes up the barber if he is 
sleeping.

At this point, customer and barber are both awake and 
the barber is ready to give that person a haircut. When the haircut is 
over, the customer exits the procedure and if there are no customers in 
waiting room barber sleeps.

!https://media.geeksforgeeks.org/wp-content/uploads/abc-3.png

**Algorithm for Sleeping Barber problem:**

`Semaphore Customers = 0;`

`Semaphore Barber = 0;`

`Mutex Seats = 1;`

`int` `FreeSeats = N;`

`Barber {`

`while(true) {`

`/* waits for a customer (sleeps). */`

`down(Customers);`

`/* mutex to protect the number of available seats.*/`

`down(Seats);`

`/* a chair gets free.*/`

`FreeSeats++;`

`/* bring customer for haircut.*/`

`up(Barber);`

`/* release the mutex on the chair.*/`

`up(Seats);`

`/* barber is cutting hair.*/`

`}`

`}`

`Customer {`

`while(true) {`

`/* protects seats so only 1 customer tries to sit`

`in a chair if that's the case.*/`

`down(Seats); //This line should not be here.`

`if(FreeSeats > 0) {`

`/* sitting down.*/`

`FreeSeats--;`

`/* notify the barber. */`

`up(Customers);`

`/* release the lock */`

`up(Seats);`

`/* wait in the waiting room if barber is busy. */`

`down(Barber);`

`// customer is having hair cut`

`} else` `{`

`/* release the lock */`

`up(Seats);`

`// customer leaves`

`}`

`}`

`}`

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Lock Variable Synchronization Mechanism
**Prerequisites –** [Process Synchronization](https://www.geeksforgeeks.org/process-synchronization-set-1/)A
 lock variable provides the simplest synchronization mechanism for 
processes. Some noteworthy points regarding Lock Variables are-

1. Its a **software mechanism** implemented in user mode, i.e. no support required from the Operating System.
2. Its a busy waiting solution (keeps the CPU busy even when its technically waiting).
3. It can be used for more than two processes.

When Lock = 0 implies critical section is vacant (initial value ) and Lock = 1 implies critical section occupied.The pseudocode looks something like this –

```
Entry section - while(lock != 0);
                Lock = 1;
//critical section
Exit section - Lock = 0;
```

A more formal approach to the Lock Variable method for process synchronization can be seen in the following code snippet :

`char` `buffer[SIZE];`

`int` `count = 0,`

`start = 0,`

`end = 0;`

`struct` `lock l;`

`// initialize lock variable`

`lock_init(&l);`

`void` `put(char` `c)`

`{`

`// entry section`

`lock_acquire(&l);`

`// critical section begins`

`while` `(count == SIZE) {`

`lock_release(&l);`

`lock_acquire(&l);`

`}`

`count++;`

`buffer[start] = c;`

`start++;`

`if` `(start == SIZE) {`

`start = 0;`

`}`

`// critical section ends`

`// exit section`

`lock_release(&l);`

`}`

`char` `get()`

`{`

`char` `c;`

`// entry section`

`lock_acquire(&l);`

`// critical section begins`

`while` `(count == 0) {`

`lock_release(&l);`

`lock_acquire(&l);`

`}`

`count--;`

`c = buffer[end];`

`end++;`

`if` `(end == SIZE) {`

`end = 0;`

`}`

`// critical section ends`

`// exit section`

`lock_release(&l);`

`return` `c;`

`}`

Here
 we can see a classic implementation of the reader-writer’s problem. The
 buffer here is the shared memory and many processes are either trying 
to read or write a character to it. To prevent any ambiguity of data we 
restrict concurrent access by using a lock variable. We have also 
applied a constraint on the number of readers/writers that can have 
access.Now every Synchronization mechanism is judged on the basis of three primary parameters :

1. Mutual Exclusion.
2. Progress.
3. Bounded Waiting.

Of which mutual exclusion is the most **important**
 of all parameters. The Lock Variable doesn’t provide mutual exclusion 
in some cases. This fact can be best verified by writing its pseudo-code
 in the form of an assembly language code as given below.

```
1. Load Lock, R0 ; (Store the value of Lock in Register R0.)
2. CMP R0, #0 ; (Compare the value of register R0 with 0.)
3. JNZ Step 1 ; (Jump to step 1 if value of R0 is not 0.)
4. Store #1, Lock ; (Set new value of Lock as 1.)
Enter critical section
5. Store #0, Lock ; (Set the value of lock as 0 again.)
```

Now 
let’s suppose that processes P1 and P2 are competing for Critical 
Section and their sequence of execution be as follows (initial value of 
Lock = 0) –

1. P1 executes statement 1 and gets pre-empted.
2. P2 executes statement 1, 2, 3, 4 and enters Critical Section and gets pre-empted.
3. P1 executes statement 2, 3, 4 and also enters Critical Section.

Here
 initially the R0 of process P1 stores lock value as 0 but fails to 
update the lock value as 1. So when P2 executes it also finds the LOCK 
value as 0 and enters Critical Section by setting LOCK value as 1. But 
the real problem arises when P1 executes again it doesn’t check the 
updated value of Lock. It only checks the previous value stored in R0 
which was 0 and it enters critical section.This is only one possible
 sequence of execution among many others. Some may even provide mutual 
exclusion but we cannot dwell on that. According to murphy’s law “**Anything that can go wrong will go wrong**“.
 So like all easy things the Lock Variable Synchronization method comes 
with its fair share of Demerits but its a good starting point for us to 
develop better Synchronization Algorithms to take care of the problems 
that we face here. This article is contributed by **[Siddhant Bajaj 2](https://auth.geeksforgeeks.org/profile.php?user=Siddhant%20Bajaj%202)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Mutex lock for Linux Thread Synchronization
**Prerequisite :** [Multithreading in C](https://www.geeksforgeeks.org/multithreading-c-2/)

**Thread synchronization**
 is defined as a mechanism which ensures that two or more concurrent 
processes or threads do not simultaneously execute some particular 
program segment known as a critical section. Processes’ access to 
critical section is controlled by using synchronization techniques. When
 one thread starts executing the [critical section](https://www.geeksforgeeks.org/g-fact-70/)
 (a serialized segment of the program) the other thread should wait 
until the first thread finishes. If proper synchronization techniques 
are not applied, it may cause a [race condition](https://practice.geeksforgeeks.org/problems/what-is-race-condition)
 where the values of variables may be unpredictable and vary depending 
on the timings of context switches of the processes or threads.

**Thread Synchronization Problems**An example code to study synchronization problems :

`#include <pthread.h>`

`#include <stdio.h>`

`#include <stdlib.h>`

`#include <string.h>`

`#include <unistd.h>`

`pthread_t tid[2];`

`int` `counter;`

`void* trythis(void* arg)`

`{`

`unsigned long` `i = 0;`

`counter += 1;`

`printf("\n Job %d has started\n", counter);`

`for` `(i = 0; i < (0xFFFFFFFF); i++)`

`;`

`printf("\n Job %d has finished\n", counter);`

`return` `NULL;`

`}`

`int` `main(void)`

`{`

`int` `i = 0;`

`int` `error;`

`while` `(i < 2) {`

`error = pthread_create(&(tid[i]), NULL, &trythis, NULL);`

`if` `(error != 0)`

`printf("\nThread can't be created : [%s]", strerror(error));`

`i++;`

`}`

`pthread_join(tid[0], NULL);`

`pthread_join(tid[1], NULL);`

`return` `0;`

`}`

**How to compile above program?**To
 compile a multithreaded program using gcc, we need to link it with the 
pthreads library. Following is the command used to compile the program.

```
gfg@ubuntu:~/$ gcc filename.c -lpthread

```

In this example, two threads(jobs) are created and in the start
 function of these threads, a counter is maintained to get the logs 
about job number which is started and when it is completed.

Output :

```
Job 1 has started
Job 2 has started
Job 2 has finished
Job 2 has finished

```

**Problem:** From the last two logs, one can see that the log ‘*Job 2 has finished*’ is repeated twice while no log for ‘*Job 1 has finished*’ is seen.

**Why it has occurred ?**On observing closely and visualizing the execution of the code, we can see that :

- The log ‘*Job 2 has started*’ is printed just after ‘*Job 1 has Started*’ so it can easily be concluded that while thread 1 was processing the scheduler scheduled the thread 2.
- If we take the above assumption true then the value of the ‘*counter*’ variable got incremented again before job 1 got finished.
- So, when Job 1 actually got finished, then the wrong value of counter produced the log ‘*Job 2 has finished*’ followed by the ‘*Job 2 has finished*’ for the actual job 2 or vice versa as it is dependent on scheduler.
- So we see that its not the repetitive log but the wrong value of the ‘counter’ variable that is the problem.
- The actual problem was the usage of the variable ‘counter’ by a second
thread when the first thread was using or about to use it.
- In
other words, we can say that lack of synchronization between the threads while using the shared resource ‘counter’ caused the problems or in one word we can say that this problem happened due to ‘Synchronization
problem’ between two threads.

**How to solve it ?**

The most popular way of achieving thread synchronization is by using **Mutexes**.

### Mutex

- A Mutex is a lock that we set before using a shared resource and release after using it.
- When the lock is set, no other thread can access the locked region of code.
- So we see that even if thread 2 is scheduled while thread 1 was not done
accessing the shared resource and the code is locked by thread 1 using
mutexes then thread 2 cannot even access that region of code.
- So this ensures synchronized access of shared resources in the code.

**Working of a mutex**

1. Suppose one thread has locked a region of code using mutex and is executing that piece of code.
2. Now if scheduler decides to do a context switch, then all the other threads which are ready to execute the same region are unblocked.
3. Only
one of all the threads would make it to the execution but if this thread tries to execute the same region of code that is already locked then it will again go to sleep.
4. Context switch will take place again
and again but no thread would be able to execute the locked region of
code until the mutex lock over it is released.
5. Mutex lock will only be released by the thread who locked it.
6. So this ensures that once a thread has locked a piece of code then no
other thread can execute the same region until it is unlocked by the
thread who locked it.

Hence, this system ensures synchronization among the threads while working on shared resources.

**A mutex is initialized and then a lock is achieved by calling the following two functions :** The first function initializes a mutex and through second function any critical region in the code can be locked.

1. **int pthread_mutex_init(pthread_mutex_t *restrict mutex, const pthread_mutexattr_t *restrict attr) :** Creates a mutex, referenced by mutex, with attributes specified by attr. If
attr is NULL, the default mutex attribute (NONRECURSIVE) is used.
    
    **Returned value**If successful, pthread_mutex_init() returns 0, and the state of the mutex becomes initialized and unlocked.If unsuccessful, pthread_mutex_init() returns -1.
    
2. **int pthread_mutex_lock(pthread_mutex_t *mutex) :** Locks a mutex object, which identifies a mutex. If the mutex is already locked by another thread, the thread waits for the mutex to become
available. The thread that has locked a mutex becomes its current owner
and remains the owner until the same thread has unlocked it. When the
mutex has the attribute of recursive, the use of the lock may be
different. When this kind of mutex is locked multiple times by the same
thread, then a count is incremented and no waiting thread is posted. The owning thread must call pthread_mutex_unlock() the same number of times to decrement the count to zero.
    
    **Returned value**If successful, pthread_mutex_lock() returns 0.If unsuccessful, pthread_mutex_lock() returns -1.
    

**The mutex can be unlocked and destroyed by calling following two functions :**The first function releases the lock and the second function destroys the lock so that it cannot be used anywhere in future.

1. **int pthread_mutex_unlock(pthread_mutex_t *mutex) :** Releases a mutex object. If one or more threads are waiting to lock the mutex, pthread_mutex_unlock() causes one of those threads to return
from pthread_mutex_lock() with the mutex object acquired. If no threads
are waiting for the mutex, the mutex unlocks with no current owner. When the mutex has the attribute of recursive the use of the lock may be
different. When this kind of mutex is locked multiple times by the same
thread, then unlock will decrement the count and no waiting thread is
posted to continue running with the lock. If the count is decremented to zero, then the mutex is released and if any thread is waiting for it is posted.
    
    **Returned value**If successful, pthread_mutex_unlock() returns 0.If unsuccessful, pthread_mutex_unlock() returns -1
    
2. **int pthread_mutex_destroy(pthread_mutex_t *mutex) :** Deletes a mutex object, which identifies a mutex. Mutexes are used to protect
shared resources. mutex is set to an invalid value, but can be
reinitialized using pthread_mutex_init().
    
    **Returned value**If successful, pthread_mutex_destroy() returns 0.If unsuccessful, pthread_mutex_destroy() returns -1.
    

!https://media.geeksforgeeks.org/wp-content/uploads/Mutex_lock_for_linux.jpg

`#include <pthread.h>`

`#include <stdio.h>`

`#include <stdlib.h>`

`#include <string.h>`

`#include <unistd.h>`

`pthread_t tid[2];`

`int` `counter;`

`pthread_mutex_t lock;`

`void* trythis(void* arg)`

`{`

`pthread_mutex_lock(&lock);`

`unsigned long` `i = 0;`

`counter += 1;`

`printf("\n Job %d has started\n", counter);`

`for` `(i = 0; i < (0xFFFFFFFF); i++)`

`;`

`printf("\n Job %d has finished\n", counter);`

`pthread_mutex_unlock(&lock);`

`return` `NULL;`

`}`

`int` `main(void)`

`{`

`int` `i = 0;`

`int` `error;`

`if` `(pthread_mutex_init(&lock, NULL) != 0) {`

`printf("\n mutex init has failed\n");`

`return` `1;`

`}`

`while` `(i < 2) {`

`error = pthread_create(&(tid[i]),`

`NULL,`

`&trythis, NULL);`

`if` `(error != 0)`

`printf("\nThread can't be created :[%s]",`

`strerror(error));`

`i++;`

`}`

`pthread_join(tid[0], NULL);`

`pthread_join(tid[1], NULL);`

`pthread_mutex_destroy(&lock);`

`return` `0;`

`}`

In the above code:

- A mutex is initialized in the beginning of the main function.
- The same mutex is locked in the ‘trythis()’ function while using the shared resource ‘counter’.
- At the end of the function ‘trythis()’ the same mutex is unlocked.
- At the end of the main function when both the threads are done, the mutex is destroyed.

Output :

```
Job 1 started
Job 1 finished
Job 2 started
Job 2 finished

```

So this time the start and finish logs of both the jobs are present. So thread synchronization took place by the use of Mutex.

References :[Synchronization (computer science)](https://en.wikipedia.org/wiki/Synchronization_(computer_science))[Lock (computer science)](https://en.wikipedia.org/wiki/Lock_(computer_science))

This article is contributed by **[Kishlay Verma](https://www.linkedin.com/in/kishlayverma/)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Priority Inversion
Let us first put ‘priority inversion’ in the context of the Big Picture i.e. where does this come from.

In
 Operating System, one of the important concepts is Task Scheduling. 
There are several Scheduling methods such as First Come First Serve, 
Round Robin, Priority-based scheduling, etc. Each scheduling method has 
its pros and cons. As you might have guessed, Priority Inversion comes 
under Priority-based Scheduling. Basically, it’s a problem which arises 
sometimes when Priority-based scheduling is used by OS. In 
Priority-based scheduling, different tasks are given different 
priorities so that higher priority tasks can intervene in lower priority
 tasks if possible.

So, in priority-based scheduling, if a lower 
priority task (L) is running and if a higher priority task (H) also 
needs to run, the lower priority task (L) would be preempted by a higher
 priority task (H). Now, suppose both lower and higher priority tasks 
need to share a common resource (say access to the same file or device) 
to achieve their respective work. In this case, since there are resource
 sharing and task synchronization is needed, several methods/techniques 
can be used for handling such scenarios. For sake of our topic on 
Priority Inversion, let us mention a synchronization method say mutex. 
Just to recap on the mutex, a task acquires mutex before entering the 
critical section (CS) and releases mutex after exiting the critical 
section (CS). While running in CS, tasks access this common resource. 
More details on this can be referred [here](https://www.geeksforgeeks.org/g-fact-70/). Now, say both L and H share a common Critical Section (CS) i.e. the same mutex is needed for this CS.

Coming to our discussion of priority inversion, let us examine some scenarios. 1)
 L is running but not in CS; H needs to run; H preempts L; H starts 
running; H relinquishes or releases control; L resumes and starts 
running 2) L is running in CS; H needs to run but not in CS; H 
preempts L; H starts running; H relinquishes control; L resumes and 
starts running. 3) L is running in CS; H also needs to run in CS; H 
waits for L to come out of CS; L comes out of CS; H enters CS and starts
 running

Please note that the above 
scenarios don’t show the problem of any Priority Inversion (not even 
scenario 3). Basically, so long as lower priority task isn’t running in 
shared CS, higher priority task can preempt it. But if L is running in 
shared CS and H also needs to run in CS, H waits until L comes out of 
CS. The idea is that CS should be small enough so that it doesn’t result
 in H waiting for a long time while L was in CS. That’s why writing a CS
 code requires careful consideration. In any of the above scenarios, 
priority inversion (i.e. reversal of priority) didn’t occur because the 
tasks are running as per the design.

Now
 let us add another task of middle priority say M. Now the task 
priorities are in the order of L < M < H. In our example, M 
doesn’t share the same Critical Section (CS). In this case, the 
following sequence of task running would result in a ‘Priority 
Inversion’ problem.

4) L is running in 
CS; H also needs to run in CS; H waits for L to come out of CS; M 
interrupts L and starts running; M runs till completion and relinquishes
 control; L resumes and starts running till the end of CS; H enters CS 
and starts running. Note that neither L nor H share CS with M.

Here,
 we can see that running of M has delayed the running of both L and H. 
Precisely speaking, H is of higher priority and doesn’t share CS with M;
 but H had to wait for M. This is where Priority-based scheduling didn’t
 work as expected because priorities of M and H got inverted in spite of
 not sharing any CS. This problem is called Priority Inversion. This is 
what the heck was Priority Inversion! In a system with priority-based 
scheduling, higher priority tasks can face this problem and it can 
result in unexpected behavior/result. In general purpose OS, it can 
result in slower performance. In RTOS, it can result in more severe 
outcomes. The most famous ‘Priority Inversion’ problem was what happened
 at Mars Pathfinder.

If we have a 
problem, there has to be a solution for this. For Priority Inversion as 
well, there’re different solutions such as Priority Inheritance, etc. 
This is going to be our next article

But for the inpatients, [this](https://www.geeksforgeeks.org/whats-difference-priority-inversion-priority-inheritance/) can be referred to for time being.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Difference between Priority Inversion and Priority Inheritance
Both of these concepts come under Priority scheduling in Operating System. But are they the same?

In one line, *Priority Inversion* is a **problem** while *Priority Inheritance* is a **solution**. *Priority Inversion* means that the priority of tasks gets inverted and *Priority Inheritance* means that the priority of tasks gets inherited. Both of these phenomena happen in priority scheduling. Basically, in *Priority Inversion*,
 the higher priority task (H) ends up waiting for the middle priority 
task (M) when H is sharing a critical section with the lower priority 
task (L) and L is already in the critical section. Effectively, H 
waiting for M results in inverted priority i.e. Priority Inversion. One 
of the solutions to this problem is *Priority Inheritance*. In *Priority Inheritance*,
 when L is in the critical section, L inherits the priority of H at the 
time when H starts pending for the critical section. By doing so, M 
doesn’t interrupt L and H doesn’t wait for M to finish. Please note that
 inheriting of priority is done temporarily i.e. L goes back to its old 
priority when L comes out of the critical section.

More details on these can be found [here](https://www.geeksforgeeks.org/priority-inversion-what-the-heck/).

**Let us see the differences in a tabular form:**

[](https://www.notion.so/21d1f11cb6494d0bad61ccfd90cfc9e6?pvs=21)
*** ProProcess Synchronization-set
Prerequisite – [Process Synchronization | Introduction](https://www.geeksforgeeks.org/process-synchronization-set-1/), [Critical Section](https://www.geeksforgeeks.org/g-fact-70/), [Semaphores](https://www.geeksforgeeks.org/semaphores-operating-system/)**Process Synchronization**
 is a technique which is used to coordinate the process that use shared 
Data. There are two types of Processes in an Operating Systems:-

1. **Cooperating Process –**The process that
affect or is affected by the other process while execution, is called a
Cooperating Process. Example The process that share file, variable,
database, etc are the Cooperating Process.

Process Synchronization is mainly used for Cooperating Process that shares the resources.Let us consider an example of//racing condition image

!https://media.geeksforgeeks.org/wp-content/uploads/Race-condition.png

It
 is the condition where several processes tries to access the resources 
and modify the shared data concurrently and outcome of the process 
depends on the particular order of execution that leads to data 
inconsistency, this condition is called **Race Condition**.This 
condition can be avoided using the technique called Synchronization or 
Process Synchronization, in which we allow only one process to enter and
 manipulates the shared data in Critical Section.//diagram of the view of CS

!https://media.geeksforgeeks.org/wp-content/uploads/CR_repr.png

This setup can be defined in various regions like:

- **Entry Section –**It is part of the process which decide the entry of a particular process in the Critical Section, out of many other processes.
- **Critical Section –**It is the part in which only one process is allowed to enter and modify
the shared variable.This part of the process ensures that only no other
process can access the resource of shared data.
- **Exit Section –**This process allows the other process that are waiting in the Entry Section, to enter into the Critical Sections. It checks that a process that
after a process has finished execution in Critical Section can be
removed through this Exit Section.
- **Remainder Section –**The other parts of the Code other than Entry Section, Critical Section and Exit Section are known as Remainder Section.

Critical Section problems must satisfy these three requirements:

1. **Mutual Exclusion –**It states that no other process is allowed to execute in the critical section if a process is executing in critical section.
2. **Progress –**When no process is in the critical section, then any process from outside
that request for execution can enter in the critical section without any delay. Only those process can enter that have requested and have finite time to enter the process.
3. **Bounded Waiting –**An upper bound must exist on the number of times a process enters so that other
processes are allowed to enter their critical sections after a process
has made a request to enter its critical section and before that request is granted.

Process Synchronization are handled by two approaches:

1. **Software Approach –**In Software Approach, Some specific Algorithm approach is used to maintain synchronization of the data. Like in Approach One or Approach Two, for a number of two process, a temporary variable like (turn) or boolean
variable (flag) value is used to store the data. When the condition is
True then the process in waiting State, known as Busy Waiting State.
This does not satisfy all the Critical Section requirements.
    
    Another 
    Software approach known as Peterson’s Solution is best for 
    Synchronization. It uses two variables in the Entry Section so as to 
    maintain consistency, like Flag (boolean variable) and Turn 
    variable(storing the process states). It satisfy all the three Critical 
    Section requirements.
    
    //Image of Peterson’s Algorithm
    
    !https://media.geeksforgeeks.org/wp-content/uploads/peterson-1.png
    
2. **Hardware Approach –**The Hardware Approach of synchronization can be done through Lock &
Unlock technique.Locking part is done in the Entry Section, so that only one process is allowed to enter into the Critical Section, after it
complete its execution, the process is moved to the Exit Section, where
Unlock Operation is done so that another process in the Lock Section can repeat this process of Execution.This process is designed in such a way that all the three conditions of the Critical Sections are satisfied.
    
    //Image of Lock
    
    !https://media.geeksforgeeks.org/wp-content/uploads/lock-2.png
    

**Using Interrupts –**These
 are easy to implement.When Interrupt are disabled then no other process
 is allowed to perform Context Switch operation that would allow only 
one process to enter into the Critical State.

//Image of Interrupts

!https://media.geeksforgeeks.org/wp-content/uploads/interrupt-1.png

**Test_and_Set Operation –**This
 allows boolean value (True/False) as a hardware Synchronization, which 
is atomic in nature i.e no other interrupt is allowed to access.This is 
mainly used in Mutual Exclusion Application. Similar type operation can 
be achieved through Compare and Swap function. In this process, a 
variable is allowed to accessed in Critical Section while its lock 
operation is ON.Till then, the other process is in Busy Waiting State. 
Hence Critical Section Requirements are achieved.
** Deadlock
*** Introduction of Deadlock in Operating System
A process in operating system uses resources in the following way. 1) Requests a resource 2) Use the resource 3) Releases the resource

***Deadlock*** is
 a situation where a set of processes are blocked because each process 
is holding a resource and waiting for another resource acquired by some 
other process. Consider an example when two trains are coming toward
 each other on the same track and there is only one track, none of the 
trains can move once they are in front of each other. A similar 
situation occurs in operating systems when there are two or more 
processes that hold some resources and wait for resources held by 
other(s). For example, in the below diagram, Process 1 is holding 
Resource 1 and waiting for resource 2 which is acquired by process 2, 
and process 2 is waiting for resource 1. 

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/deadlock.png

**Deadlock can arise if** the **following four conditions hold simultaneously (Necessary Conditions) *Mutual Exclusion:*** Two or more resources are non-shareable (Only one process can use at a time) ***Hold and Wait:*** A process is holding at least one resource and waiting for resources. ***No Preemption:*** A resource cannot be taken from a process unless the process releases the resource. ***Circular Wait:*** A set of processes are waiting for each other in circular form.

**Methods for handling deadlock** There are three ways to handle deadlock 1) Deadlock prevention or avoidance: The idea is to not let the system into a deadlock state. One
 can zoom into each category individually, Prevention is done by 
negating one of above mentioned necessary conditions for deadlock. Avoidance
 is kind of futuristic in nature. By using strategy of “Avoidance”, we 
have to make an assumption. We need to ensure that all information about
 resources which process will need are known to us prior to execution of
 the process. We use Banker’s algorithm (Which is in-turn a gift from 
Dijkstra) in order to avoid deadlock.

2) Deadlock detection and recovery: Let deadlock occur, then do preemption to handle it once occurred.

3)
 Ignore the problem altogether: If deadlock is very rare, then let it 
happen and reboot the system. This is the approach that both Windows and
 UNIX take.

**Exercise:**   **1)** 
Suppose n processes, P1, …. Pn share m identical resource units, which 
can be reserved and released one at a time. The maximum resource 
requirement of process Pi is Si, where Si > 0. Which one of the 
following is a sufficient condition for ensuring that deadlock does not 
occur? (GATE CS 2005) 

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gate2009OS.png

**(A)** A **(B)** B **(C)** C **(D)** D

For solution, see Question 4 of https://www.geeksforgeeks.org/operating-systems-set-16/

See [QUIZ ON DEADLOCK](https://www.geeksforgeeks.org/deadlock-gq/) for more questions.

**References:** http://www2.latech.edu/~box/os/ch07.pdf http://www.cs.uic.edu/~jbell/CourseNotes/OperatingSystems/7_Deadlocks.html

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Deadlock Detection And Recovery
In the previous post, we have discussed [Deadlock Prevention and Avoidance](https://www.geeksforgeeks.org/deadlock-prevention/). In this post, the Deadlock Detection and Recovery technique to handle deadlock is discussed.

**Deadlock Detection :**

**1. If resources have a single instance –**In
 this case for Deadlock detection, we can run an algorithm to check for 
the cycle in the Resource Allocation Graph. The presence of a cycle in 
the graph is a sufficient condition for deadlock. 

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/deadlock.png

**2.** In
 the above diagram, resource 1 and resource 2 have single instances. 
There is a cycle R1 → P1 → R2 → P2. So, Deadlock is Confirmed. 

**3. If there are multiple instances of resources –**Detection
 of the cycle is necessary but not sufficient condition for deadlock 
detection, in this case, the system may or may not be in deadlock varies
 according to different situations.

**Deadlock Recovery :** A
 traditional operating system such as Windows doesn’t deal with deadlock
 recovery as it is a time and space-consuming process. Real-time 
operating systems use Deadlock recovery.

1. **Killing the process –**Killing all the processes involved in the deadlock. Killing process one by one. After killing each process check for deadlock again keep repeating the
process till the system recovers from deadlock. Killing all the
processes one by one helps a system to break circular wait condition.
2. **Resource Preemption –**Resources are preempted from the processes involved in the deadlock, preempted
resources are allocated to other processes so that there is a
possibility of recovering the system from deadlock. In this case, the
system goes into starvation.
*** Deadlock, Starvation, and Livelock
Prerequisite – [Deadlock and Starvation](https://www.geeksforgeeks.org/deadlock-starvation-java/)

**Livelock**
 occurs when two or more processes continually repeat the same 
interaction in response to changes in the other processes without doing 
any useful work. These processes are not in the waiting state, and they 
are running concurrently. This is different from a deadlock because in a
 deadlock all processes are in the waiting state.

!https://media.geeksforgeeks.org/wp-content/uploads/20220408130945/Deadlock.jpg

**Example:** Imagine a pair of processes using two resources, as shown:

`void` `process_A(void)`

`{`

`enter_reg(&amp; resource_1);`

`enter_reg(&amp; resource_2);`

`use_both_resources();`

`leave_reg(&amp; resource_2);`

`leave_reg(&amp; resource_1);`

`}`

`void` `process_B(void)`

`{`

`enter_reg(&amp; resource_1);`

`enter_reg(&amp; resource_2);`

`use_both_resources();`

`leave_reg(&amp; resource_2);`

`leave_reg(&amp; resource_1);`

`}`

Each
 of the two processes needs the two resources and they use the polling 
primitive enter_reg to try to acquire the locks necessary for them. In 
case, the attempt fails, the process just tries again. If process A runs
 first and acquires resource 1 and then process B runs and acquires 
resource 2, no matter which one runs next, it will make no further 
progress, but neither of the two processes blocks. What actually happens
 is that it uses its CPU quantum over and over again without any 
progress being made but also without any sort of blocking. Thus, this 
situation is not that of a deadlock( as no process is being blocked) but
 we have something functionally equivalent to a deadlock: LIVELOCK.

### What leads to Livelocks?

The
 occurrence of livelocks can occur in the most surprising ways. The 
total number of allowed processes in some systems is determined by the 
number of entries in the process table. Thus process table slots can be 
referred to as Finite Resources. If a fork fails because of the table 
being full, waiting a random time and trying again would be a reasonable
 approach for the program doing the fork. Consider a UNIX system having 
100 process slots. Ten programs are running, each of which has to create
 12 (sub)processes. After each process has created 9 processes, the 10 
original processes and the 90 new processes have exhausted the table. 
Each of the 10 original processes now sits in an endless loop forking 
and failing – which is aptly the situation of a deadlock. The 
probability of this happening is very little but it could happen.

### Difference between Deadlock, Starvation, and Livelock:

A **livelock** is similar to a **deadlock**,
 except that the states of the processes involved in the livelock 
constantly change with regard to one another, none progressing. Livelock
 is a special case of resource **starvation**; the general definition states that a specific process is not progressing.

**Livelock:**

`var l1 = .... // lock object like semaphore or mutex etc`

`var l2 = .... // lock object like semaphore or mutex etc`

`// Thread1`

`Thread.Start( ()=&gt; {`

`while` `(true) {`

`if` `(!l1.Lock(1000)) {`

`continue;`

`}`

`if` `(!l2.Lock(1000)) {`

`continue;`

`}`

`/// do some work`

`});`

`// Thread2`

`Thread.Start( ()=&gt; {`

`while` `(true) {`

`if` `(!l2.Lock(1000)) {`

`continue;`

`}`

`if` `(!l1.Lock(1000)) {`

`continue;`

`}`

`// do some work`

`});`

**Deadlock:**

`var p = new` `object();`

`lock(p)`

`{`

`lock(p)`

`{`

`// deadlock. Since p is previously locked`

`// we will never reach here...`

`}`

A *deadlock* is a state in which each member of a group of actions, is waiting for some other member to release a lock. A *livelock*
 on the other hand is almost similar to a deadlock, except that the 
states of the processes involved in a livelock constantly keep on 
changing with regard to one another, none progressing. Thus Livelock is a
 special case of resource starvation, as stated from the general 
definition, the process is not progressing.

**Starvation:**

Starvation
 is a problem that is closely related to both, Livelock and Deadlock. In
 a dynamic system, requests for resources keep on happening. Thereby, 
some policy is needed to make a decision about who gets the resource and
 when this process, being reasonable, may lead to some processes never 
getting serviced even though they are not deadlocked.

`Queue q = .....`

`while` `(q.Count & gt; 0)`

`{`

`var c = q.Dequeue();`

`.........`

`// Some method in different thread accidentally`

`// puts c back in queue twice within same time frame`

`q.Enqueue(c);`

`q.Enqueue(c);`

`// leading to growth of queue twice then it`

`// can consume, thus starving of computing`

`}`

Starvation
 happens when “greedy” threads make shared resources unavailable for 
long periods. For instance, suppose an object provides a synchronized 
method that often takes a long time to return. If one thread invokes 
this method frequently, other threads that also need frequent 
synchronized access to the same object will often be blocked.
*** Dreadlock Preservation And Avoidance
**Deadlock Characteristics** As discussed in the previous post, deadlock has following characteristics. 

1. Mutual Exclusion
2. Hold and Wait
3. No preemption
4. Circular wait

**Deadlock Prevention** We can prevent Deadlock by eliminating any of the above four conditions. 

**Eliminate Mutual Exclusion** It
 is not possible to dis-satisfy the mutual exclusion because some 
resources, such as the tape drive and printer, are inherently 
non-shareable.

**Eliminate Hold and wait**

1. Allocate all required resources to the process before the start of its
execution, this way hold and wait condition is eliminated but it will
lead to low device utilization. for example, if a process requires
printer at a later time and we have allocated printer before the start
of its execution printer will remain blocked till it has completed its
execution. 
2. The process will make a new request for
resources after releasing the current set of resources. This solution
may lead to starvation.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/06/holdnwait-300x183.png

**Eliminate No Preemption** Preempt resources from the process when resources required by other high priority processes.

**Eliminate Circular Wait** Each
 resource will be assigned with a numerical number. A process can 
request the resources increasing/decreasing. order of numbering. For
 Example, if P1 process is allocated R5 resources, now next time if P1 
ask for R4, R3 lesser than R5 such request will not be granted, only 
request for resources more than R5 will be granted.

**Deadlock Avoidance** Deadlock avoidance can be done with Banker’s Algorithm.

**Banker’s Algorithm** Bankers’s
 Algorithm is resource allocation and deadlock avoidance algorithm which
 test all the request made by processes for resources, it checks for the
 safe state, if after granting request system remains in the safe state 
it allows the request and if there is no safe state it doesn’t allow the
 request made by the process.

**Inputs to Banker’s Algorithm:** 

1. Max need of resources by each process.
2. Currently, allocated resources by each process.
3. Max free available resources in the system.

**The request will only be granted under the below condition:**

1. If the request made by the process is less than equal to max need to that process.
2. If the request made by the process is less than equal to the freely available resource in the system.

**Example:**

```
Total resources in system:
A B C D
6 5 7 6
```

```
Available system resources are:
A B C D
3 1 1 2
```

```
Processes (currently allocated resources):
    A B C D
P1  1 2 2 1
P2  1 0 3 3
P3  1 2 1 0
```

```
Processes (maximum resources):
    A B C D
P1  3 3 2 2
P2  1 2 3 4
P3  1 3 5 0
```

```
Need = maximum resources - currently allocated resources.
Processes (need resources):
    A B C D
P1  2 1 0 1
P2  0 2 0 1
P3  0 1 4 0
```

**Note:** Deadlock prevention is more strict than Deadlock Avoidance.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Banker’s Algorithm in Operating System
The
 banker’s algorithm is a resource allocation and deadlock avoidance 
algorithm that tests for safety by simulating the allocation for 
predetermined maximum possible amounts of all resources, then makes an 
“s-state” check to test for possible activities, before deciding whether
 allocation should be allowed to continue.**Why Banker’s algorithm is named so?** Banker’s
 algorithm is named so because it is used in banking system to check 
whether loan can be sanctioned to a person or not. Suppose there are n 
number of account holders in a bank and the total sum of their money is 
S. If a person applies for a loan then the bank first subtracts the loan
 amount from the total money that bank has and if the remaining amount 
is greater than S then only the loan is sanctioned. It is done because 
if all the account holders comes to withdraw their money then the bank 
can easily do it.In other words, the bank would never allocate its 
money in such a way that it can no longer satisfy the needs of all its 
customers. The bank would try to be in safe state always.Following **Data structures** are used to implement the Banker’s Algorithm:Let **‘n’** be the number of processes in the system and **‘m’** be the number of resources types.**Available :**  

- It is a 1-d array of size **‘m’** indicating the number of available resources of each type.
- Available[ j ] = k means there are **‘k’** instances of resource type **Rj**

**Max :** 

!https://videocdn.geeksforgeeks.org/geeksforgeeks/BankersAlgorithminOperatingSystems/BankersAlgorithminOperatingSystems20220708161609.jpg

- It is a 2-d array of size ‘**n*m’** that defines the maximum demand of each process in a system.
- Max[ i, j ] = k means process **Pi** may request at most **‘k’** instances of resource type **Rj.**

**Allocation :** 

- It is a 2-d array of size **‘n*m’** that defines the number of resources of each type currently allocated to each process.
- Allocation[ i, j ] = k means process **Pi** is currently allocated **‘k’** instances of resource type **Rj**

**Need :** 

- It is a 2-d array of size **‘n*m’** that indicates the remaining resource need of each process.
- Need [ i, j ] = k means process **Pi** currently need **‘k’** instances of resource type **Rj**
- Need [ i, j ] = Max [ i, j ] – Allocation [ i, j ]

Allocationi specifies the resources currently allocated to process Pi and Needi specifies the additional resources that process Pi may still request to complete its task.Banker’s algorithm consists of Safety algorithm and Resource request algorithm**Safety Algorithm**The algorithm for finding out whether or not a system is in a safe state can be described as follows: 

> 1) Let Work and Finish be vectors of length ‘m’ and ‘n’ respectively. Initialize: Work = Available Finish[i] = false; for i=1, 2, 3, 4….n2) Find an i such that both a) Finish[i] = false b) Needi <= Work if no such i exists goto step (4)3) Work = Work + Allocation[i] Finish[i] = true goto step (2)4) if Finish [i] = true for all i then the system is in a safe state
> 

**Resource-Request Algorithm**Let Requesti be the request array for process Pi. Requesti [j] = k means process Pi wants k instances of resource type Rj. When a request for resources is made by process Pi, the following actions are taken:

> 1) If Requesti <= Needi Goto step (2) ; otherwise, raise an error condition, since the process has exceeded its maximum claim.2) If Requesti <= Available Goto step (3); otherwise, Pi must wait, since the resources are not available.3) Have the system pretend to have allocated the requested resources to process Pi by modifying the state as follows: Available = Available – Requesti Allocationi = Allocationi + Requesti Needi = Needi– Requesti
> 

**Example:Considering a system with five processes P0 through P4
 and three resources of type A, B, C. Resource type A has 10 instances, B
 has 5 instances and type C has 7 instances. Suppose at time t0 following snapshot of the system has been taken:**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/01/safety.png

**Question1. What will be the content of the Need matrix?**Need [i, j] = Max [i, j] – Allocation [i, j]So, the content of Need Matrix is:

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/01/unnamed.png

**Question2.  Is the system in a safe state? If Yes, then what is the safe sequence?**Applying the Safety algorithm on the given system,

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/01/questionsolved-1024x632.png

**Question3. What will happen if process P1 requests one additional instance of resource type A and two instances of resource type C?**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/01/Allocation.png

We
 must determine whether this new system state is safe. To do so, we 
again execute Safety algorithm on the above data structures.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/01/Q31-1024x636.png

Hence the new system state is safe, so we can immediately grant the request for process  **P1 .**Code for Banker’s Algorithm

`// Banker's Algorithm`

`#include <stdio.h>`

`int` `main()`

`{`

`// P0, P1, P2, P3, P4 are the Process names here`

`int` `n, m, i, j, k;`

`n = 5; // Number of processes`

`m = 3; // Number of resources`

`int` `alloc[5][3] = { { 0, 1, 0 }, // P0    // Allocation Matrix`

`{ 2, 0, 0 }, // P1`

`{ 3, 0, 2 }, // P2`

`{ 2, 1, 1 }, // P3`

`{ 0, 0, 2 } }; // P4`

`int` `max[5][3] = { { 7, 5, 3 }, // P0    // MAX Matrix`

`{ 3, 2, 2 }, // P1`

`{ 9, 0, 2 }, // P2`

`{ 2, 2, 2 }, // P3`

`{ 4, 3, 3 } }; // P4`

`int` `avail[3] = { 3, 3, 2 }; // Available Resources`

`int` `f[n], ans[n], ind = 0;`

`for` `(k = 0; k < n; k++) {`

`f[k] = 0;`

`}`

`int` `need[n][m];`

`for` `(i = 0; i < n; i++) {`

`for` `(j = 0; j < m; j++)`

`need[i][j] = max[i][j] - alloc[i][j];`

`}`

`int` `y = 0;`

`for` `(k = 0; k < 5; k++) {`

`for` `(i = 0; i < n; i++) {`

`if` `(f[i] == 0) {`

`int` `flag = 0;`

`for` `(j = 0; j < m; j++) {`

`if` `(need[i][j] > avail[j]){`

`flag = 1;`

`break;`

`}`

`}`

`if` `(flag == 0) {`

`ans[ind++] = i;`

`for` `(y = 0; y < m; y++)`

`avail[y] += alloc[i][y];`

`f[i] = 1;`

`}`

`}`

`}`

`}`

`int` `flag = 1;`

`for(int` `i=0;i<n;i++)`

`{`

`if(f[i]==0)`

`{`

`flag=0;`

`printf("The following system is not safe");`

`break;`

`}`

`}`

`if(flag==1)`

`{`

`printf("Following is the SAFE Sequence\n");`

`for` `(i = 0; i < n - 1; i++)`

`printf(" P%d ->", ans[i]);`

`printf(" P%d", ans[n - 1]);`

`}`

`return` `(0);`

`// This code is contributed by Deep Baldha (CandyZack)`

`}`

**Output**

`Following is the SAFE Sequence
 P1 -> P3 -> P4 -> P0 -> P2`

- As the
processes enter the system, they must predict the maximum number of
resources needed which is not impractical to determine.
- In this algorithm, the number of processes remain fixed which is not possible in interactive systems.
- This algorithm requires that there should be a fixed number of resources to
allocate. If a device breaks and becomes suddenly unavailable the
algorithm would not work.
- Overhead cost incurred by the
algorithm can be high when there are many processes and resources
because it has to be invoked for every processes.
*** Resource Allocation Graph (RAG) in Operating System
As [Banker’s algorithm](https://www.geeksforgeeks.org/operating-system-bankers-algorithm/)
 using some kind of table like allocation, request, available all that 
thing to understand what is the state of the system. Similarly, if you 
want to understand the state of the system instead of using those table,
 actually tables are very easy to represent and understand it, but then 
still you could even represent the same information in the graph. That 
graph is called **Resource Allocation Graph (RAG)**.

So, resource allocation graph is explained to us what is the state of the system in terms of **processes and resources**.
 Like how many resources are available, how many are allocated and what 
is the request of each process. Everything can be represented in terms 
of the diagram. One of the advantages of having a diagram is, sometimes 
it is possible to see a deadlock directly by using RAG, but then you 
might not be able to know that by looking at the table. But the tables 
are better if the system contains lots of process and resource and Graph
 is better if the system contains less number of process and resource.We know that any graph contains vertices and edges. So RAG also contains vertices and edges. In RAG vertices are two type –

**1. Process vertex –** Every process will be represented as a process vertex.Generally, the process will be represented with a circle.**2. Resource vertex –** Every resource will be represented as a resource vertex. It is also two type –

- **Single instance type resource –** It represents as a box, inside the box, there will be one dot.So the
number of dots indicate how many instances are present of each resource
type.
- **Multi-resource instance type resource –** It also represents as a box, inside the box, there will be many dots present.

!https://media.geeksforgeeks.org/wp-content/uploads/Prjjj1-1.jpg

Now coming to the edges of RAG.There are two types of edges in RAG –

**1. Assign Edge –** If you already assign a resource to a process then it is called Assign edge.**2. Request Edge –** It means in future the process might want some resource to complete the execution, that is called request edge.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide6-1.jpg

So,
 if a process is using a resource, an arrow is drawn from the resource 
node to the process node. If a process is requesting a resource, an 
arrow is drawn from the process node to the resource node.

**Example 1 (Single instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide1.jpg

If
 there is a cycle in the Resource Allocation Graph and each resource in 
the cycle provides only one instance, then the processes will be in 
deadlock. For example, if process P1 holds resource R1, process P2 holds
 resource R2 and process P1 is waiting for R2 and process P2 is waiting 
for R1, then process P1 and process P2 will be in deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Presentation1-1.jpg

Here’s
 another example, that shows Processes P1 and P2 acquiring resources R1 
and R2 while process P3 is waiting to acquire both resources. In this 
example, there is no deadlock because there is no circular dependency.So cycle in single-instance resource type is the sufficient condition for deadlock.

**Example 2 (Multi-instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide2.jpg

From
 the above example, it is not possible to say the RAG is in a safe state
 or in an unsafe state.So to see the state of this RAG, let’s construct 
the allocation matrix and request matrix.

!https://media.geeksforgeeks.org/wp-content/uploads/sli-1.jpg

- The total number of processes are three; P1, P2 & P3 and the total number of resources are two; R1 & R2.

**Allocation matrix –**

- For constructing the allocation matrix, just go to the resources and see to which process it is allocated.
- R1 is allocated to P1, therefore write 1 in allocation matrix and
similarly, R2 is allocated to P2 as well as P3 and for the remaining
element just write 0.

**Request matrix –**

- In order to find out the request matrix, you have to go to the process and see the outgoing edges.
- P1 is requesting resource R2, so write 1 in the matrix and similarly, P2 requesting R1 and for the remaining element write 0.

So now available resource is = (0, 0).

**Checking deadlock (safe or not) –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/hgh-1.jpg

So,
 there is no deadlock in this RAG.Even though there is a cycle, still 
there is no deadlock.Therefore in multi-instance resource cycle is not 
sufficient condition for deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide3.jpg

Above example is the same as the previous example except that, the process P3 requesting for resource R1.So the table becomes as shown in below.

!https://media.geeksforgeeks.org/wp-content/uploads/vg-1.jpg

So,the
 Available resource is = (0, 0), but requirement are (0, 1), (1, 0) and 
(1, 0).So you can’t fulfill any one requirement.Therefore, it is in 
deadlock.

Therefore, every cycle in a multi-instance resource type
 graph is not a deadlock, if there has to be a deadlock, there has to be
 a cycle.So, in case of RAG with multi-instance resource type, the cycle
 is a necessary condition for deadlock, but not sufficient.

# Resource Allocation Graph (RAG) in Operating System

- Difficulty Level :
[Easy](https://www.geeksforgeeks.org/easy/)
- Last Updated :
16 Aug, 2019

As [Banker’s algorithm](https://www.geeksforgeeks.org/operating-system-bankers-algorithm/)
 using some kind of table like allocation, request, available all that 
thing to understand what is the state of the system. Similarly, if you 
want to understand the state of the system instead of using those table,
 actually tables are very easy to represent and understand it, but then 
still you could even represent the same information in the graph. That 
graph is called **Resource Allocation Graph (RAG)**.

So, resource allocation graph is explained to us what is the state of the system in terms of **processes and resources**.
 Like how many resources are available, how many are allocated and what 
is the request of each process. Everything can be represented in terms 
of the diagram. One of the advantages of having a diagram is, sometimes 
it is possible to see a deadlock directly by using RAG, but then you 
might not be able to know that by looking at the table. But the tables 
are better if the system contains lots of process and resource and Graph
 is better if the system contains less number of process and resource.We know that any graph contains vertices and edges. So RAG also contains vertices and edges. In RAG vertices are two type –

**1. Process vertex –** Every process will be represented as a process vertex.Generally, the process will be represented with a circle.**2. Resource vertex –** Every resource will be represented as a resource vertex. It is also two type –

- **Single instance type resource –** It represents as a box, inside the box, there will be one dot.So the
number of dots indicate how many instances are present of each resource
type.
- **Multi-resource instance type resource –** It also represents as a box, inside the box, there will be many dots present.

!https://media.geeksforgeeks.org/wp-content/uploads/Prjjj1-1.jpg

Now coming to the edges of RAG.There are two types of edges in RAG –

**1. Assign Edge –** If you already assign a resource to a process then it is called Assign edge.**2. Request Edge –** It means in future the process might want some resource to complete the execution, that is called request edge.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide6-1.jpg

So,
 if a process is using a resource, an arrow is drawn from the resource 
node to the process node. If a process is requesting a resource, an 
arrow is drawn from the process node to the resource node.

**Example 1 (Single instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide1.jpg

If
 there is a cycle in the Resource Allocation Graph and each resource in 
the cycle provides only one instance, then the processes will be in 
deadlock. For example, if process P1 holds resource R1, process P2 holds
 resource R2 and process P1 is waiting for R2 and process P2 is waiting 
for R1, then process P1 and process P2 will be in deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Presentation1-1.jpg

Here’s
 another example, that shows Processes P1 and P2 acquiring resources R1 
and R2 while process P3 is waiting to acquire both resources. In this 
example, there is no deadlock because there is no circular dependency.So cycle in single-instance resource type is the sufficient condition for deadlock.

**Example 2 (Multi-instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide2.jpg

From
 the above example, it is not possible to say the RAG is in a safe state
 or in an unsafe state.So to see the state of this RAG, let’s construct 
the allocation matrix and request matrix.

!https://media.geeksforgeeks.org/wp-content/uploads/sli-1.jpg

- The total number of processes are three; P1, P2 & P3 and the total number of resources are two; R1 & R2.

**Allocation matrix –**

- For constructing the allocation matrix, just go to the resources and see to which process it is allocated.
- R1 is allocated to P1, therefore write 1 in allocation matrix and
similarly, R2 is allocated to P2 as well as P3 and for the remaining
element just write 0.

**Request matrix –**

- In order to find out the request matrix, you have to go to the process and see the outgoing edges.
- P1 is requesting resource R2, so write 1 in the matrix and similarly, P2 requesting R1 and for the remaining element write 0.

So now available resource is = (0, 0).

**Checking deadlock (safe or not) –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/hgh-1.jpg

So,
 there is no deadlock in this RAG.Even though there is a cycle, still 
there is no deadlock.Therefore in multi-instance resource cycle is not 
sufficient condition for deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide3.jpg

Above example is the same as the previous example except that, the process P3 requesting for resource R1.So the table becomes as shown in below.

!https://media.geeksforgeeks.org/wp-content/uploads/vg-1.jpg

So,the
 Available resource is = (0, 0), but requirement are (0, 1), (1, 0) and 
(1, 0).So you can’t fulfill any one requirement.Therefore, it is in 
deadlock.

Therefore, every cycle in a multi-instance resource type
 graph is not a deadlock, if there has to be a deadlock, there has to be
 a cycle.So, in case of RAG with multi-instance resource type, the cycle
 is a necessary condition for deadlock, but not sufficient.

# Resource Allocation Graph (RAG) in Operating System

- Difficulty Level :
[Easy](https://www.geeksforgeeks.org/easy/)
- Last Updated :
16 Aug, 2019

As [Banker’s algorithm](https://www.geeksforgeeks.org/operating-system-bankers-algorithm/)
 using some kind of table like allocation, request, available all that 
thing to understand what is the state of the system. Similarly, if you 
want to understand the state of the system instead of using those table,
 actually tables are very easy to represent and understand it, but then 
still you could even represent the same information in the graph. That 
graph is called **Resource Allocation Graph (RAG)**.

So, resource allocation graph is explained to us what is the state of the system in terms of **processes and resources**.
 Like how many resources are available, how many are allocated and what 
is the request of each process. Everything can be represented in terms 
of the diagram. One of the advantages of having a diagram is, sometimes 
it is possible to see a deadlock directly by using RAG, but then you 
might not be able to know that by looking at the table. But the tables 
are better if the system contains lots of process and resource and Graph
 is better if the system contains less number of process and resource.We know that any graph contains vertices and edges. So RAG also contains vertices and edges. In RAG vertices are two type –

**1. Process vertex –** Every process will be represented as a process vertex.Generally, the process will be represented with a circle.**2. Resource vertex –** Every resource will be represented as a resource vertex. It is also two type –

- **Single instance type resource –** It represents as a box, inside the box, there will be one dot.So the
number of dots indicate how many instances are present of each resource
type.
- **Multi-resource instance type resource –** It also represents as a box, inside the box, there will be many dots present.

!https://media.geeksforgeeks.org/wp-content/uploads/Prjjj1-1.jpg

Now coming to the edges of RAG.There are two types of edges in RAG –

**1. Assign Edge –** If you already assign a resource to a process then it is called Assign edge.**2. Request Edge –** It means in future the process might want some resource to complete the execution, that is called request edge.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide6-1.jpg

So,
 if a process is using a resource, an arrow is drawn from the resource 
node to the process node. If a process is requesting a resource, an 
arrow is drawn from the process node to the resource node.

**Example 1 (Single instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide1.jpg

If
 there is a cycle in the Resource Allocation Graph and each resource in 
the cycle provides only one instance, then the processes will be in 
deadlock. For example, if process P1 holds resource R1, process P2 holds
 resource R2 and process P1 is waiting for R2 and process P2 is waiting 
for R1, then process P1 and process P2 will be in deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Presentation1-1.jpg

Here’s
 another example, that shows Processes P1 and P2 acquiring resources R1 
and R2 while process P3 is waiting to acquire both resources. In this 
example, there is no deadlock because there is no circular dependency.So cycle in single-instance resource type is the sufficient condition for deadlock.

**Example 2 (Multi-instances RAG) –**

!https://media.geeksforgeeks.org/wp-content/uploads/Slide2.jpg

From
 the above example, it is not possible to say the RAG is in a safe state
 or in an unsafe state.So to see the state of this RAG, let’s construct 
the allocation matrix and request matrix.

!https://media.geeksforgeeks.org/wp-content/uploads/sli-1.jpg

- The total number of processes are three; P1, P2 & P3 and the total number of resources are two; R1 & R2.

**Allocation matrix –**

- For constructing the allocation matrix, just go to the resources and see to which process it is allocated.
- R1 is allocated to P1, therefore write 1 in allocation matrix and
similarly, R2 is allocated to P2 as well as P3 and for the remaining
element just write 0.

**Request matrix –**

- In order to find out the request matrix, you have to go to the process and see the outgoing edges.
- P1 is requesting resource R2, so write 1 in the matrix and similarly, P2 requesting R1 and for the remaining element write 0.

So now available resource is = (0, 0).

**Checking deadlock (safe or not) –**

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/hgh-1.jpg

So,
 there is no deadlock in this RAG.Even though there is a cycle, still 
there is no deadlock.Therefore in multi-instance resource cycle is not 
sufficient condition for deadlock.

!https://media.geeksforgeeks.org/wp-content/uploads/Slide3.jpg

Above example is the same as the previous example except that, the process P3 requesting for resource R1.So the table becomes as shown in below.

!https://media.geeksforgeeks.org/wp-content/uploads/vg-1.jpg

So,the
 Available resource is = (0, 0), but requirement are (0, 1), (1, 0) and 
(1, 0).So you can’t fulfill any one requirement.Therefore, it is in 
deadlock.

Therefore, every cycle in a multi-instance resource type
 graph is not a deadlock, if there has to be a deadlock, there has to be
 a cycle.So, in case of RAG with multi-instance resource type, the cycle
 is a necessary condition for deadlock, but not sufficient.
 
*** Resource Allocation Techniques for Processes
The 
Operating System allocates resources when a program need them. When the 
program terminates, the resources are de-allocated, and allocated to 
other programs that need them. Now the question is, what strategy does 
the operating system use to allocate these resources to user programs?

There are two Resource allocation techniques:

1. **Resource partitioning approach –**In this approach, the operating system decides beforehand, that what
resources should be allocated to which user program. It divides the
resources in the system to many *resource partitions*, where each partition may include various resources – for example, 1 MB memory, disk blocks, and a printer.
    
    Then,
     it allocates one resource partition to each user program before the 
    program’s initiation. A resource table records the resource partition 
    and its current allocation status (Allocated or Free).
    
    **Advantages:**
    
    - Easy to Implement
    - Less Overhead
    
    **Disadvantages:**
    
    - **Lacks flexibility –** if a resource partition contains more resources than what a particular process requires, the additional resources are wasted.
    - If a program needs more resources than a single resource partition, it
    cannot execute (Though free resources are present in other partitions).
    
    An example resource table may look like:
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-5-10.png
    
2. **Pool based approach –**In this approach, there is a *common pool of resources*. The operating System checks the allocation status in the resource table whenever a program makes a request for a resource. If the resource is
free, it allocates the resource to the program.
    
    **Advantages:**
    
    - Allocated resources are not wasted.
    - Any resource requirement can be fulfilled if the resource is free (unlike Partitioning approach)
    
    **Disadvantages:**
    
    - Overhead of allocating and de-allocating the resources on every request and release.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Program for Banker’s Algorithm | Set 1 (Safety Algorithm)
Prerequisite: [Banker’s Algorithm](https://www.geeksforgeeks.org/bankers-algorithm-in-operating-system-2/)

The
 banker’s algorithm is a resource allocation and deadlock avoidance 
algorithm that tests for safety by simulating the allocation for 
predetermined maximum possible amounts of all resources, then makes an 
“s-state” check to test for possible activities, before deciding whether
 allocation should be allowed to continue.

Following **Data structures** are used to implement the Banker’s Algorithm:

Let **‘n’** be the number of processes in the system and **‘m’** be the number of resources types.

**Available :**

- It is a 1-d array of size **‘m’** indicating the number of available resources of each type.
- Available[ j ] = k means there are **‘k’** instances of resource type **Rj**

**Max :**

- It is a 2-d array of size ‘**n*m’** that defines the maximum demand of each process in a system.
- Max[ i, j ] = k means process **Pi** may request at most **‘k’** instances of resource type **Rj.**

**Allocation :**

- It is a 2-d array of size **‘n*m’** that defines the number of resources of each type currently allocated to each process.
- Allocation[ i, j ] = k means process **Pi** is currently allocated **‘k’** instances of resource type **Rj**

**Need :**

- It is a 2-d array of size **‘n*m’** that indicates the remaining resource need of each process.
- Need [ i, j ] = k means process **Pi** currently allocated **‘k’** instances of resource type **Rj**
- Need [ i, j ] = Max [ i, j ] – Allocation [ i, j ]

Allocationi specifies the resources currently allocated to process Pi and Needi specifies the additional resources that process Pi may still request to complete its task.

Banker’s algorithm consist of Safety algorithm and Resource request algorithm

**Safety Algorithm**

The algorithm for finding out whether or not a system is in a safe state can be described as follows:

1. Let Work and Finish be vectors of length ‘m’ and ‘n’ respectively.Initialize: Work= AvailableFinish [i]=false; for i=1,2,……,n
2. Find an i such that botha) Finish [i]=falseb) Need_i<=workif no such i exists goto step (4)
3. Work=Work + Allocation_iFinish[i]= truegoto step(2)
4. If Finish[i]=true for all i,then the system is in safe state.

**Safe sequence is the sequence in which the processes can be safely executed.**

In this post, implementation of Safety algorithm of Banker’s Algorithm is done.

## [Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

`// C++ program to illustrate Banker's Algorithm`

`#include<iostream>`

`using` `namespace` `std;`

`// Number of processes`

`const` `int` `P = 5;`

`// Number of resources`

`const` `int` `R = 3;`

`// Function to find the need of each process`

`void` `calculateNeed(int` `need[P][R], int` `maxm[P][R],`

`int` `allot[P][R])`

`{`

`// Calculating Need of each P`

`for` `(int` `i = 0 ; i < P ; i++)`

`for` `(int` `j = 0 ; j < R ; j++)`

`// Need of instance = maxm instance -`

`//                    allocated instance`

`need[i][j] = maxm[i][j] - allot[i][j];`

`}`

`// Function to find the system is in safe state or not`

`bool` `isSafe(int` `processes[], int` `avail[], int` `maxm[][R],`

`int` `allot[][R])`

`{`

`int` `need[P][R];`

`// Function to calculate need matrix`

`calculateNeed(need, maxm, allot);`

`// Mark all processes as infinish`

`bool` `finish[P] = {0};`

`// To store safe sequence`

`int` `safeSeq[P];`

`// Make a copy of available resources`

`int` `work[R];`

`for` `(int` `i = 0; i < R ; i++)`

`work[i] = avail[i];`

`// While all processes are not finished`

`// or system is not in safe state.`

`int` `count = 0;`

`while` `(count < P)`

`{`

`// Find a process which is not finish and`

`// whose needs can be satisfied with current`

`// work[] resources.`

`bool` `found = false;`

`for` `(int` `p = 0; p < P; p++)`

`{`

`// First check if a process is finished,`

`// if no, go for next condition`

`if` `(finish[p] == 0)`

`{`

`// Check if for all resources of`

`// current P need is less`

`// than work`

`int` `j;`

`for` `(j = 0; j < R; j++)`

`if` `(need[p][j] > work[j])`

`break;`

`// If all needs of p were satisfied.`

`if` `(j == R)`

`{`

`// Add the allocated resources of`

`// current P to the available/work`

`// resources i.e.free the resources`

`for` `(int` `k = 0 ; k < R ; k++)`

`work[k] += allot[p][k];`

`// Add this process to safe sequence.`

`safeSeq[count++] = p;`

`// Mark this p as finished`

`finish[p] = 1;`

`found = true;`

`}`

`}`

`}`

`// If we could not find a next process in safe`

`// sequence.`

`if` `(found == false)`

`{`

`cout << "System is not in safe state";`

`return` `false;`

`}`

`}`

`// If system is in safe state then`

`// safe sequence will be as below`

`cout << "System is in safe state.\nSafe"`

`" sequence is: ";`

`for` `(int` `i = 0; i < P ; i++)`

`cout << safeSeq[i] << " ";`

`return` `true;`

`}`

`// Driver code`

`int` `main()`

`{`

`int` `processes[] = {0, 1, 2, 3, 4};`

`// Available instances of resources`

`int` `avail[] = {3, 3, 2};`

`// Maximum R that can be allocated`

`// to processes`

`int` `maxm[][R] = {{7, 5, 3},`

`{3, 2, 2},`

`{9, 0, 2},`

`{2, 2, 2},`

`{4, 3, 3}};`

`// Resources allocated to processes`

`int` `allot[][R] = {{0, 1, 0},`

`{2, 0, 0},`

`{3, 0, 2},`

`{2, 1, 1},`

`{0, 0, 2}};`

`// Check system is in safe state or not`

`isSafe(processes, avail, maxm, allot);`

`return` `0;`

`}`

**Output:**

```
System is in safe state.
Safe sequence is: 1 3 4 0 2

```

**Illustration :**

Considering a system with 
five processes P0 through P4 and three resources types A, B, C. Resource
 type A has 10 instances, B has 5 instances and type C has 7 instances. 
Suppose at time t0 following snapshot of the system has been taken:

!https://media.geeksforgeeks.org/wp-content/uploads/allocation_table-1.png

We
 must determine whether the new system state is safe. To do so, we need 
to execute Safety algorithm on the above given allocation chart.

!https://media.geeksforgeeks.org/wp-content/uploads/bankers-algorithm.png

Following is the resource allocation graph:Executing safety algorithm shows that sequence < P1, P3, P4, P0, P2 > satisfies safety requirement.

!https://media.geeksforgeeks.org/wp-content/uploads/Bankers.png

**Time complexity** = O(n*n*m) where n = number of processes and m = number of resources.

This article is contributed by **[Sahil Chhabra (akku)](https://www.facebook.com/sahil.chhabra.965)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](https://write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Deadlock Detection Algorithm in Operating System
If a system does not employ either a deadlock prevention or [deadlock avoidance algorithm](https://www.geeksforgeeks.org/operating-system-bankers-algorithm-print-safe-state-safe-sequences/) then a deadlock situation may occur. In this case-

- Apply an algorithm to examine state of system to determine whether deadlock has occurred or not.
- Apply an algorithm to recover from the deadlock. For more refer- [Deadlock Recovery](https://www.geeksforgeeks.org/deadlock-detection-recovery/)

**Deadlock Avoidance Algorithm/ [Bankers Algorithm](https://www.geeksforgeeks.org/operating-system-bankers-algorithm/):** The algorithm employs several times varying data structures: 

- **Available –** A vector of length m indicates the number of available resources of each type.
- **Allocation –** An n*m matrix defines the number of resources of each type currently
allocated to a process. Column represents resource and rows represent
process.
- **Request –** An n*m matrix indicates the current request of each process. If request[i][j] equals k then process P is requesting k more instances of resource type R.
    
    i
    
    j
    

**This algorithm has already been discussed [here](https://www.geeksforgeeks.org/operating-system-bankers-algorithm-print-safe-state-safe-sequences/)** 

Now, Bankers algorithm includes a **Safety Algorithm / Deadlock Detection Algorithm** The algorithm for finding out whether a system is in a safe state can be described as follows:

**Steps of Algorithm:** 

1. Let *Work* and *Finish* be vectors of length m and n respectively. Initialize *Work= Available*. For *i=0, 1, …., n-1*, if *Requesti* = 0, then *Finish[i]* = true; otherwise, *Finish[i]*= false.
2. Find an index i such that both a) *Finish[i] == false* b) *Requesti <= Work* If no such *i* exists go to step 4.
3. *Work= Work+ Allocationi* *Finish[i]= true* Go to Step 2.
4. If *Finish[i]== false* for some i, 0<=i<n, then the system is in a deadlocked state. Moreover, if *Finish[i]==false* the process P is deadlocked.
    
    i
    

For example, 

!https://media.geeksforgeeks.org/wp-content/uploads/deadlockdetection.jpg

1. In this, Work = [0, 0, 0] & Finish = [false, false, false, false, false]
2. *i=0* is selected as both Finish[0] = false and [0, 0, 0]<=[0, 0, 0].
3. Work =[0, 0, 0]+[0, 1, 0] =>[0, 1, 0] & Finish = [true, false, false, false, false].
4. *i=2* is selected as both Finish[2] = false and [0, 0, 0]<=[0, 1, 0].
5. Work =[0, 1, 0]+[3, 0, 3] =>[3, 1, 3] & Finish = [true, false, true, false, false].
6. *i=1* is selected as both Finish[1] = false and [2, 0, 2]<=[3, 1, 3].
7. Work =[3, 1, 3]+[2, 0, 0] =>[5, 1, 3] & Finish = [true, true, true, false, false].
8. *i=3* is selected as both Finish[3] = false and [1, 0, 0]<=[5, 1, 3].
9. Work =[5, 1, 3]+[2, 1, 1] =>[7, 2, 4] & Finish = [true, true, true, true, false].
10. *i=4* is selected as both Finish[4] = false and [0, 0, 2]<=[7, 2, 4].
11. Work =[7, 2, 4]+[0, 0, 2] =>[7, 2, 6] & Finish = [true, true, true, true, true].
12. Since Finish is a vector of all true it means **there is no deadlock** in this example.
*** Program for Deadlock free condition in Operating System
Given:
 A system has R identical resources, P processes competing for them and N
 is the maximum need of each process. The task is to find the minimum 
number of Resources required So that deadlock will never occur.

**Formula:**

```
R >= P * (N - 1) + 1
```

**Examples:**

```
Input : P = 3, N = 4
Output : R >= 10

Input : P = 7, N = 2
Output : R >= 8
```

**Approach:**

> Consider, 3 process A, B and C. Let, Need of each process is 4 Therefore, The maximum resources require will be 3 * 4 = 12 i.e, Give 4 resources to each Process. And, The minimum resources required will be 3 * (4 – 1) + 1 = 10. i.e, Give 3 Resources to each of the Process, and we are left out with 1 Resource. That 1 resource will be given to any of the Process A, B or C. So
 that after using that resource by any one of the Process, It left the 
resources and that resources will be used by any other Process and thus Deadlock will Never Occur.
> 

`// C++ implementation of above program.`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// function that calculates`

`// the minimum no. of resources`

`int` `Resources(int` `process, int` `need)`

`{`

`int` `minResources = 0;`

`// Condition so that deadlock`

`// will not occur`

`minResources = process * (need - 1) + 1;`

`return` `minResources;`

`}`

`// Driver code`

`int` `main()`

`{`

`int` `process = 3, need = 4;`

`cout << "R >= "` `<< Resources(process, need);`

`return` `0;`

`}`

**Output:**

```
R >= 10
```

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Deadlock detection in Distributed systems
In a
 distributed system deadlock can neither be prevented nor avoided as the
 system is so vast that it is impossible to do so. Therefore, only 
deadlock detection can be implemented. The techniques of deadlock 
detection in the distributed system require the following: 

- **Progress –** The method should be able to detect all the deadlocks in the system.
- **Safety –** The method should not detect false or phantom deadlocks.

There are three approaches to detect deadlocks in distributed systems. They are as follows: 

1. **Centralized approach –** In the centralized approach, there is only one responsible resource to
detect deadlock. The advantage of this approach is that it is simple and easy to implement, while the drawbacks include excessive workload at
one node, single-point failure (that is the whole system is dependent on one node if that node fails the whole system crashes) which in turns
makes the system less reliable. 
2. **Distributed approach –** In the distributed approach different nodes work together to detect
deadlocks. No single point failure (that is the whole system is
dependent on one node if that node fails the whole system crashes) as
the workload is equally divided among all nodes. The speed of deadlock
detection also increases. 
3. **Hierarchical approach –** This approach is the most advantageous. It is the combination of both
centralized and distributed approaches of deadlock detection in a
distributed system. In this approach, some selected nodes or clusters of nodes are responsible for deadlock detection and these selected nodes
are controlled by a single node.
** Processes & Threads
*** Thread in Operating System
**What is a Thread?**A thread is a path of execution within a process. A process can contain multiple threads.**Why Multithreading?**A
 thread is also known as lightweight process. The idea is to achieve 
parallelism by dividing a process into multiple threads. For example, in
 a browser, multiple tabs can be different threads. MS Word uses 
multiple threads: one thread to format the text, another thread to 
process inputs, etc. More advantages of multithreading are discussed 
below**Process vs Thread?**The primary difference 
is that threads within the same process run in a shared memory space, 
while processes run in separate memory spaces.Threads are not 
independent of one another like processes are, and as a result threads 
share with other threads their code section, data section, and OS 
resources (like open files and signals). But, like process, a thread has
 its own program counter (PC), register set, and stack space.***Advantages of Thread over Process**1. Responsiveness:*
 If the process is divided into multiple threads, if one thread 
completes its execution, then its output can be immediately returned.

*2. Faster context switch:* Context
 switch time between threads is lower compared to process context 
switch. Process context switching requires more overhead from the CPU.

*3. Effective utilization of multiprocessor system:*
 If we have multiple threads in a single process, then we can schedule 
multiple threads on multiple processor. This will make process execution
 faster.

*4. Resource sharing:* Resources like code, data, and files can be shared among all threads within a process.Note: stack and registers can’t be shared among the threads. Each thread has its own stack and registers.

*5. Communication:* Communication
 between multiple threads is easier, as the threads shares common 
address space. while in process we have to follow some specific 
communication technique for communication between two process.

*6. Enhanced throughput of the system:*
 If a process is divided into multiple threads, and each thread function
 is considered as one job, then the number of jobs completed per unit of
 time is increased, thus increasing the throughput of the system.**Types of Threads**There are two types of threads.User Level ThreadKernel Level ThreadRefer [User Thread vs Kernel Thread](https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/) for more details.
 
*** Difference between User Level thread and Kernel Level thread
https://www.geeksforgeeks.org/difference-between-user-level-thread-and-kernel-level-thread/
*** Process-Based and Thread-Based Multitasking
lost content search again 
*** Multi Threading Models in Process Management
Multi threading-It is a process of multiple threads executes at same time.

Many
 operating systems support kernel thread and user thread in a combined 
way. Example of such system is Solaris. Multi threading model are of 
three types. 

```
Many to many model.
Many to one model.
one to one model.
```

**Many to Many Model**

In 
this model, we have multiple user threads multiplex to same or lesser 
number of kernel level threads. Number of kernel level threads are 
specific to the machine, advantage of this model is if a user thread is 
blocked we can schedule others user thread to other kernel thread. Thus,
 System doesn’t block if a particular thread is blocked.

It is the best multi threading model.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/many_to_many1-300x200.jpg

**Many to One Model**

In
 this model, we have multiple user threads mapped to one kernel thread. 
In this model when a user thread makes a blocking system call entire 
process blocks. As we have only one kernel thread and only one user 
thread can access kernel at a time, so multiple threads are not able 
access multiprocessor at the same time.

The thread management is done on the user level so it is more efficient.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/many_to_many2-300x200.jpg

**One to One Model** In
 this model, one to one relationship between kernel and user thread. In 
this model multiple thread can run on multiple processor. Problem with 
this model is that creating a user thread requires the corresponding 
kernel thread.

As each user thread is connected to different 
kernel , if any user thread makes a blocking system call, the other user
 threads won’t be blocked.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2015/07/many_to_many3-300x200.jpg

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above
*** Benefits of Multithreading in Operating System
Prerequisite – [Operating-System-Thread](https://www.geeksforgeeks.org/operarting-system-thread/)The benefits of multi threaded programming can be broken down into four major categories:

1. **Responsiveness –**Multithreading in an interactive application may allow a program to continue running
even if a part of it is blocked or is performing a lengthy operation,
thereby increasing responsiveness to the user.
    
    In a non multi threaded
     environment, a server listens to the port for some request and when the
     request comes, it processes the request and then resume listening to 
    another request. The time taken while processing of request makes other 
    users wait unnecessarily. Instead a better approach would be to pass the
     request to a worker thread and continue listening to port.
    
    For 
    example, a multi threaded web browser allow user interaction in one 
    thread while an video is being loaded in another thread. So instead of 
    waiting for the whole web-page to load the user can continue viewing 
    some portion of the web-page.
    
2. **Resource Sharing –**Processes may share resources only through techniques such as-
    - Message Passing
    - Shared Memory
    
    Such
     techniques must be explicitly organized by programmer. However, threads
     share the memory and the resources of the process to which they belong 
    by default.The benefit of sharing code and data is that it allows an
     application to have several threads of activity within same address 
    space.
    
3. **Economy –**Allocating memory and resources for process creation is a costly job in terms of time and space.Since, threads share memory with the process it belongs, it is more economical to create and context switch threads. Generally much more time is
consumed in creating and managing processes than in threads.In Solaris, for example, creating process is 30 times slower than creating threads and context switching is 5 times slower.
4. **Scalability –**The benefits of multi-programming greatly increase in case of
multiprocessor architecture, where threads may be running parallel on
multiple processors. If there is only one thread then it is not possible to divide the processes into smaller tasks that different processors
can perform.Single threaded process can run only on one processor regardless of how many processors are available.Multi-threading on a multiple CPU machine increases parallelism.

References- Operating System concepts by Abraham Silberschatz, Peter B. Galvin& Greg Gagne

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Zombie Processes and their Prevention
Prerequisites: [fork() in C](http://geeksquiz.com/fork-system-call/), [Zombie Process](https://www.geeksforgeeks.org/zombie-and-orphan-processes-in-c/)**Zombie state**:
 When a process is created in UNIX using fork() system call, the address
 space of the Parent process is replicated. If the parent process calls 
wait() system call, then the execution of the parent is suspended until 
the child is terminated. At the termination of the child, a ‘SIGCHLD’ 
signal is generated which is delivered to the parent by the kernel. 
Parent, on receipt of ‘SIGCHLD’ reads the status of the child from the 
process table. Even though the child is terminated, there is an entry in
 the process table corresponding to the child where the status is 
stored. When the parent collects the status, this entry is deleted. 
Thus, all the traces of the child process are removed from the system. 
If the parent decides not to wait for the child’s termination and 
executes its subsequent task, then at the termination of the child, the 
exit status is not read. Hence, there remains an entry in the process 
table even after the termination of the child. This state of the child 
process is known as the Zombie state.

`// A C program to demonstrate working of`

`// fork() and process table entries.`

`#include<stdio.h>`

`#include<unistd.h>`

`#include<sys/wait.h>`

`#include<sys/types.h>`

`int` `main()`

`{`

`int` `i;`

`int` `pid = fork();`

`if` `(pid == 0)`

`{`

`for` `(i=0; i<20; i++)`

`printf("I am Child\n");`

`}`

`else`

`{`

`printf("I am Parent\n");`

`while(1);`

`}`

`}`

**Output :**

!https://media.geeksforgeeks.org/wp-content/uploads/zombie1_1.png

Now check the process table using the following command in the terminal**$ ps -eaf**

!https://media.geeksforgeeks.org/wp-content/uploads/zombie1_2.png

Here the entry **[a.out] defunct** shows the zombie process.

**Why do we need to prevent the creation of the Zombie process?** There
 is one process table per system. The size of the process table is 
finite. If too many zombie processes are generated, then the process 
table will be full. That is, the system will not be able to generate any
 new process, then the system will come to a standstill. Hence, we need 
to prevent the creation of zombie processes.

### Different ways in which the creation of Zombie can be Prevented

**1. Using wait() system call:**
 When the parent process calls wait(), after the creation of a child, it
 indicates that, it will wait for the child to complete and it will reap
 the exit status of the child. The parent process is suspended(waits in a
 waiting queue) until the child is terminated. It must be understood 
that during this period, the parent process does nothing just wait.

`// A C program to demonstrate working of`

`// fork()/wait() and Zombie processes`

`#include<stdio.h>`

`#include<unistd.h>`

`#include<sys/wait.h>`

`#include<sys/types.h>`

`int` `main()`

`{`

`int` `i;`

`int` `pid = fork();`

`if` `(pid==0)`

`{`

`for` `(i=0; i<20; i++)`

`printf("I am Child\n");`

`}`

`else`

`{`

`wait(NULL);`

`printf("I am Parent\n");`

`while(1);`

`}`

`}`

**2. By ignoring the SIGCHLD signal:**
 When a child is terminated, a corresponding SIGCHLD signal is delivered
 to the parent, if we call the ‘signal(SIGCHLD,SIG_IGN)’, then the 
SIGCHLD signal is ignored by the system, and the child process entry is 
deleted from the process table. Thus, no zombie is created. However, in 
this case, the parent cannot know about the exit status of the child.

`// A C program to demonstrate ignoring`

`// SIGCHLD signal to prevent Zombie processes`

`#include<stdio.h>`

`#include<unistd.h>`

`#include<sys/wait.h>`

`#include<sys/types.h>`

`int` `main()`

`{`

`int` `i;`

`int` `pid = fork();`

`if` `(pid == 0)`

`for` `(i=0; i<20; i++)`

`printf("I am Child\n");`

`else`

`{`

`signal(SIGCHLD,SIG_IGN);`

`printf("I am Parent\n");`

`while(1);`

`}`

`}`

**3. By using a signal handler:**
 The parent process installs a signal handler for the SIGCHLD signal. 
The signal handler calls wait() system call within it. In this scenario,
 when the child terminated, the SIGCHLD is delivered to the parent. On 
receipt of SIGCHLD, the corresponding handler is activated, which in 
turn calls the wait() system call. Hence, the parent collects the exit 
status almost immediately and the child entry in the process table is 
cleared. Thus no zombie is created.

`// A C program to demonstrate handling of`

`// SIGCHLD signal to prevent Zombie processes.`

`#include<stdio.h>`

`#include<unistd.h>`

`#include<sys/wait.h>`

`#include<sys/types.h>`

`void` `func(int` `signum)`

`{`

`wait(NULL);`

`}`

`int` `main()`

`{`

`int` `i;`

`int` `pid = fork();`

`if` `(pid == 0)`

`for` `(i=0; i<20; i++)`

`printf("I am Child\n");`

`else`

`{`

`signal(SIGCHLD, func);`

`printf("I am Parent\n");`

`while(1);`

`}`

`}`

**Output:**

!https://media.geeksforgeeks.org/wp-content/uploads/zom_final.png

Here no any **[a.out] defunct** i.e. no Zombie process is created.

This article is contributed by **Kishlay Verma**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](https://write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Remote Procedure Call (RPC) in Operating System
**Remote Procedure Call (RPC)** is a powerful technique for constructing **distributed, client-server based applications**. It is based on extending the conventional local procedure calling so that the **called procedure need not exist in the same address space as the calling procedure**. The two processes may be on the same system, or they may be on different systems with a network connecting them.

**When making a Remote Procedure Call:**

!https://media.geeksforgeeks.org/wp-content/uploads/operating-system-remote-procedure-call-1.png

**1.**
 The calling environment is suspended, procedure parameters are 
transferred across the network to the environment where the procedure is
 to execute, and the procedure is executed there.

**2.**
 When the procedure finishes and produces its results, its results are 
transferred back to the calling environment, where execution resumes as 
if returning from a regular procedure call.

**NOTE: RPC** is especially well suited for client-server **(e.g. query-response)** interaction in which the flow of control **alternates between the caller and callee**.
 Conceptually, the client and server do not both execute at the same 
time. Instead, the thread of execution jumps from the caller to the 
callee and then back again.

**Working of RPC**

!https://media.geeksforgeeks.org/wp-content/uploads/operating-system-remote-call-procedure-working.png

**The following steps take place during a RPC :**

1. A client invokes a **client stub procedure**, passing parameters in the usual way. The client stub resides within the client’s own address space.
2. The client stub **marshalls(pack)** the parameters into a message. Marshalling includes converting the
representation of the parameters into a standard format, and copying
each parameter into the message.
3. The client stub passes the message to the transport layer, which sends it to the remote server machine.
4. On the server, the transport layer passes the message to a server stub, which **demarshalls(unpack)** the parameters and calls the desired server routine using the regular procedure call mechanism.
5. When the server procedure completes, it returns to the server stub **(e.g., via a normal procedure call return)**, which marshalls the return values into a message. The server stub then hands the message to the transport layer.
6. The transport layer sends the result message back to the client transport
layer, which hands the message back to the client stub.
7. The client stub demarshalls the return parameters and execution returns to the caller.

**RPC ISSUES** **:Issues that must be addressed:**

**1. RPC Runtime:** RPC
 run-time system is a library of routines and a set of services that 
handle the network communications that underlie the RPC mechanism. In 
the course of an RPC call, client-side and server-side run-time systems’
 code handle **binding, establish communications over an 
appropriate protocol, pass call data between the client and server, and 
handle communications errors.**

**2. Stub:** The function of the stub is to **provide transparency to the programmer-written application code**.

- **On the client side**, the stub handles the interface between the client’s local procedure
call and the run-time system, marshalling and unmarshalling data,
invoking the RPC run-time protocol, and if requested, carrying out some
of the binding steps.
- **On the server side**, the
stub provides a similar interface between the run-time system and the
local manager procedures that are executed by the server.

**3. Binding: How does the client know who to call, and where the service resides?** The
 most flexible solution is to use dynamic binding and find the server at
 run time when the RPC is first made. The first time the client stub is 
invoked, it contacts a name server to determine the transport address at
 which the server resides.

**Binding consists of two parts:**

- Naming:
- Locating:
1. **A Server** having a service to offer exports an interface for it. Exporting an
interface registers it with the system so that clients can use it.
2. **A Client** must import an (exported) interface before communication can begin.

**4. The call semantics associated with RPC :**It is mainly classified into following choices-

- **Retry request message –**Whether to retry sending a request message when a server has failed or the receiver didn’t receive the message.
- **Duplicate filtering –**Remove the duplicate server requests.
- **Retransmission of results –**To resend lost messages without re-executing the operations at the server side.

**ADVANTAGES :**

1. RPC provides **ABSTRACTION** i.e message-passing nature of network communication is hidden from the user.
2. RPC often omits many of the protocol layers to improve performance. Even a
small performance improvement is important because a program may invoke
RPCs often.
3. RPC enables the usage of the applications in the distributed environment, not only in the local environment.
4. With RPC code re-writing / re-developing effort is minimized.
5. Process-oriented and thread oriented models supported by RPC.
** Memory Management
*** Memory Hierarchy Design and its Characteristics
In 
the Computer System Design, Memory Hierarchy is an enhancement to 
organize the memory such that it can minimize the access time. The 
Memory Hierarchy was developed based on a program behavior known as 
locality of references.The figure below clearly demonstrates the 
different levels of memory hierarchy :

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-4-4.png

This Memory Hierarchy Design is divided into 2 main types:

1. **External Memory or Secondary Memory –**Comprising of Magnetic Disk, Optical Disk, Magnetic Tape i.e. peripheral storage
devices which are accessible by the processor via I/O Module.
2. **Internal Memory or Primary Memory –**Comprising of Main Memory, Cache Memory & CPU registers. This is directly accessible by the processor.

We can infer the following characteristics of Memory Hierarchy Design from above figure:

1. **Capacity:**It is the global volume of information the memory can store. As we move
from top to bottom in the Hierarchy, the capacity increases.
2. **Access Time:**It is the time interval between the read/write request and the
availability of the data. As we move from top to bottom in the
Hierarchy, the access time increases.
3. **Performance:**Earlier when the computer system was designed without Memory Hierarchy design,
the speed gap increases between the CPU registers and Main Memory due to large difference in access time. This results in lower performance of
the system and thus, enhancement was required. This enhancement was made in the form of Memory Hierarchy Design because of which the performance of the system increases. One of the most significant ways to increase
system performance is minimizing how far down the memory hierarchy one
has to go to manipulate data.
4. **Cost per bit:**As we move from bottom to top in the Hierarchy, the cost per bit increases i.e. Internal Memory is costlier than External Memory.
*** Introduction to memory and memory units
Memory
 devices are digital system that store data either temporarily or for a 
long term. Digital computers to hard disk have built in memory devices 
that can store data of user or manufacturers. The data either be in the 
form of control programs or programs that boot the system. Hence, to 
store such huge amount of data the memory devices must have enormous 
capacity. The challenge is to build memory devices that have large 
capacity but cost effective. The memory devices must be capable of 
storing both permanent data and instantaneous data.

Memories
 are made up of registers. Each register in the memory is one storage 
location. The storage location is also called a memory location. Memory 
locations are identified using **Address**. The total number of bits a memory can store is its **capacity**.

A storage element is called a **Cell**.
 Each register is made up of a storage element in which one bit of data 
is stored. The data in a memory are stored and retrieved by the process 
called **writing** and **reading** respectively.

!https://media.geeksforgeeks.org/wp-content/uploads/Read-and-write.png

A **word** is a group of bits where a memory unit stores binary information. A word with a group of 8 bits is called a **byte**. A
 memory unit consists of data lines, address selection lines, and 
control lines that specify the direction of transfer. The block diagram 
of a memory unit is shown below: 

!https://media.geeksforgeeks.org/wp-content/uploads/memory-unit.png

Data
 lines provide the information to be stored in memory. The control 
inputs specify the direct transfer. The k-address lines specify the word
 chosen.

When there are k address lines, 2k memory words can be accessed.

### Following are some important memory units :

**Bit (Binary Units):** bit is a logical representation of the electric state. It can be 1 or 0.

**Nibble:** it means the group of 4 bits.

**Byte:** a ****byte is a group of 8 bits.

**Word:** it
 is a fixed number of bits, it is different from computer to computer, 
but the same for each device. Compute store information in the form of 
words.****

### Following are conversations of units:

**Kilobyte (kb):** 1kb = 1024 byte

**Megabyte (mb):** 1mb = 1024 kb

**Gigabyte (gb):** 1gb = 1024 mb

**Terabyte (tb):** 1tb = 1024 gb

**Petabyte (pb):** 1pb = 1024 tb

Refer for [RAM and ROM](https://www.geeksforgeeks.org/types-computer-memory-ram-rom/), [different types of RAM](https://www.geeksforgeeks.org/different-types-ram-random-access-memory/), [cache memory](https://www.geeksforgeeks.org/cache-memory/), and [secondary memory](https://www.geeksforgeeks.org/secondary-memory/)
*** Different Types of RAM (Random Access Memory )
RAM(Random
 Access Memory) is a part of computer’s Main Memory which is directly 
accessible by CPU. RAM is used to Read and Write data into it which is 
accessed by CPU randomly. RAM is volatile in nature, it means if the 
power goes off, the stored information is lost. RAM is used to store the
 data that is currently processed by the CPU. Most of the programs and 
data that are modifiable are stored in RAM.

Integrated RAM chips are available in two form:

1. SRAM(Static RAM)
2. DRAM(Dynamic RAM)

The block diagram of RAM chip is given below.

!https://media.geeksforgeeks.org/wp-content/uploads/RAM.png

**1. SRAM :**The
 SRAM memories consist of circuits capable of retaining the stored 
information as long as the power is applied. That means this type of 
memory requires constant power. SRAM memories are used to build Cache 
Memory.

**SRAM Memory Cell:** Static memories(SRAM) 
are memories that consist of circuits capable of retaining their state 
as long as power is on. Thus this type of memory is called volatile 
memory. The below figure shows a cell diagram of SRAM. A latch is formed
 by two inverters connected as shown in the figure. Two transistors T1 
and T2 are used for connecting the latch with two-bit lines. The purpose
 of these transistors is to act as switches that can be opened or closed
 under the control of the word line, which is controlled by the address 
decoder. When the word line is at 0-level, the transistors are turned 
off and the latch remains its information. For example, the cell is at 
state 1 if the logic value at point A is 1 and at point, B is 0. This 
state is retained as long as the word line is not activated. 

!https://media.geeksforgeeks.org/wp-content/uploads/SRAM.png

For **Read operation**,
 the word line is activated by the address input to the address decoder.
 The activated word line closes both the transistors (switches) T1 and 
T2. Then the bit values at points A and B can transmit to their 
respective bit lines. The sense/write circuit at the end of the bit 
lines sends the output to the processor. For **Write operation**,
 the address provided to the decoder activates the word line to close 
both the switches. Then the bit value that is to be written into the 
cell is provided through the sense/write circuit and the signals in bit 
lines are then stored in the cell.

**2. DRAM :**DRAM
 stores the binary information in the form of electric charges applied 
to capacitors. The stored information on the capacitors tends to lose 
over a period of time and thus the capacitors must be periodically 
recharged to retain their usage. The main memory is generally made up of
 DRAM chips.

**DRAM Memory Cell:** Though SRAM is 
very fast, but it is expensive because of its every cell requires 
several transistors. Relatively less expensive RAM is DRAM, due to the 
use of one transistor and one capacitor in each cell, as shown in the 
below figure., where C is the capacitor and T is the transistor. 
Information is stored in a DRAM cell in the form of a charge on a 
capacitor and this charge needs to be periodically recharged. For 
storing information in this cell, transistor T is turned on and an 
appropriate voltage is applied to the bit line. This causes a known 
amount of charge to be stored in the capacitor. After the transistor is 
turned off, due to the property of the capacitor, it starts to 
discharge. Hence, the information stored in the cell can be read 
correctly only if it is read before the charge on the capacitors drops 
below some threshold value. 

!https://media.geeksforgeeks.org/wp-content/uploads/DRAM.png

**Types of DRAM :**There are mainly 5 types of DRAM:

1. **Asynchronous DRAM (ADRAM) –**The DRAM described above is the asynchronous type DRAM. The timing of the
memory device is controlled asynchronously. A specialized memory
controller circuit generates the necessary control signals to control
the timing. The CPU must take into account the delay in the response of
the memory. 
2. **Synchronous DRAM (SDRAM) –**These RAM chips’ access speed is directly synchronized with the CPU’s clock.
For this, the memory chips remain ready for operation when the CPU
expects them to be ready. These memories operate at the CPU-memory bus
without imposing wait states. SDRAM is commercially available as modules incorporating multiple SDRAM chips and forming the required capacity
for the modules. 
3. **Double-Data-Rate SDRAM (DDR SDRAM) –**This faster version of SDRAM performs its operations on both edges of the
clock signal; whereas a standard SDRAM performs its operations on the
rising edge of the clock signal. Since they transfer data on both edges
of the clock, the data transfer rate is doubled. To access the data at
high rate, the memory cells are organized into two groups. Each group is accessed separately. 
4. **Rambus DRAM (RDRAM) –**The RDRAM provides a very high data transfer rate over a narrow CPU-memory
bus. It uses various speedup mechanisms, like synchronous memory
interface, caching inside the DRAM chips and very fast signal timing.
The Rambus data bus width is 8 or 9 bits. 
5. **Cache DRAM (CDRAM) –**This memory is a special type DRAM memory with an on-chip cache memory
(SRAM) that acts as a high-speed buffer for the main DRAM. 

**Difference between SRAM and DRAM :**Below table lists some of the differences between SRAM and DRAM: 

!https://media.geeksforgeeks.org/wp-content/uploads/SRAM-DRAM-1.png

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

*** Buddy System – Memory allocation technique
Prerequisite – [Partition Allocation Methods](https://www.geeksforgeeks.org/operating-system-memory-management-partition-allocation-method/)**Static partition** schemes suffer from the **limitation** of having the fixed number of active processes and the usage of space may also not be optimal. The **buddy system** is a memory allocation and management algorithm that manages memory in **power of two increments**. Assume the memory size is 2U, suppose a size of S is required.

- **If 2U-1<S<=2U:** Allocate the whole block
- **Else:** Recursively divide the block equally and test the condition at each
time, when it satisfies, allocate the block and get out the loop.

System also keep the record of all the unallocated blocks each and can merge these different size blocks to make one big chunk.**Advantage –**

- Easy to implement a buddy system
- Allocates block of correct size
- It is easy to merge adjacent holes
- Fast to allocate memory and de-allocating memory

**Disadvantage –**

- It requires all allocation unit to be powers of two
- It leads to internal fragmentation

**Example –**Consider a system having buddy system with physical address space 128 KB.Calculate the size of partition for 18 KB process.**Solution –**

!https://media.geeksforgeeks.org/wp-content/uploads/Capture-34.png

So, size of partition for 18 KB process = 32 KB. It divides by 2, till possible to get minimum block to fit 18 KB.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

[Previous](https://www.geeksforgeeks.org/memory-hierarchy-design-and-its-characteristics/)

[Memory Hierarchy Design and its Characteristics](https://www.geeksforgeeks.org/memory-hierarchy-design-and-its-characteristics/)

[Next](https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/)

[Fixed (or static) Partitioning in Operating System](https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/)
*** Partition Allocation Methods in Memory Management
In the operating system, the following are four common memory management techniques.

**Single contiguous allocation:** Simplest allocation method used by MS-DOS. All memory (except some reserved for OS) is available to a process.

**Partitioned allocation:** Memory is divided into different blocks or partitions. Each process is allocated according to the requirement.

**Paged memory management:** Memory is divided into fixed-sized units called page frames, used in a virtual memory environment.

**Segmented memory management:**
 Memory is divided into different segments (a segment is a logical 
grouping of the process’ data or code).In this management, allocated 
memory doesn’t have to be contiguous.

Most of the operating 
systems (for example Windows and Linux) use Segmentation with Paging. A 
process is divided into segments and individual segments have pages.

In **Partition Allocation**,
 when there is more than one partition freely available to accommodate a
 process’s request, a partition must be selected. To choose a particular
 partition, a partition allocation method is needed. A partition 
allocation method is considered better if it avoids internal 
fragmentation.

When it is time to load a process into the main 
memory and if there is more than one free block of memory of sufficient 
size then the OS decides which free block to allocate.

There are different Placement Algorithm:

A. First Fit

B. Best Fit

C. Worst Fit

D. Next Fit

**1. First Fit**:
 In the first fit, the partition is allocated which is the first 
sufficient block from the top of Main Memory. It scans memory from the 
beginning and chooses the first available block that is large enough. 
Thus it allocates the first hole that is large enough.

!https://media.geeksforgeeks.org/wp-content/uploads/20200524132212/FIRST-FIT-300x225.png

**2. Best Fit** Allocate
 the process to the partition which is the first smallest sufficient 
partition among the free available partition. It searches the entire 
list of holes to find the smallest hole whose size is greater than or 
equal to the size of the process.

!https://media.geeksforgeeks.org/wp-content/uploads/20200524132547/BEST-FIT-300x225.png

**3. Worst Fit** Allocate
 the process to the partition which is the largest sufficient among the 
freely available partitions available in the main memory. It is opposite
 to the best-fit algorithm. It searches the entire list of holes to find
 the largest hole and allocate it to process.

!https://media.geeksforgeeks.org/wp-content/uploads/20200524132634/WORST-FIT-300x225.png

**4. Next Fit:** Next fit is similar to the first fit but it will search for the first sufficient partition from the last allocation point.

**Is Best-Fit really best?** Although
 best fit minimizes the wastage space, it consumes a lot of processor 
time for searching the block which is close to the required size. Also, 
Best-fit may perform poorer than other algorithms in some cases. For 
example, see the below exercise.

**Exercise:** Consider
 the requests from processes in given order 300K, 25K, 125K, and 50K. 
Let there be two blocks of memory available of size 150K followed by a 
block size 350K.

Which of the following partition allocation schemes can satisfy the above requests? A) Best fit but not first fit. B) First fit but not best fit. C) Both First fit & Best fit. D) neither first fit nor best fit.

Solution: Let us try all options. Best Fit: 300K is allocated from a block of size 350K. 50 is left in the block. 25K is allocated from the remaining 50K block. 25K is left in the block. 125K is allocated from 150 K block. 25K is left in this block also. 50K can’t be allocated even if there is 25K + 25K space available.

First Fit: 300K request is allocated from 350K block, 50K is left out. 25K is be allocated from the 150K block, 125K is left out. Then 125K and 50K are allocated to the remaining left out partitions. So, the first fit can handle requests.

So option B is the correct choice.
*** Fixed (or static) Partitioning in Operating System
In 
operating systems, Memory Management is the function responsible for 
allocating and managing a computer’s main memory. Memory Management 
function keeps track of the status of each memory location, either 
allocated or free to ensure effective and efficient use of Primary 
Memory.

There are two Memory Management Techniques: **Contiguous**, and **Non-Contiguous**.
 In Contiguous Technique, executing process must be loaded entirely in 
the main memory. Contiguous Technique can be divided into:

1. Fixed (or static) partitioning 
2. Variable (or dynamic) partitioning 

**Fixed Partitioning:** This
 is the oldest and simplest technique used to put more than one process 
in the main memory. In this partitioning, the number of partitions 
(non-overlapping) in RAM is **fixed but the size** of each partition may or **may not be the same**. As it is a **contiguous** allocation, hence no spanning is allowed. Here partitions are made before execution or during system configure.

!https://media.geeksforgeeks.org/wp-content/uploads/444-4.png

As illustrated in above figure, first process is only consuming 1MB out of 4MB in the main memory. Hence, Internal Fragmentation in first block is (4-1) = 3MB. Sum of Internal Fragmentation in every block = (4-1)+(8-7)+(8-7)+(16-14)= 3+1+1+2 = 7MB.

Suppose
 process P5 of size 7MB comes. But this process cannot be accommodated 
in spite of available free space because of contiguous allocation (as 
spanning is not allowed). Hence, 7MB becomes part of External 
Fragmentation.

There are some advantages and disadvantages of fixed partitioning.

**Advantages of Fixed Partitioning –**

1. **Easy to implement:** Algorithms needed to implement Fixed Partitioning are easy to implement. It simply requires putting a process into a certain partition without focusing on the emergence of Internal and External Fragmentation. 
2. **Little OS overhead:** Processing of Fixed Partitioning requires lesser excess and indirect computational power. 

**Disadvantages of Fixed Partitioning –**

1. **Internal Fragmentation:** Main memory use is inefficient. Any program, no matter how small, occupies
an entire partition. This can cause internal fragmentation. 
2. **External Fragmentation:** The total unused space (as stated above) of various partitions cannot be
used to load the processes even though there is space available but not
in the contiguous form (as spanning is not allowed). 
3. **Limit process size:** Process of size greater than the size of the partition in Main Memory cannot be accommodated. The partition size cannot be varied according to the size of the incoming process size. Hence, the process size of 32MB in the
above-stated example is invalid. 
4. **Limitation on Degree of Multiprogramming:** Partitions in Main Memory are made before execution or during system configure.
Main Memory is divided into a fixed number of partitions. Suppose if
there are partitions in RAM and are the number of processes, then condition must be fulfilled. Number of processes greater than the number of partitions in RAM is invalid in Fixed Partitioning.
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-76c2b62fb1ba7724f0a10da48b6d7b12_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-34b58483818a88cf1625de858193d82b_l3.svg
    
    !https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-20f611e94b287ea945c6c298baab8292_l3.svg
    
*** Variable (or dynamic) Partitioning in Operating System
In 
operating systems, Memory Management is the function responsible for 
allocating and managing computer’s main memory. Memory Management 
function keeps track of the status of each memory location, either 
allocated or free to ensure effective and efficient use of Primary 
Memory.

There are two Memory Management Techniques: **Contiguous**, and **Non-Contiguous**. In Contiguous Technique, executing process must be loaded entirely in main-memory. Contiguous Technique can be divided into:

1. [Fixed (or static) partitioning](https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/)
2. Variable (or dynamic) partitioning

**Variable Partitioning –**It
 is a part of Contiguous allocation technique. It is used to alleviate 
the problem faced by Fixed Partitioning. In contrast with fixed 
partitioning, partitions are not made before the execution or during 
system configure. Various **features** associated with variable Partitioning-

1. Initially RAM is empty and partitions are made during the run-time according to
process’s need instead of partitioning during system configure.
2. The size of partition will be equal to incoming process.
3. The partition size varies according to the need of the process so that the
internal fragmentation can be avoided to ensure efficient utilisation of RAM.
4. Number of partitions in RAM is not fixed and depends on the number of incoming process and Main Memory’s size.

!https://media.geeksforgeeks.org/wp-content/uploads/111-10.png

There are some advantages and disadvantages of variable partitioning over fixed partitioning as given below.

**Advantages of Variable Partitioning –**

1. **No Internal Fragmentation:**In variable Partitioning, space in main memory is allocated strictly
according to the need of process, hence there is no case of internal
fragmentation. There will be no unused space left in the partition.
2. **No restriction on Degree of Multiprogramming:**More number of processes can be accommodated due to absence of internal
fragmentation. A process can be loaded until the memory is empty.
3. **No Limitation on the size of the process:**In Fixed partitioning, the process with the size greater than the size of
the largest partition could not be loaded and process can not be divided as it is invalid in contiguous allocation technique. Here, In variable
partitioning, the process size can’t be restricted since the partition
size is decided according to the process size.

**Disadvantages of Variable Partitioning –**

1. **Difficult Implementation:**Implementing variable Partitioning is difficult as compared to Fixed Partitioning as it involves allocation of memory during run-time rather than during
system configure.
2. **External Fragmentation:**There will be external fragmentation inspite of absence of internal fragmentation.
    
    For
     example, suppose in above example- process P1(2MB) and process P3(1MB) 
    completed their execution. Hence two spaces are left i.e. 2MB and 1MB. 
    Let’s suppose process P5 of size 3MB comes. The empty space in memory 
    cannot be allocated as no spanning is allowed in contiguous allocation. 
    The rule says that process must be contiguously present in main memory 
    to get executed. Hence it results in External Fragmentation.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/222-12.png
    
    Now P5 of size 3 MB cannot be accommodated in spite of required available space because in contiguous no spanning is allowed.
    
*** Non-Contiguous Allocation in Operating System
Prerequisite – [Variable Partitioning](https://www.geeksforgeeks.org/variable-or-dynamic-partitioning-in-operating-system/), [Fixed Partitioning](https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/) [Paging](https://www.geeksforgeeks.org/operating-system-paging/) and [Segmentation](https://www.geeksforgeeks.org/operating-systems-segmentation/) are the two ways that allow a process’s physical address space to be non-contiguous. It has the **advantage**
 of reducing memory wastage but it increases the overheads due to 
address translation. It slows the execution of the memory because time 
is consumed in address translation.

In non-contiguous allocation, the Operating system needs to maintain the table which is called the **Page Table**
 for each process which contains the base address of each block that is 
acquired by the process in memory space. In non-contiguous memory 
allocation, different parts of a process are allocated to different 
places in Main Memory. Spanning is allowed which is not possible in 
other techniques like Dynamic or Static Contiguous memory allocation. 
That’s why paging is needed to ensure effective memory allocation. 
Paging is done to remove External Fragmentation.

There are five types of Non-Contiguous Allocation of Memory in the Operating System:

1. Paging
2. Multilevel Paging
3. Inverted Paging
4. Segmentation
5. Segmented Paging

**Working:** Here
 a process can be spanned across different spaces in the main memory in a
 non-consecutive manner. Suppose process P of size 4KB. Consider main 
memory has two empty slots each of size 2KB. Hence total free space is, 
2*2= 4 KB. In contiguous memory allocation, process P cannot be 
accommodated as spanning is not allowed.

In contiguous 
allocation, space in memory should be allocated to the whole process. If
 not, then that space remains unallocated. But in Non-Contiguous 
allocation, the process can be divided into different parts and hence 
filling the space in the main memory. In this example, process P can be 
divided into two parts of equal size – 2KB. Hence one part of process P 
can be allocated to the first 2KB space of main memory and the other 
part of the process can be allocated to the second 2KB space of main 
memory. The below diagram will explain in a better way:

!https://media.geeksforgeeks.org/wp-content/uploads/1-353.png

But,
 in what manner we divide a process to allocate them into main memory is
 very important to understand. The process is divided after analysing 
the number of empty spaces and their size in the main memory. Then only 
we do divide our process. It is a very time-consuming process. Their 
number as well as their sizes changing every time due to execution of 
already present processes in main memory.

In order to avoid this 
time-consuming process, we divide our process in secondary memory in 
advance before reaching the main memory for its execution. Every process
 is divided into various parts of equal size called Pages. We also 
divide our main memory into different parts of equal size called Frames.
 It is important to understand that:

```
Size of page in process
= Size of frame in memory
```

Although their numbers can be 
different. Below diagram will make you understand it in a better way: 
consider empty main memory having a size of each frame is 2 KB, and two 
processes P1 and P2 are 2 KB each.

!https://media.geeksforgeeks.org/wp-content/uploads/1-1-1.png

Resolvent main memory,

!https://media.geeksforgeeks.org/wp-content/uploads/2222-6.png

Concluding,
 we can say that Paging allows the memory address space of a process to 
be non-contiguous. Paging is more flexible as only pages of a process 
are moved. It allows more processes to reside in main memory than 
Contiguous memory allocation.
*** Logical and Physical Address in Operating System
**Logical Address**
 is generated by CPU while a program is running. The logical address is 
virtual address as it does not exist physically, therefore, it is also 
known as Virtual Address. This address is used as a reference to access 
the physical memory location by CPU. The term Logical Address Space is 
used for the set of all logical addresses generated by a program’s 
perspective. The hardware device called Memory-Management Unit is used for mapping logical address to its corresponding physical address.

**Physical Address**
 identifies a physical location of required data in a memory. The user 
never directly deals with the physical address but can access by its 
corresponding logical address. The user program generates the logical 
address and thinks that the program is running in this logical address 
but the program needs physical memory for its execution, therefore, the 
logical address must be mapped to the physical address by MMU before 
they are used. The term Physical Address Space is used for all physical 
addresses corresponding to the logical addresses in a Logical address 
space.

!https://media.geeksforgeeks.org/wp-content/uploads/operating_system.png

[Mapping virtual-address to physical-addresses](https://www.geeksforgeeks.org/memory-management-mapping-virtual-address-physical-addresses/) **Differences Between Logical and Physical Address in Operating System**

1. The basic difference between Logical and physical address is that Logical
address is generated by CPU in perspective of a program whereas the
physical address is a location that exists in the memory unit.
2. Logical Address Space is the set of all logical addresses generated by CPU for a program whereas the set of all physical address mapped to corresponding logical addresses is called Physical Address Space.
3. The logical address does not exist physically in the memory whereas physical
address is a location in the memory that can be accessed physically.
4. Identical logical addresses are generated by Compile-time and Load time address
binding methods whereas they differs from each other in run-time address binding method. Please refer [this](https://www.geeksforgeeks.org/memory-management-mapping-virtual-address-physical-addresses/) for details.
5. The logical address is generated by the CPU while the program is running
whereas the physical address is computed by the Memory Management Unit
(MMU).

**Comparison Chart:** 

[](https://www.notion.so/821e413cda264f90bd2233e2ab0a2fb1?pvs=21)
*** Paging in Operating System
Paging
 is a memory management scheme that eliminates the need for contiguous 
allocation of physical memory. This scheme permits the physical address 
space of a process to be non – contiguous.

- Logical Address or Virtual Address (represented in bits): An address generated by the CPU
- Logical Address Space or Virtual Address Space( represented in words or bytes): The set of all logical addresses generated by a program
- Physical Address (represented in bits): An address actually available on memory unit
- Physical Address Space (represented in words or bytes): The set of all physical addresses corresponding to the logical addresses

The
 mapping from virtual to physical address is done by the memory 
management unit (MMU) which is a hardware device and this mapping is 
known as paging technique.

- The Physical Address Space is conceptually divided into a number of fixed-size blocks, called **frames**.
- The Logical address Space is also splitted into fixed-size blocks, called **pages**.
- Page Size = Frame Size

Let us consider an example:

- Physical Address = 12 bits, then Physical Address Space = 4 K words
- Logical Address = 13 bits, then Logical Address Space = 8 K words
- Page size = frame size = 1 K words (assumption)

!https://media.geeksforgeeks.org/wp-content/uploads/paging.jpg

Address generated by CPU is divided into

- **Page number(p):** Number of bits required to represent the pages in Logical Address Space or Page number
- **Page offset(d):** Number of bits required to represent particular word in a page or page
size of Logical Address Space or word number of a page or page offset.

Physical Address is divided into

- **Frame number(f):** Number of bits required to represent the frame of Physical Address Space or Frame number.
- **Frame offset(d):** Number of bits required to represent particular word in a frame or
frame size of Physical Address Space or word number of a frame or frame
offset.

The hardware implementation of page table 
can be done by using dedicated registers. But the usage of register for 
the page table is satisfactory only if page table is small. If page 
table contain large number of entries then we can use TLB(translation 
Look-aside buffer), a special, small, fast look up hardware cache.

- The TLB is associative, high speed memory.
- Each entry in TLB consists of two parts: a tag and a value.
- When this memory is used, then an item is compared with all tags
simultaneously.If the item is found, then corresponding value is
returned.

!https://media.geeksforgeeks.org/wp-content/uploads/paging-2.jpg

Main memory access time = mIf page table are kept in main memory,Effective access time = m(for page table) + m(for particular page in page table)

!https://media.geeksforgeeks.org/wp-content/uploads/paging-3.jpg

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Requirements of Memory Management System
Memory
 management keeps track of the status of each memory location, whether 
it is allocated or free. It allocates the memory dynamically to the 
programs at their request and frees it for reuse when it is no longer 
needed. Memory management meant to satisfy some requirements that we 
should keep in mind.

These Requirements of memory management are:

1. **Relocation –** The available memory is generally shared among a number of processes in a multiprogramming system, so it is not possible to know in advance
which other programs will be resident in main memory at the time of
execution of his program. Swapping the active processes in and out of
the main memory enables the operating system to have a larger pool of
ready-to-execute process.When a program gets swapped out to a
disk memory, then it is not always possible that when it is swapped back into main memory then it occupies the previous memory location, since
the location may still be occupied by another process. We may need to **relocate** the process to a different area of memory. Thus there is a possibility
that program may be moved in main memory due to swapping.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/aas6.png
    
    The
     figure depicts a process image. The process image is occupying a 
    continuous region of main memory. The operating system will need to know
     many things including the location of process control information, the 
    execution stack, and the code entry. Within a program, there are memory 
    references in various instructions and these are called logical 
    addresses.After loading of the program into main memory, the 
    processor and the operating system must be able to translate logical 
    addresses into physical addresses. Branch instructions contain the 
    address of the next instruction to be executed. Data reference 
    instructions contain the address of byte or word of data referenced.
    
2. **Protection –** There is always a danger when we have multiple programs at the same
time as one program may write to the address space of another program.
So every process must be protected against unwanted interference when
other process tries to write in a process whether accidental or
incidental. Between relocation and protection requirement a trade-off
occurs as the satisfaction of relocation requirement increases the
difficulty of satisfying the protection requirement.Prediction
of the location of a program in main memory is not possible, that’s why
it is impossible to check the absolute address at compile time to assure protection. Most of the programming language allows the dynamic
calculation of address at run time. The memory protection requirement
must be satisfied by the processor rather than the operating system
because the operating system can hardly control a process when it
occupies the processor. Thus it is possible to check the validity of
memory references.
3. **Sharing –** A protection
mechanism must have to allow several processes to access the same
portion of main memory. Allowing each processes access to the same copy
of the program rather than have their own separate copy has an
advantage.For example, multiple processes may use the same
system file and it is natural to load one copy of the file in main
memory and let it shared by those processes. It is the task of Memory
management to allow controlled access to the shared areas of memory
without compromising the protection. Mechanisms are used to support
relocation supported sharing capabilities.
4. **Logical organization –** Main memory is organized as linear or it can be a one-dimensional
address space which consists of a sequence of bytes or words. Most of
the programs can be organized into modules, some of those are
unmodifiable (read-only, execute only) and some of those contain data
that can be modified. To effectively deal with a user program, the
operating system and computer hardware must support a basic module to
provide the required protection and sharing. It has the following
advantages:
    - Modules are written and compiled independently
    and all the references from one module to another module are resolved by `the system at run time.
    - Different modules are provided with different degrees of protection.
    - There are mechanisms by which modules can be shared among processes. Sharing
    can be provided on a module level that lets the user specify the sharing that is desired.
5. **Physical organization –** The structure of computer memory has two levels referred to as main
memory and secondary memory. Main memory is relatively very fast and
costly as compared to the secondary memory. Main memory is volatile.
Thus secondary memory is provided for storage of data on a long-term
basis while the main memory holds currently used programs. The major
system concern between main memory and secondary memory is the flow of
information and it is impractical for programmers to understand this for two reasons:
    - The programmer may engage in a practice known
    as overlaying when the main memory available for a program and its data
    may be insufficient. It allows different modules to be assigned to the
    same region of memory. One disadvantage is that it is time-consuming for the programmer.
    - In a multiprogramming environment, the
    programmer does not know how much space will be available at the time of coding and where that space will be located inside the memory.
    
*** Memory Allocation Techniques | Mapping Virtual Addresses to Physical Addresses
Prerequisite : [Requirements of Memory Management System](https://www.geeksforgeeks.org/requirements-of-memory-management-system/), [Logical and Physical Address](https://www.geeksforgeeks.org/logical-and-physical-address-in-operating-system/)

**Memory Allocation Techniques:**To
 store the data and to manage the processes, we need a large-sized 
memory and, at the same time, we need to access the data as fast as 
possible. But if we increase the size of memory, the access time will 
also increase and, as we know, the CPU always generates addresses
 for secondary memory, i.e. logical addresses. But we want to access the
 main memory, so we need Address translation of logical address into 
physical address.The main memory interacts with both the user 
processes and the operating system.So we need to efficiently use the 
main memory.Main memory is divided into non-overlapping memory regions 
called partitions.

The main memory can be broadly allocated in two ways –

1. [Contiguous memory allocation](https://www.geeksforgeeks.org/implementation-of-contiguous-memory-management-techniques/)
2. [Non-Contiguous memory allocation](https://www.geeksforgeeks.org/non-contiguous-allocation-in-operating-system/)

**Contiguous memory allocation can be categorized into two ways :**

1. [Fixed partition scheme](https://www.geeksforgeeks.org/fixed-or-static-partitioning-in-operating-system/)
2. [Variable partition scheme.](https://www.geeksforgeeks.org/variable-or-dynamic-partitioning-in-operating-system/)

**Different Partition Allocation methods are used in Contiguous memory allocations –**

1. [First Fit](https://www.geeksforgeeks.org/first-fit-allocation-in-operating-systems/)
2. [Best Fit](https://www.geeksforgeeks.org/best-fit-allocation-in-operating-system/)
3. [Worst Fit](https://www.geeksforgeeks.org/worst-fit-allocation-in-operating-systems/)
4. [Next Fit](https://www.geeksforgeeks.org/program-for-next-fit-algorithm-in-memory-management/)

**Non-Contiguous memory allocation can be categorized into many ways :**

1. [Paging](https://www.geeksforgeeks.org/paging-in-operating-system/)
2. [Multilevel paging](https://www.geeksforgeeks.org/multilevel-paging-in-operating-system/)
3. [Inverted paging](https://www.geeksforgeeks.org/inverted-page-table-in-operating-system/)
4. [Segmentation](https://www.geeksforgeeks.org/segmentation-in-operating-system/)
5. [Segmented paging](https://www.geeksforgeeks.org/paged-segmentation-and-segmented-paging/)

**MMU(Memory Management Unit) :**The run time mapping between Virtual address and Physical Address is done by a hardware device known as MMU.In
 memory management, the Operating System will handle the processes and 
move the processes between disk and memory for execution . It keeps 
track of available and used memory.

**MMU scheme :**

```
 CPU------- MMU------Memory
```

!https://media.geeksforgeeks.org/wp-content/uploads/operating_system.png

Dynamic relocation using a relocation register.

1. CPU will generate logical address for eg: 346
2. MMU will generate a relocation register (base register) for eg: 14000
3. In memory, the physical address is located eg:(346+14000= 14346)

The
 value in the relocation register is added to every address generated by
 a user process at the time the address is sent to memory. The user 
program never sees the real physical addresses. The program can create a
 pointer to location 346, store it in memory, manipulate it, and compare
 it with other addresses—all like the number 346. The user program 
generates only logical addresses. However, these logical addresses must 
be mapped to physical addresses before they are used.

**Address binding :**Address
 binding is the process of mapping from one address space to another 
address space. Logical address is an address generated by the CPU during
 execution, whereas Physical Address refers to the location in the 
memory unit(the one that is loaded into memory).The logical address 
undergoes translation by the MMU or address translation unit in 
particular. The output of this process is the appropriate physical 
address or the location of code/data in RAM.

An address binding can be done in three different ways :

**Compile Time –** If
 you know that during compile time, where process will reside in memory,
 then an absolute address is generated. i.e The physical address is 
embedded to the executable of the program during compilation. Loading 
the executable as a process in memory is very fast. But if the generated
 address space is preoccupied by other processes, then the program 
crashes and it becomes necessary to recompile the program to change the 
address space.

**Load time –** If it is not known
 at the compile time where the process will reside, then a relocatable 
address will be generated. The loader translates the relocatable address
 to an absolute address. The base address of the process in main memory 
is added to all logical addresses by the loader to generate an absolute 
address. In this, if the base address of the process changes, then we 
need to reload the process again.

**Execution time –** The
 instructions are in memory and are being processed by the CPU. 
Additional memory may be allocated and/or deallocated at this time. This
 is used if a process can be moved from one memory to another during 
execution(dynamic linking-Linking that is done during load or run time).
 e.g – Compaction.

!https://media.geeksforgeeks.org/wp-content/uploads/20210531163111/addressbinding1.jpg

**Mapping Virtual Addresses to Physical Addresses :**In
 Contiguous memory allocation mapping from virtual addresses to physical
 addresses is not a difficult task, because if we take a process from 
secondary memory and copy it to the main memory, the addresses  will be 
stored in a contiguous manner, so if we know the base address of the 
process, we can find out the next addresses.

The Memory Management Unit is a combination of 2 registers –

1. Base Register (Relocation Register)
2. Limit Register.

**Base Register –** contains the starting physical address of the process.**Limit Register** -mentions the limit relative to the base address on the region occupied by the process.

The
 logical address generated by the CPU is first checked by the limit 
register, If the value of the logical address generated is less than the
 value of the limit register, the base address stored in the relocation 
register is added to the logical address to get the physical address of 
the memory location.If the logical address value is greater than the
 limit register, then the CPU traps to the OS, and the OS terminates the
 program by giving fatal error.

!https://media.geeksforgeeks.org/wp-content/uploads/20210531165409/addresstranslation.jpg

In
 Non Contiguous Memory allocation, processes can be allocated anywhere 
in available space. The address translation in non-contiguous memory 
allocation is difficult.There are several techniques used for address translation in non contiguous memory allocation like [Paging](https://www.geeksforgeeks.org/paging-in-operating-system/), [Multilevel paging](https://www.geeksforgeeks.org/multilevel-paging-in-operating-system/), [Inverted paging](https://www.geeksforgeeks.org/inverted-page-table-in-operating-system/), [Segmentation](https://www.geeksforgeeks.org/segmentation-in-operating-system/), [Segmented paging](https://www.geeksforgeeks.org/paged-segmentation-and-segmented-paging/). Different data structures and hardware support like TLB are required in these techniques.
*** Page Table Entries in Page Table
**Prerequisite –** [Paging](https://www.geeksforgeeks.org/operating-system-paging/)

Page
 table has page table entries where each page table entry stores a frame
 number and optional status (like protection) bits. Many of status bits 
used in the virtual memory system. The most **important** thing in PTE is **frame Number**.

**Page table entry has the following information –**

!https://media.geeksforgeeks.org/wp-content/uploads/Capture-24.png

1. **Frame Number –** It gives the frame number in which the current page you are looking for is present. The number of bits required depends on the number of
frames.Frame bit is also known as address translation bit.
    
    ```
    Number of bits for frame = Size of physical memory/frame size
    
    ```
    
2. **Present/Absent bit –** Present or absent
bit says whether a particular page you are looking for is present or
absent. In case if it is not present, that is called Page Fault. It is
set to 0 if the corresponding page is not in memory. Used to control
page fault by the operating system to support virtual memory. Sometimes
this bit is also known as **valid/invalid** bits.
3. **Protection bit –** Protection bit says that what kind of protection you want on that page. So, these bit for the protection of the page frame (read, write etc).
4. **Referenced bit –** Referenced bit will say whether this page has been referred in the last clock cycle or not. It is set to 1 by hardware when the page is
accessed.
5. **Caching enabled/disabled –** Some times we need the fresh data. Let us say the user is typing some information
from the keyboard and your program should run according to the input
given by the user. In that case, the information will come into the main memory. Therefore main memory contains the latest information which is
typed by the user. Now if you try to put that page in the cache, that
cache will show the old information. So whenever freshness is required,
we don’t want to go for caching or many levels of the memory.The
information present in the closest level to the CPU and the information
present in the closest level to the user might be different. So we want
the information has to be consistency, which means whatever information
user has given, CPU should be able to see it as first as possible. That
is the reason we want to disable caching. So, this bit **enables or disable** caching of the page.
6. **Modified bit –** Modified bit says whether the page has been modified or not. Modified
means sometimes you might try to write something on to the page. If a
page is modified, then whenever you should replace that page with some
other page, then the modified information should be kept on the hard
disk or it has to be written back or it has to be saved back. It is set
to 1 by hardware on write-access to page which is used to avoid writing
when swapped out. Sometimes this modified bit is also called as the **Dirty bit**.
*** Virtual Memory in Operating System
Virtual
 Memory is a storage allocation scheme in which secondary memory can be 
addressed as though it were part of the main memory. The addresses a 
program may use to reference memory are distinguished from the addresses
 the memory system uses to identify physical storage sites, and 
program-generated addresses are translated automatically to the 
corresponding machine addresses.

The
 size of virtual storage is limited by the addressing scheme of the 
computer system and the amount of secondary memory is available not by 
the actual number of the main storage locations.

!https://videocdn.geeksforgeeks.org/geeksforgeeks/VirtualMemoryinOperatingSystem/346VirtualMemoryinOS20220617163047.jpg

It
 is a technique that is implemented using both hardware and software. It
 maps memory addresses used by a program, called virtual addresses, into
 physical addresses in computer memory.

1. All memory
references within a process are logical addresses that are dynamically
translated into physical addresses at run time. This means that a
process can be swapped in and out of the main memory such that it
occupies different places in the main memory at different times during
the course of execution.
2. A process may be broken into a number
of pieces and these pieces need not be continuously located in the main
memory during execution. The combination of dynamic run-time address
translation and use of page or segment table permits this.

If
 these characteristics are present then, it is not necessary that all 
the pages or segments are present in the main memory during execution. 
This means that the required pages need to be loaded into memory 
whenever required. Virtual memory is implemented using Demand Paging or 
Demand Segmentation.

**Demand Paging :** The process of loading the page into memory on demand (whenever page fault occurs) is known as demand paging. The process includes the following steps :

!https://media.geeksforgeeks.org/wp-content/uploads/VirtualDiagram-1.png

1. If the CPU tries to refer to a page that is currently not available in the main memory, it generates an interrupt indicating a memory access
fault.
2. The OS puts the interrupted process in a blocking state.
For the execution to proceed the OS must bring the required page into
the memory.
3. The OS will search for the required page in the logical address space.
4. The required page will be brought from logical address space to physical
address space. The page replacement algorithms are used for the
decision-making of replacing the page in physical address space.
5. The page table will be updated accordingly.
6. The signal will be sent to the CPU to continue the program execution and it will place the process back into the ready state.

Hence
 whenever a page fault occurs these steps are followed by the operating 
system and the required page is brought into memory.

**Advantages :**

- More processes may be maintained in the main memory: Because we are going to load only some of the pages of any particular process, there is room
for more processes. This leads to more efficient utilization of the
processor because it is more likely that at least one of the more
numerous processes will be in the ready state at any particular time.
- A process may be larger than all of the main memory: One of the most
fundamental restrictions in programming is lifted. A process larger than the main memory can be executed because of demand paging. The OS itself loads pages of a process in the main memory as required.
- It allows greater multiprogramming levels by using less of the available (primary) memory for each process.

**Page Fault Service Time :** The
 time taken to service the page fault is called page fault service time.
 The page fault service time includes the time taken to perform all the 
above six steps.

```
Let Main memory access time is: m
Page fault service time is: s
Page fault rate is : p
Then, Effective memory access time = (p*s) + (1-p)*m
```

**Swapping:**

Swapping
 a process out means removing all of its pages from memory, or marking 
them so that they will be removed by the normal page replacement 
process. Suspending a process ensures that it is not runnable while it 
is swapped out. At some later time, the system swaps back the process 
from the secondary storage to the main memory. When a process is busy 
swapping pages in and out then this situation is called thrashing.

!https://media.geeksforgeeks.org/wp-content/uploads/VirtualDiagram-2.png

**Thrashing :**

!https://media.geeksforgeeks.org/wp-content/uploads/VirtualDiagram-3.png

At
 any given time, only a few pages of any process are in the main memory 
and therefore more processes can be maintained in memory. Furthermore, 
time is saved because unused pages are not swapped in and out of memory.
 However, the OS must be clever about how it manages this scheme. In the
 steady-state practically, all of the main memory will be occupied with 
process pages, so that the processor and OS have direct access to as 
many processes as possible. Thus when the OS brings one page in, it must
 throw another out. If it throws out a page just before it is used, then
 it will just have to get that page again almost immediately. Too much 
of this leads to a condition called Thrashing. The system spends most of
 its time swapping pages rather than executing instructions. So a good 
page replacement algorithm is required.

In the given diagram, the
 initial degree of multiprogramming up to some extent of point(lambda), 
the CPU utilization is very high and the system resources are utilized 
100%. But if we further increase the degree of multiprogramming the CPU 
utilization will drastically fall down and the system will spend more 
time only on the page replacement and the time is taken to complete the 
execution of the process will increase. This situation in the system is 
called thrashing.

**Causes of Thrashing :**

1. **High degree of multiprogramming** : If the number of processes keeps on increasing in the memory then the
number of frames allocated to each process will be decreased. So, fewer
frames will be available for each process. Due to this, a page fault
will occur more frequently and more CPU time will be wasted in just
swapping in and out of pages and the utilization will keep on
decreasing.
    
    For example: Let free frames = 400 **Case 1**: Number of process = 100 Then, each process will get 4 frames.
    
    **Case 2**: Number of processes = 400 Each process will get 1 frame. Case
     2 is a condition of thrashing, as the number of processes is increased,
     frames per process are decreased. Hence CPU time will be consumed in 
    just swapping pages. 
    
2. **Lacks of Frames**: If a process has fewer frames then fewer pages of that process will be
able to reside in memory and hence more frequent swapping in and out
will be required. This may lead to thrashing. Hence sufficient amount of frames must be allocated to each process in order to prevent thrashing.

**Recovery of Thrashing :**

- Do not allow the system to go into thrashing by instructing the long-term
scheduler not to bring the processes into memory after the threshold.
- If the system is already thrashing then instruct the mid-term scheduler to suspend some of the processes so that we can recover the system from
thrashing.
*** Memory Interleaving
Prerequisite – [Virtual Memory](https://www.geeksforgeeks.org/virtual-memory-operating-systems/) Abstraction is one of the most important aspects of computing. It is a widely implemented Practice in the Computational field.

**Memory Interleaving**
 is less or More an Abstraction technique. Though it’s a bit different 
from Abstraction. It is a Technique that divides memory into a number of
 modules such that Successive words in the address space are placed in 
the Different modules.

**Consecutive Word in a Module:**

!https://media.geeksforgeeks.org/wp-content/uploads/raw-1.png

**Figure-1:** Consecutive Word in a Module

Let
 us assume 16 Data’s to be Transferred to the Four Module. Where Module 
00 be Module 1, Module 01 be Module 2, Module 10 be Module 3 & 
Module 11 be Module 4. Also, 10, 20, 30….130 are the data to be 
transferred.

From the figure above in Module 1, 10 [Data] is 
transferred then 20, 30 & finally, 40 which are the Data. That means
 the data are added consecutively in the Module till its max capacity.

Most
 significant bit (MSB) provides the Address of the Module & the 
least significant bit (LSB) provides the address of the data in the 
module.

For **Example**, to get 90 (Data) 1000 will 
be provided by the processor. This 10 will indicate that the data is in 
module 10 (module 3) & 00 is the address of 90 in Module 10 (module 
3). So, 

```
Module 1 Contains Data : 10, 20, 30, 40
Module 2 Contains Data : 50, 60, 70, 80
Module 3 Contains Data : 90, 100, 110, 120
Module 4 Contains Data : 130, 140, 150, 160
```

**Consecutive Word in Consecutive Module:**

!https://media.geeksforgeeks.org/wp-content/uploads/raw-2.png

**Figure-2:** Consecutive Word in Consecutive Module

Now
 again we assume 16 Data’s to be transferred to the Four Module. But Now
 the consecutive Data are added in Consecutive Module. That is, 10 
[Data] is added in Module 1, 20 [Data] in Module 2 and So on.

Least
 Significant Bit (LSB) provides the Address of the Module & Most 
significant bit (MSB) provides the address of the data in the module.

For **Example**,
 to get 90 (Data) 1000 will be provided by the processor. This 00 will 
indicate that the data is in module 00 (module 1) & 10 is the 
address of 90 in Module 00 (module 1). That is, 

```
Module 1 Contains Data : 10, 50, 90, 130
Module 2 Contains Data : 20, 60, 100, 140
Module 3 Contains Data : 30, 70, 110, 150
Module 4 Contains Data : 40, 80, 120, 160
```

**Why do we use Memory Interleaving? [Advantages]:** Whenever
 Processor requests Data from the main memory. A block (chunk) of Data 
is Transferred to the cache and then to Processor. So whenever a cache 
miss occurs the Data is to be fetched from the main memory. But main 
memory is relatively slower than the cache. So to improve the access 
time of the main memory interleaving is used.

We can access all 
four Modules at the same time thus achieving Parallelism. From Figure 2 
the data can be acquired from the Module using the Higher bits. This 
method Uses memory effectively.
*** Virtual Memory | Questions
**Advantages**

- Large virtual memory.
- More efficient use of memory.
- Unconstrained multiprogramming. There is no limit on degree of multiprogramming.

**Disadvantages**

- Number of tables and amount of processor overhead for handling page interrupts are greater than in the case of the simple paged management techniques.
- Due to lack of an explicit constraint on a job’s address space size.

**A way to control Thrashing**

Set
 the lower and upper bounds of page fault rate for each process. Using 
the above step, establish ‘acceptable’ page fault rate.

- If actual rate is lower than lower bound, decrease the number of frames
- If actual rate is larger than upper bound, increase the number of frames.

**Q1.** Virtual memory is**(a)** Large secondary memory**(b)** Large main memory**(c)** Illusion of large main memory**(d)** None of the above

**Answer: (c)**Explanation: Virtual memory is illusion of large main memory.

**Q2.** Thrashing occurs when**(a)**When a page fault occurs**(b)** Processes on system frequently access pages not memory**(c)** Processes on system are in running state**(d)** Processes on system are in waiting state**Answer: (b)Explanation:**
 Thrashing occurs when processes on system require more memory than it 
has. If processes do not have “enough” pages, the page fault rate is 
very high. This leads to:– low CPU utilization– operating system spends most of its time swapping to diskThe above situation is called thrashing

**Q3.**
 A computer system supports 32-bit virtual addresses as well as 32-bit 
physical addresses. Since the virtual address space is of the same size 
as the physical address space, the operating system designers decide to 
get rid of the virtual memory entirely. Which one of the following is 
true?**(a)** Efficient implementation of multi-user support is no longer possible**(b)** The processor cache organization can be made more efficient now**(c)** Hardware support for memory management is no longer needed**(d)** CPU scheduling can be made more efficient now**Answer: (c)Explanation:** For
 supporting virtual memory, special hardware support is needed from 
Memory Management Unit. Since operating system designers decide to get 
rid of the virtual memory entirely, hardware support for memory 
management is no longer needed.

This article is contributed by **Mithlesh Upadhyay**

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Operating system based Virtualization
Prerequisites – [Types of Server Virtualization](https://www.geeksforgeeks.org/computer-network-types-server-virtualization/), [Hardware based Virtualization](https://www.geeksforgeeks.org/hardware-based-virtualization/) Operating
 system-based Virtualization refers to an operating system feature in 
which the kernel enables the existence of various isolated user-space 
instances. The installation of virtualization software also refers to 
Operating system-based virtualization. It is installed over a 
pre-existing operating system and that operating system is called the 
host operating system.

In
 this virtualization, a user installs the virtualization software in the
 operating system of his system like any other program and utilizes this
 application to operate and generate various virtual machines. Here, the
 virtualization software allows direct access to any of the created 
virtual machines to the user. As the host OS can provide hardware 
devices with the mandatory support, operating system virtualization may 
affect compatibility issues of hardware even when the hardware driver is
 not allocated to the virtualization software.

Virtualization 
software is able to convert hardware IT resources that require unique 
software for operation into virtualized IT resources. As the host OS is a
 complete operating system in itself, many OS-based services are 
available as organizational management and administration tools can be 
utilized for the virtualization host management.

!https://media.geeksforgeeks.org/wp-content/uploads/33-11.png

Some major operating system-based services are mentioned below:

1. Backup and Recovery.
2. Security Management.
3. Integration to Directory Services.

Various major operations of Operating System Based Virtualization are described below:

1. Hardware capabilities can be employed, such as the network connection and CPU.
2. Connected peripherals with which it can interact, such as webcam, printer, keyboard, or Scanners.
3. Data that can be read or written, such as files, folders, and network shares.

The
 Operating system may have the capability to allow or deny access to 
such resources based on which the program requests them and the user 
account in the context of which it runs. OS may also hide these 
resources, which leads that when a computer program computes them, they 
do not appear in the enumeration results. Nevertheless, from a 
programming perspective, the computer program has interacted with those 
resources and the operating system has managed an act of interaction.

With
 operating-system-virtualization or containerization, it is probable to 
run programs within containers, to which only parts of these resources 
are allocated. A program that is expected to perceive the whole 
computer, once run inside a container, can only see the allocated 
resources and believes them to be all that is available. Several 
containers can be formed on each operating system, to each of which a 
subset of the computer’s resources is allocated. Each container may 
include many computer programs. These programs may run parallel or 
distinctly, even interrelate with each other.

Operating system-based virtualization can raise demands and problems related to performance overhead, such as:

1. The host operating system employs CPU, memory, and other hardware IT resources.
2. Hardware-related calls from guest operating systems need to navigate numerous layers to
and from the hardware, which shrinkage overall performance.
3. Licenses are frequently essential for host operating systems, in addition to
individual licenses for each of their guest operating systems.
*** Inverted Page Table in Operating System
Prerequisites – [Paging](https://www.geeksforgeeks.org/operating-system-paging/), [Page table entries](https://www.geeksforgeeks.org/operating-system-page-table-entries/), [Segmentation](https://www.geeksforgeeks.org/operating-systems-segmentation/)

Most
 Operating Systems implement a separate page table for each process, 
i.e. for ‘n’ number of processes running on a Multiprocessing/ 
Timesharing operating system, there is ‘n’ number of page tables stored 
in the memory. Sometimes when a process is very large in size and it 
occupies virtual memory then with the size of the process, its page 
table size also increases substantially.

```
Example: A process of size 2 GB with:
Page size = 512 Bytes
Size of page table entry = 4 Bytes, then
Number of pages in the process = 2 GB / 512 B = 222
PageTable Size = 222 * 22 = 224 bytes
```

Through
 this example, it can be concluded that for multiple processes running 
simultaneously in an OS, a considerable part of memory is occupied by 
page tables only. Operating Systems also incorporate **multilevel paging schemes**
 which further increase the space required for storing the page tables 
and a large amount of memory is invested in storing them. The amount of 
memory occupied by the page tables can turn out to be a huge overhead 
and is always unacceptable as main memory is always a scarce resource. 
Various efforts are made to utilize the memory efficiently and to 
maintain a good balance in the level of multiprogramming and efficient 
CPU utilization.

**Inverted Page Table –** An alternate approach is to use the **Inverted Page Table**
 structure that consists of a one-page table entry for every frame of 
the main memory. So the number of page table entries in the Inverted 
Page Table reduces to the number of frames in physical memory and a 
single page table is used to represent the paging information of all the
 processes. Through the inverted page table, the overhead of storing an 
individual page table for every process gets eliminated and only a fixed
 portion of memory is required to store the paging information of all 
the processes together. This technique is called inverted paging as the 
indexing is done with respect to the frame number instead of the logical
 page number. Each entry in the page table contains the following 
fields.

- **Page number –** It specifies the page number range of the logical address.
- **Process id –** An inverted page table contains the address space information of all
the processes in execution. Since two different processes can have a
similar set of virtual addresses, it becomes necessary in the Inverted
Page Table to store a process Id of each process to identify its address space uniquely. This is done by using the combination of PId and Page
Number. So this Process Id acts as an address space identifier and
ensures that a virtual page for a particular process is mapped correctly to the corresponding physical frame.
- **Control bits –** These bits are used to store extra paging-related information. These
include the valid bit, dirty bit, reference bits, protection, and
locking information bits.
- **Chained pointer –** It
may be possible sometimes that two or more processes share a part of the main memory. In this case, two or more logical pages map to the same
Page Table Entry then a chaining pointer is used to map the details of
these logical pages to the root page table.

**Working –**

The operation of an inverted page table is shown below.

!https://media.geeksforgeeks.org/wp-content/uploads/33-6.png

The virtual address generated by the CPU contains the fields and each 
page table entry contains the other relevant information required in 
paging related mechanism. When a memory reference takes place, this 
virtual address is matched by the Memory-Mapping unit(MMU), the Inverted
 Page table is searched and the corresponding frame number is obtained. 
If the match is found at the i

th

entry then the physical address of the process

*is sent as the real address otherwise if no match is found then Segmentation Fault is generated.*

**Note: Number of Entries in Inverted page table = Number of frames in Physical address Space(PAS).**

**Examples –**
 The Inverted Page table and its variations are implemented in various 
systems like PowerPC, UltraSPARC, and the IA-64 architecture. An 
implementation of the Mach operating system on the RT-PC also uses this 
technique.

**Advantages and Disadvantages:**

- **Reduced memory space –** Inverted Pagetables typically reduce the amount of memory required to
store the page tables to a size bound of physical memory. The maximum
number of entries could be the number of page frames in the physical
memory.
- **Longer lookup time –** Inverted Page
tables are sorted in order of frame number but the memory look-up takes
place with respect to the virtual address, so, it usually takes a longer time to find the appropriate entry but often these page tables are
implemented using hash data structures for a faster lookup.
- **Difficult shared memory implementation –** As the Inverted Page Table stores a single entry for each frame, it
becomes difficult to implement the shared memory in the page tables.
Chaining techniques are used to map more than one virtual address to the entry specified in the order of frame number.
*** Swap Space in Operating System
Swap Space in Operating System
A 
computer has a sufficient amount of physical memory but most of the time
 we need more so we swap some memory on disk. Swap space is a space on a
 hard disk that is a substitute for physical memory. It is used as 
virtual memory which contains process memory images. Whenever our 
computer runs short of physical memory it uses its virtual memory and 
stores information in memory on disk. Swap space helps the computer’s 
operating system in pretending that it has more RAM than it actually 
has. It is also called a swap file. This interchange of data between 
virtual memory and real memory is called swapping and space on disk as 
“swap space”.

Virtual memory is a combination of RAM and disk space that running processes can use. **Swap space** is the **portion of virtual memory** that is on the hard disk, used when RAM is full.

Swap space can be useful to computers in various ways: 

- It can be used as a single contiguous memory which reduces I/O operations to read or write a file.
- Applications that are not used or are used less can be kept in a swap file.
- Having sufficient swap files helps the system keep some physical memory free all the time.
- The space in physical memory which has been freed due to swap space can be used by OS for some other important tasks.

Operating
 systems such as Windows, Linux, etc systems provide a certain amount of
 swap space by default which can be changed by users according to their 
needs. If you don’t want to use virtual memory you can easily disable it
 all together but in case if you run out of memory then the kernel will 
kill some of the processes in order to create a sufficient amount of 
space in physical memory. So it totally depends upon the user whether he
 wants to use swap space or not.
 
*** Page Fault Handling in Operating System
A 
page fault occurs when a program attempts to access data or code that is
 in its address space, but is not currently located in the system RAM. 
So when page fault occurs then following sequence of events happens :

!https://media.geeksforgeeks.org/wp-content/uploads/121-1.png

- The computer hardware traps to the kernel and program counter (PC) is saved on the stack. Current instruction state information is saved in CPU
registers.
- An assembly program is started to save the general registers and other volatile information to keep the OS from destroying it.
- Operating system finds that a page fault has occurred and tries to find out which virtual page is needed. Some times hardware register contains this
required information. If not, the operating system must retrieve PC,
fetch instruction and find out what it was doing when the fault
occurred.
- Once virtual address caused page fault is known,
system checks to see if address is valid and checks if there is no
protection access problem.
- If the virtual address is valid, the
system checks to see if a page frame is free. If no frames are free, the page replacement algorithm is run to remove a page.
- If frame
selected is dirty, page is scheduled for transfer to disk, context
switch takes place, fault process is suspended and another process is
made to run until disk transfer is completed.
- As soon as page
frame is clean, operating system looks up disk address where needed page is, schedules disk operation to bring it in.
- When disk
interrupt indicates page has arrived, page tables are updated to reflect its position, and frame marked as being in normal state.
- Faulting instruction is backed up to state it had when it began and PC is reset. Faulting is scheduled, operating system returns to routine that called
it.
- Assembly Routine reloads register and other state information, returns to user space to continue execution.
*** Segmentation in Operating System
A
 process is divided into Segments. The chunks that a program is divided 
into which are not necessarily all of the same sizes are called 
segments. Segmentation gives user’s view of the process which paging 
does not give. Here the user’s view is mapped to physical memory.There are types of segmentation:

1. **Virtual memory segmentation –**Each process is divided into a number of segments, not all of which are resident at any one point in time.
2. **Simple segmentation –**Each process is divided into a number of segments, all of which are loaded
into memory at run time, though not necessarily contiguously.

There
 is no simple relationship between logical addresses and physical 
addresses in segmentation. A table stores the information about all such
 segments and is called Segment Table.

**Segment Table –** It maps two-dimensional Logical address into one-dimensional Physical address. It’s each table entry has:

- **Base Address:** It ****contains the starting physical address where the segments reside in memory.
- **Limit:** It specifies the length of the segment.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/02/segmentation.png

Translation of Two dimensional Logical Address to one dimensional Physical Address.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/gq/2016/02/Translation.png

Address generated by the CPU is divided into:

- **Segment number (s):** Number of bits required to represent the segment.
- **Segment offset (d):** Number of bits required to represent the size of the segment.

**Advantages of Segmentation –**

- No Internal fragmentation.
- Segment Table consumes less space in comparison to Page table in paging.

**Disadvantage of Segmentation –**

- As processes are loaded and removed from the memory, the free memory space is broken into little pieces, causing External fragmentation.
  
*** Memory Segmentation in 8086 Microprocessor
Prerequisite – [Segmentation](https://www.geeksforgeeks.org/operating-systems-segmentation/)**Segmentation**
 is the process in which the main memory of the computer is logically 
divided into different segments and each segment has its own base 
address. It is basically used to enhance the speed of execution of the 
computer system, so that the processor is able to fetch and execute the 
data from the memory easily and fast.

**Need for Segmentation –**The Bus Interface Unit (BIU) contains four 16 bit special purpose registers (mentioned below) called as Segment Registers.

- **Code segment register (CS):** is used for addressing memory location in the code segment of the memory, where the executable program is stored.
- **Data segment register (DS):** points to the data segment of the memory where the data is stored.
- **Extra Segment Register (ES):** also refers to a segment in the memory which is another data segment in the memory.
- **Stack Segment Register (SS):** is used for addressing stack segment of the memory. The stack segment
is that segment of memory which is used to store stack data.

The
 number of address lines in 8086 is 20, 8086 BIU will send 20bit 
address, so as to access one of the 1MB memory locations. The four 
segment registers actually contain the upper 16 bits of the starting 
addresses of the four memory segments of 64 KB each with which the 8086 
is working at that instant of time. A segment is a logical unit of 
memory that may be up to 64 kilobytes long. Each segment is made up of 
contiguous memory locations. It is an independent, separately 
addressable unit. Starting address will always be changing. It will not 
be fixed.

Note that the 8086 does not work the whole 1MB memory at
 any given time. However, it works only with four 64KB segments within 
the whole 1MB memory.

Below is the one way of positioning four 64 kilobyte segments within the 1M byte memory space of an 8086.

!https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2018-05-10-09-52-11.png

**Types Of Segmentation –**

1. **Overlapping Segment –** A segment starts at a particular address and its maximum size can go up to 64kilobytes. But if another segment starts along with this
64kilobytes location of the first segment, then the two are said to be *Overlapping Segment*.
2. **Non-Overlapped Segment –** A segment starts at a particular address and its maximum size can go up to 64kilobytes. But if another segment starts before this 64kilobytes
location of the first segment, then the two segments are said to be *Non-Overlapped Segment*.

**Rules of Segmentation** Segmentation process follows some rules as follows:

- The starting address of a segment should be such that it can be evenly divided by 16.
- Minimum size of a segment can be 16 bytes and the maximum can be 64 kB.

!https://media.geeksforgeeks.org/wp-content/uploads/20190528231429/Screenshot-10521-300x86.png

**Advantages of the Segmentation** The main advantages of segmentation are as follows:

- It provides a powerful memory management mechanism.
- Data related or stack related operations can be performed in different segments.
- Code related operation can be done in separate code segments.
- It allows to processes to easily share data.
- It allows to extend the address ability of the processor, i.e.
segmentation allows the use of 16 bit registers to give an addressing
capability of 1 Megabytes. Without segmentation, it would require 20 bit registers.
- It is possible to enhance the memory size of code
data or stack segments beyond 64 KB by allotting more than one segment
for each area.
*** Program for Next Fit algorithm in Memory Management
Prerequisite: [Partition allocation methods](https://www.geeksforgeeks.org/partition-allocation-methods-in-memory-management/) **What is Next Fit ?** Next fit is a modified version of [‘first fit’](https://www.geeksforgeeks.org/program-first-fit-algorithm-memory-management/).
 It begins as the first fit to find a free partition but when called 
next time it starts searching from where it left off, not from the 
beginning. This policy makes use of a roving pointer. The pointer moves 
along the memory chain to search for a next fit. This helps in, to avoid
 the usage of memory always from the head (beginning) of the free block 
chain. **What are its advantage over first fit ?** 

- First fit is a straight and fast algorithm, but tends to cut large portion of free parts into small pieces due to which, processes that need a large
portion of memory block would not get anything even if the sum of all
small pieces is greater than it required which is so-called external
fragmentation problem.
- Another problem of the first fit is that
it tends to allocate memory parts at the beginning of the memory, which
may lead to more internal fragments at the beginning. Next fit tries to
address this problem by starting the search for the free portion of
parts not from the start of the memory, but from where it ends last
time.
- Next fit is a very fast searching algorithm and is also
comparatively faster than First Fit and Best Fit Memory Management
Algorithms.

```
Example:
Input :  blockSize[] = {5, 10, 20};
     processSize[] = {10, 20, 30};
Output:
Process No.     Process Size    Block no.
 1              10              2
 2              20              3
 3              30              Not Allocated
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

**Algorithm:** 

1. Input the number of memory blocks and their sizes and initializes all the blocks as free.
2. Input the number of processes and their sizes.
3. Start by picking each process and check if it can be assigned to the current
block, if yes, allocate it the required memory and check for next
process but from the block where we left not from starting.
4. If the current block size is smaller then keep checking the further blocks.

!https://media.geeksforgeeks.org/wp-content/uploads/next-fit-algorithm-operating-system.png

`// C/C++ program for next fit`

`// memory management algorithm`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Function to allocate memory to blocks as per Next fit`

`// algorithm`

`void` `NextFit(int` `blockSize[], int` `m, int` `processSize[], int` `n)`

`{`

`// Stores block id of the block allocated to a`

`// process`

`int` `allocation[n], j = 0;`

`// Initially no block is assigned to any process`

`memset(allocation, -1, sizeof(allocation));`

`// pick each process and find suitable blocks`

`// according to its size ad assign to it`

`for` `(int` `i = 0; i < n; i++) {`

`// Do not start from beginning`

`while` `(j < m) {`

`if` `(blockSize[j] >= processSize[i]) {`

`// allocate block j to p[i] process`

`allocation[i] = j;`

`// Reduce available memory in this block.`

`blockSize[j] -= processSize[i];`

`break;`

`}`

`// mod m will help in traversing the blocks from`

`// starting block after we reach the end.`

`j = (j + 1) % m;`

`}`

`}`

`cout << "\nProcess No.\tProcess Size\tBlock no.\n";`

`for` `(int` `i = 0; i < n; i++) {`

`cout << " "` `<< i + 1 << "\t\t"` `<< processSize[i]`

`<< "\t\t";`

`if` `(allocation[i] != -1)`

`cout << allocation[i] + 1;`

`else`

`cout << "Not Allocated";`

`cout << endl;`

`}`

`}`

`// Driver program`

`int` `main()`

`{`

`int` `blockSize[] = { 5, 10, 20 };`

`int` `processSize[] = { 10, 20, 5 };`

`int` `m = sizeof(blockSize) / sizeof(blockSize[0]);`

`int` `n = sizeof(processSize) / sizeof(processSize[0]);`

`NextFit(blockSize, m, processSize, n);`

`return` `0;`

`}`

**Output:**

```
Process No.    Process Size    Block no.
 1               10              2
 2               20              3
 3               5               1
```

This article is contributed by **Akash Gupta**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Overlays in Memory Management
The 
main problem in Fixed partitioning is the size of a process has to be 
limited by the maximum size of the partition, which means a process can 
never be span over another.In order to solve this problem, earlier 
people have used some solution which is called as Overlays.

The concept of **overlays**
 is that whenever a process is running it will not use the complete 
program at the same time, it will use only some part of it. Then 
overlays concept says that whatever part you required, you load it and 
once the part is done, then you just unload it, means just pull it back 
and get the new part you required and run it. Formally, “The process of **transferring a block** of program code or other data into internal memory, replacing what is already stored”. Sometimes
 it happens that compare to the size of the biggest partition, the size 
of the program will be even more, then, in that case, you should go with
 overlays.

So overlay is a technique to run a program that is 
bigger than the size of the physical memory by keeping only those 
instructions and data that are needed at any given time.Divide the 
program into modules in such a way that not all modules need to be in 
the memory at the same time.

**Advantage –**

- Reduce memory requirement
- Reduce time requirement

**Disadvantage –**

- Overlap map must be specified by programmer
- Programmer must know memory requirement
- Overlapped module must be completely disjoint
- Programming design of overlays structure is complex and not possible in all cases

**Example –** The
 best example of overlays is assembler. Consider the assembler has 2 
passes, 2 pass means at any time it will be doing only one thing, either
 the 1st pass or the 2nd pass. This means it will finish 1st pass first 
and then 2nd pass.Let assume that available main memory size is 150KB 
and total code size is 200KB

```
Pass 1.......................70KB
Pass 2.......................80KB
Symbol table.................30KB
Common routine...............20KB
```

As the total code size is 
200KB and main memory size is 150KB, it is not possible to use 2 passes 
together.So, in this case, we should go with the overlays 
technique.According to the overlays concept at any time only one pass 
will be used and both the passes always need symbol table and common 
routine.Now the question is if overlays-driver* is 10KB, then what is 
the minimum partition size required?For pass 1 total memory needed is = 
(70KB + 30KB + 20KB + 10KB) = 130KB and for pass 2 total memory needed 
is = (80KB + 30KB + 20KB + 10KB) = 140KB.So if we have minimum 140KB 
size partition then we can run this code very easily.

- Overlays
driver:-It is the user responsibility to take care of overlaying, the
operating system will not provide anything.Which means the user should
write even what part is required in the 1st pass and once the 1st pass
is over, the user should write the code to pull out the pass 1 and load
the pass 2.That is what is the responsibility of the user, that is known as the Overlays driver.Overlays driver will just help us to move out
and move in the various part of the code.

**Question –** The overlay tree for a program is as shown below: 

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/os_overlaymemory.png

What will be the size of the partition (in physical memory) required to load (and run) this program? (a) 12 KB (b) 14 KB (c) 10 KB (d) 8 KB

**Explanation –** Using
 the overlay concept we need not actually have the entire program inside
 the main memory.Only we need to have the part which are required at 
that instance of time, either we need Root-A-D or Root-A-E or Root-B-F 
or Root-C-G part.

```
Root+A+D = 2KB + 4KB + 6KB = 12KB
Root+A+E = 2KB + 4KB + 8KB = 14KB
Root+B+F = 2KB + 6KB + 2KB = 10KB
Root+C+G = 2KB + 8KB + 4KB = 14KB
```

So if we have 14KB size of partition then we can run any of them. Answer -(b) 14KB

This article is contributed by **[Samit Mandal](https://auth.geeksforgeeks.org/profile.php?user=Samit%20Mandal)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](https://write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Page Replacement Algorithms in Operating Systems
In 
an operating system that uses paging for memory management, a page 
replacement algorithm is needed to decide which page needs to be 
replaced when a new page comes in.

**Page Fault:**
 A page fault happens when a running program accesses a memory page that
 is mapped into the virtual address space but not loaded in physical 
memory. Since actual physical memory is much smaller than virtual 
memory, page faults happen. In case of a page fault, Operating System 
might have to replace one of the existing pages with the newly needed 
page. Different page replacement algorithms suggest different ways to 
decide which page to replace. The target for all algorithms is to reduce
 the number of page faults.

!https://videocdn.geeksforgeeks.org/geeksforgeeks/PageReplacementAlgorithminOperatingSystems/319PageReplacementinOS20220608181319.jpg

### Page Replacement Algorithms:

**1. First In First Out (FIFO):** This
 is the simplest page replacement algorithm. In this algorithm, the 
operating system keeps track of all pages in the memory in a queue, the 
oldest page is in the front of the queue. When a page needs to be 
replaced page in the front of the queue is selected for removal.

**Example 1:** Consider page reference string 1, 3, 0, 3, 5, 6, 3 with 3 page frames.Find the number of page faults.

!https://media.geeksforgeeks.org/wp-content/uploads/20190412160604/fifo2.png

Initially, all slots are empty, so when 1, 3, 0 came they are allocated to the empty slots —> **3 Page Faults.** when 3 comes, it is already in memory so —> **0 Page Faults.** Then 5 comes, it is not available in memory so it replaces the oldest page slot i.e 1. —>**1 Page Fault.** 6 comes, it is also not available in memory so it replaces the oldest page slot i.e 3 —>**1 Page Fault.** Finally, when 3 come it is not available so it replaces 0 **1 page fault.**

**[Belady’s anomaly](https://www.geeksforgeeks.org/operating-system-beladys-anomaly/)** proves
 that it is possible to have more page faults when increasing the number
 of page frames while using the First in First Out (FIFO) page 
replacement algorithm.  For example, if we consider reference strings 
3, 2, 1, 0, 3, 2, 4, 3, 2, 1, 0, 4, and 3 slots, we get 9 total page 
faults, but if we increase slots to 4, we get 10-page faults.

**2. Optimal Page replacement:** In this algorithm, pages are replaced which would not be used for the longest duration of time in the future.

**Example-2:** Consider the page references 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 3 with 4 page frame. Find number of page fault.

!https://media.geeksforgeeks.org/wp-content/uploads/20190412160500/optimal.png

Initially, all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> **4 Page faults** 0 is already there so —> **0 Page fault.** when 3 came it will take the place of 7 because it is not used for the longest duration of time in the future.—>**1 Page fault.** 0 is already there so —> **0 Page fault.** 4 will takes place of 1 —> **1 Page Fault.**

Now for the further page reference string —> **0 Page fault** because they are already available in the memory. Optimal
 page replacement is perfect, but not possible in practice as the 
operating system cannot know future requests. The use of Optimal Page 
replacement is to set up a benchmark so that other replacement 
algorithms can be analyzed against it.

**3. Least Recently Used:** In this algorithm, page will be replaced which is least recently used.

**Example-3:** Consider the page reference string 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2, 3 with 4 page frames. Find number of page faults.

!https://media.geeksforgeeks.org/wp-content/uploads/20190412161533/optimal2.png

Initially, all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> **4 Page faults** 0 is already their so —> **0 Page fault.** when 3 came it will take the place of 7 because it is least recently used —>**1 Page fault** 0 is already in memory so —> **0 Page fault**. 4 will takes place of 1 —> **1 Page Fault** Now for the further page reference string —> **0 Page fault** because they are already available in the memory.

**4. Most Recently Used (MRU):** In this algorithm, page will be replaced which has been used recently. Belady’s anomaly can occur in this algorithm.
*** Program for Least Recently Used (LRU) Page Replacement algorithm
Prerequisite: [Page Replacement Algorithms](https://www.geeksforgeeks.org/operating-system-page-replacement-algorithm/)In
 operating systems that use paging for memory management, page 
replacement algorithm are needed to decide which page needed to be 
replaced when new page comes in. Whenever a new page is referred and not
 present in memory, page fault occurs and Operating System replaces one 
of the existing pages with newly needed page. Different page replacement
 algorithms suggest different ways to decide which page to replace. The 
target for all algorithms is to reduce number of page faults.In **L**east **R**ecently **U**sed
 (LRU) algorithm is a Greedy algorithm where the page to be replaced is 
least recently used. The idea is based on locality of reference, the 
least recently used page is not likely Let say the page reference string 7 0 1 2 0 3 0 4 2 3 0 3 2 . Initially we have 4 page slots empty. Initially all slots are empty, so when 7 0 1 2 are allocated to the empty slots —> **4 Page faults** 0 is already their so —> **0 Page fault.** when 3 came it will take the place of 7 because it is least recently used —>**1 Page fault** 0 is already in memory so —> **0 Page fault**. 4 will takes place of 1 —> **1 Page Fault** Now for the further page reference string —> **0 Page fault** because they are already available in the memory.

!https://media.geeksforgeeks.org/wp-content/uploads/LRU-page-replacement.png

***Given
 memory capacity (as number of pages it can hold) and a string 
representing pages to be referred, write a function to find number of 
page faults.***

[Recommended: Please solve it on “***PRACTICE*** ” first, before moving on to the solution.](https://practice.geeksforgeeks.org/problems/page-faults-in-lru/0) 

```
Letcapacity be the number of pages that
memory can hold.  Letset be the current
set of pages in memory.

1- Start traversing the pages.
 i)If set holds less pages than capacity.
   a) Insert page into the set one by one until
      the size  ofset reachescapacity or all
      page requests are processed.
   b) Simultaneously maintain the recent occurred
      index of each page in a map calledindexes.
   c) Increment page fault
 ii)ElseIf current page is present inset, do nothing.
Else
     a) Find the page in the set that was least
     recently used. We find it using index array.
     We basically need to replace the page with
     minimum index.
     b) Replace the found page with current page.
     c) Increment page faults.
     d) Update index of current page.

2. Return page faults.
```

Below is implementation of above steps.

`//C++ implementation of above algorithm`

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`// Function to find page faults using indexes`

`int` `pageFaults(int` `pages[], int` `n, int` `capacity)`

`{`

`// To represent set of current pages. We use`

`// an unordered_set so that we quickly check`

`// if a page is present in set or not`

`unordered_set<int> s;`

`// To store least recently used indexes`

`// of pages.`

`unordered_map<int, int> indexes;`

`// Start from initial page`

`int` `page_faults = 0;`

`for` `(int` `i=0; i<n; i++)`

`{`

`// Check if the set can hold more pages`

`if` `(s.size() < capacity)`

`{`

`// Insert it into set if not present`

`// already which represents page fault`

`if` `(s.find(pages[i])==s.end())`

`{`

`s.insert(pages[i]);`

`// increment page fault`

`page_faults++;`

`}`

`// Store the recently used index of`

`// each page`

`indexes[pages[i]] = i;`

`}`

`// If the set is full then need to perform lru`

`// i.e. remove the least recently used page`

`// and insert the current page`

`else`

`{`

`// Check if current page is not already`

`// present in the set`

`if` `(s.find(pages[i]) == s.end())`

`{`

`// Find the least recently used pages`

`// that is present in the set`

`int` `lru = INT_MAX, val;`

`for` `(auto` `it=s.begin(); it!=s.end(); it++)`

`{`

`if` `(indexes[*it] < lru)`

`{`

`lru = indexes[*it];`

`val = *it;`

`}`

`}`

`// Remove the indexes page`

`s.erase(val);`

`// insert the current page`

`s.insert(pages[i]);`

`// Increment page faults`

`page_faults++;`

`}`

`// Update the current page index`

`indexes[pages[i]] = i;`

`}`

`}`

`return` `page_faults;`

`}`

`// Driver code`

`int` `main()`

`{`

`int` `pages[] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2};`

`int` `n = sizeof(pages)/sizeof(pages[0]);`

`int` `capacity = 4;`

`cout << pageFaults(pages, n, capacity);`

`return` `0;`

`}`

**Output:**

```
6
```

**Another approach:** (Without using HashMap) 

`// C++ program for page replacement algorithms`

`#include <iostream>`

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`int` `main()`

`{`

`int` `capacity = 4;`

`int` `arr[] = {7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2};`

`deque<int> q(capacity);`

`int` `count=0;`

`int` `page_faults=0;`

`deque<int>::iterator itr;`

`q.clear();`

`for(int` `i:arr)`

`{`

`// Insert it into set if not present`

`// already which represents page fault`

`itr = find(q.begin(),q.end(),i);`

`if(!(itr != q.end()))`

`{`

`++page_faults;`

`// Check if the set can hold equal pages`

`if(q.size() == capacity)`

`{`

`q.erase(q.begin());`

`q.push_back(i);`

`}`

`else{`

`q.push_back(i);`

`}`

`}`

`else`

`{`

`// Remove the indexes page`

`q.erase(itr);`

`// insert the current page`

`q.push_back(i);`

`}`

`}`

`cout<<page_faults;`

`}`

`// This code is contributed by Akshit Saxena`

**Output:** 

```
6
```

Note : We can also find the number of page hits. Just have to maintain a separate count. If the current page is already in the memory then that must be count as Page-hit.We will discuss other Page-replacement Algorithms in further sets.This article is contributed by **[Sahil Chhabra](https://www.facebook.com/sahil.chhabra.965)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Optimal Page Replacement Algorithm
Prerequisite: [Page Replacement Algorithms](https://www.geeksforgeeks.org/operating-system-page-replacement-algorithm/)

In
 operating systems, whenever a new page is referred and not present in 
memory, page fault occurs and Operating System replaces one of the 
existing pages with newly needed page. Different page replacement 
algorithms suggest different ways to decide which page to replace. The 
target for all algorithms is to reduce number of page faults. In this 
algorithm, OS replaces the page that will not be used for the longest 
period of time in future. Examples :

!https://videocdn.geeksforgeeks.org/geeksforgeeks/OptimalPageReplacementAlgorithminOS/OptimalPageReplacement20220627124822.jpg

```
Input : Number of frames, fn = 3
        Reference String, pg[] = {7, 0, 1, 2,
               0, 3, 0, 4, 2, 3, 0, 3, 2, 1,
               2, 0, 1, 7, 0, 1};
Output : No. of hits = 11
         No. of misses = 9

Input : Number of frames, fn = 4
        Reference String, pg[] = {7, 0, 1, 2,
                  0, 3, 0, 4, 2, 3, 0, 3, 2};
Output : No. of hits = 7
         No. of misses = 6
```

[Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

The idea is simple, for every reference we do following :

1. If referred page is already present, increment hit count.
2. If not present, find if a page that is never referenced in future. If such a page exists, replace this page with new page. If no such page exists, find a page that is referenced farthest in future. Replace this page
with new page.

!https://media.geeksforgeeks.org/wp-content/uploads/optimal_page.png

`// CPP program to demonstrate optimal page`

`// replacement algorithm.`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Function to check whether a page exists`

`// in a frame or not`

`bool` `search(int` `key, vector<int>& fr)`

`{`

`for` `(int` `i = 0; i < fr.size(); i++)`

`if` `(fr[i] == key)`

`return` `true;`

`return` `false;`

`}`

`// Function to find the frame that will not be used`

`// recently in future after given index in pg[0..pn-1]`

`int` `predict(int` `pg[], vector<int>& fr, int` `pn, int` `index)`

`{`

`// Store the index of pages which are going`

`// to be used recently in future`

`int` `res = -1, farthest = index;`

`for` `(int` `i = 0; i < fr.size(); i++) {`

`int` `j;`

`for` `(j = index; j < pn; j++) {`

`if` `(fr[i] == pg[j]) {`

`if` `(j > farthest) {`

`farthest = j;`

`res = i;`

`}`

`break;`

`}`

`}`

`// If a page is never referenced in future,`

`// return it.`

`if` `(j == pn)`

`return` `i;`

`}`

`// If all of the frames were not in future,`

`// return any of them, we return 0. Otherwise`

`// we return res.`

`return` `(res == -1) ? 0 : res;`

`}`

`void` `optimalPage(int` `pg[], int` `pn, int` `fn)`

`{`

`// Create an array for given number of`

`// frames and initialize it as empty.`

`vector<int> fr;`

`// Traverse through page reference array`

`// and check for miss and hit.`

`int` `hit = 0;`

`for` `(int` `i = 0; i < pn; i++) {`

`// Page found in a frame : HIT`

`if` `(search(pg[i], fr)) {`

`hit++;`

`continue;`

`}`

`// Page not found in a frame : MISS`

`// If there is space available in frames.`

`if` `(fr.size() < fn)`

`fr.push_back(pg[i]);`

`// Find the page to be replaced.`

`else` `{`

`int` `j = predict(pg, fr, pn, i + 1);`

`fr[j] = pg[i];`

`}`

`}`

`cout << "No. of hits = "` `<< hit << endl;`

`cout << "No. of misses = "` `<< pn - hit << endl;`

`}`

`// Driver Function`

`int` `main()`

`{`

`int` `pg[] = { 7, 0, 1, 2, 0, 3, 0, 4, 2, 3, 0, 3, 2 };`

`int` `pn = sizeof(pg) / sizeof(pg[0]);`

`int` `fn = 4;`

`optimalPage(pg, pn, fn);`

`return` `0;`

`}`

`// This code is contributed by Karandeep Singh`

Output:

```
No. of hits = 7
No. of misses = 6
```

- The above implementation can optimized using hashing. We can use an [unordered_set](https://www.geeksforgeeks.org/unorderd_set-stl-uses/) in place of vector so that search operation can be done in O(1) time.
- Note that optimal page replacement algorithm is not practical as we cannot
predict future. However it is used as a reference for other page
replacement algorithms.
*** Least Frequently Used (LFU) Cache Implementation
**Least Frequently Used (LFU)**
 is a caching algorithm in which the least frequently used cache block 
is removed whenever the cache is overflowed. In LFU we check the old 
page as well as the frequency of that page and if the frequency of the 
page is larger than the old page we cannot remove it and if all the old 
pages are having same frequency then take last i.e FIFO method for that 
and remove that page.

## [Recommended: Please try your approach on ***{IDE}*** first, before moving on to the solution.](https://ide.geeksforgeeks.org/)

**Min-heap**
 data structure is a good option to implement this algorithm, as it 
handles insertion, deletion, and update in logarithmic time complexity. A
 tie can be resolved by removing the least recently used cache block. 
The following two containers have been used to solve the problem:

- A vector of integer pairs has been used to represent the cache, where
each pair consists of the block number and the number of times it has
been used. The vector is ordered in the form of a min-heap, which allows us to access the least frequently used block in constant time.
- A hashmap has been used to store the indices of the cache blocks which allows searching in constant time.

Below is the implementation of the above approach:

`// C++ program for LFU cache implementation`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Generic function to swap two pairs`

`void` `swap(pair<int, int>& a, pair<int, int>& b)`

`{`

`pair<int, int> temp = a;`

`a = b;`

`b = temp;`

`}`

`// Returns the index of the parent node`

`inline` `int` `parent(int` `i)`

`{`

`return` `(i - 1) / 2;`

`}`

`// Returns the index of the left child node`

`inline` `int` `left(int` `i)`

`{`

`return` `2 * i + 1;`

`}`

`// Returns the index of the right child node`

`inline` `int` `right(int` `i)`

`{`

`return` `2 * i + 2;`

`}`

`// Self made heap tp Rearranges`

`//  the nodes in order to maintain the heap property`

`void` `heapify(vector<pair<int, int> >& v,`

`unordered_map<int, int>& m, int` `i, int` `n)`

`{`

`int` `l = left(i), r = right(i), minim;`

`if` `(l < n)`

`minim = ((v[i].second < v[l].second) ? i : l);`

`else`

`minim = i;`

`if` `(r < n)`

`minim = ((v[minim].second < v[r].second) ? minim : r);`

`if` `(minim != i) {`

`m[v[minim].first] = i;`

`m[v[i].first] = minim;`

`swap(v[minim], v[i]);`

`heapify(v, m, minim, n);`

`}`

`}`

`// Function to Increment the frequency`

`// of a node and rearranges the heap`

`void` `increment(vector<pair<int, int> >& v,`

`unordered_map<int, int>& m, int` `i, int` `n)`

`{`

`++v[i].second;`

`heapify(v, m, i, n);`

`}`

`// Function to Insert a new node in the heap`

`void` `insert(vector<pair<int, int> >& v,`

`unordered_map<int, int>& m, int` `value, int& n)`

`{`

`if` `(n == v.size()) {`

`m.erase(v[0].first);`

`cout << "Cache block "` `<< v[0].first`

`<< " removed.\n";`

`v[0] = v[--n];`

`heapify(v, m, 0, n);`

`}`

`v[n++] = make_pair(value, 1);`

`m.insert(make_pair(value, n - 1));`

`int` `i = n - 1;`

`// Insert a node in the heap by swapping elements`

`while` `(i && v[parent(i)].second > v[i].second) {`

`m[v[i].first] = parent(i);`

`m[v[parent(i)].first] = i;`

`swap(v[i], v[parent(i)]);`

`i = parent(i);`

`}`

`cout << "Cache block "` `<< value << " inserted.\n";`

`}`

`// Function to refer to the block value in the cache`

`void` `refer(vector<pair<int, int> >& cache, unordered_map<int,`

`int>& indices, int` `value, int& cache_size)`

`{`

`if` `(indices.find(value) == indices.end())`

`insert(cache, indices, value, cache_size);`

`else`

`increment(cache, indices, indices[value], cache_size);`

`}`

`// Driver Code`

`int` `main()`

`{`

`int` `cache_max_size = 4, cache_size = 0;`

`vector<pair<int, int> > cache(cache_max_size);`

`unordered_map<int, int> indices;`

`refer(cache, indices, 1, cache_size);`

`refer(cache, indices, 2, cache_size);`

`refer(cache, indices, 1, cache_size);`

`refer(cache, indices, 3, cache_size);`

`refer(cache, indices, 2, cache_size);`

`refer(cache, indices, 4, cache_size);`

`refer(cache, indices, 5, cache_size);`

`return` `0;`

`}`

**Output:**

`Cache block 1 inserted.
Cache block 2 inserted.
Cache block 3 inserted.
Cache block 4 inserted.
Cache block 3 removed.
Cache block 5 inserted.
*** Second Chance (or Clock) Page Replacement Policy
Prerequisite – [Page Replacement Algorithms](https://www.geeksforgeeks.org/operating-system-page-replacement-algorithm/) Apart
 from LRU, OPT and FIFO page replacement policies, we also have the 
second chance/clock page replacement policy. In the Second Chance page 
replacement policy, the candidate pages for removal are considered in a 
round robin matter, and a page that has been accessed between 
consecutive considerations will not be replaced. The page replaced is 
the one that, when considered in a round robin matter, has not been 
accessed since its last consideration. It can be implemented by 
adding a “second chance” bit to each memory frame-every time the frame 
is considered (due to a reference made to the page inside it), this bit 
is set to 1, which gives the page a second chance, as when we consider 
the candidate page for replacement, we replace the first one with this 
bit set to 0 (while zeroing out bits of the other pages we see in the 
process). Thus, a page with the “second chance” bit set to 1 is never 
replaced during the first consideration and will only be replaced if all
 the other pages deserve a second chance too!**Example –** Let’s say the reference string is **0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4** and we have **3** frames. Let’s see how the algorithm proceeds by tracking the second chance bit and the pointer. 

- Initially, all frames are empty so after first 3 passes they will be filled with
{0, 4, 1} and the second chance array will be {0, 0, 0} as none has been referenced yet. Also, the pointer will cycle back to 0.
- **Pass-4:** Frame={0, 4, 1}, second_chance = {0, 1, 0} [4 will get a second
chance], pointer = 0 (No page needed to be updated so the candidate is
still page in frame 0), pf = 3 (No increase in page fault number).
- **Pass-5:** Frame={2, 4, 1}, second_chance= {0, 1, 0} [0 replaced; it’s second
chance bit was 0, so it didn’t get a second chance], pointer=1
(updated), pf=4
- **Pass-6:** Frame={2, 4, 1}, second_chance={0, 1, 0}, pointer=1, pf=4 (No change)
- **Pass-7:** Frame={2, 4, 3}, second_chance= {0, 0, 0} [4 survived but it’s second
chance bit became 0], pointer=0 (as element at index 2 was finally
replaced), pf=5
- **Pass-8:** Frame={2, 4, 3}, second_chance= {0, 1, 0} [4 referenced again], pointer=0, pf=5
- **Pass-9:** Frame={2, 4, 3}, second_chance= {1, 1, 0} [2 referenced again], pointer=0, pf=5
- **Pass-10:** Frame={2, 4, 3}, second_chance= {1, 1, 0}, pointer=0, pf=5 (no change)
- **Pass-11:** Frame={2, 4, 0}, second_chance= {0, 0, 0}, pointer=0, pf=6 (2 and 4 got second chances)
- **Pass-12:** Frame={2, 4, 0}, second_chance= {0, 1, 0}, pointer=0, pf=6 (4 will again get a second chance)
- **Pass-13:** Frame={1, 4, 0}, second_chance= {0, 1, 0}, pointer=1, pf=7 (pointer updated, pf updated)
- **Page-14:** Frame={1, 4, 0}, second_chance= {0, 1, 0}, pointer=1, pf=7 (No change)
- **Page-15:** Frame={1, 4, 2}, second_chance= {0, 0, 0}, pointer=0, pf=8 (4 survived again due to 2nd chance!)
- **Page-16:** Frame={1, 4, 2}, second_chance= {0, 1, 0}, pointer=0, pf=8 (2nd chance updated)
- **Page-17:** Frame={3, 4, 2}, second_chance= {0, 1, 0}, pointer=1, pf=9 (pointer, pf updated)
- **Page-18:** Frame={3, 4, 2}, second_chance= {0, 1, 0}, pointer=1, pf=9 (No change)

In this example, second chance algorithm does as well as the LRU method, which is much more expensive to implement in hardware.**More Examples –** 

```
Input: 2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1
3
Output: 14

Input: 2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1
4
Output: 11
```

**Algorithm –** Create an array **frames** to track the pages currently in memory and another Boolean array **second_chance**
 to track whether that page has been accessed since it’s last 
replacement (that is if it deserves a second chance or not) and a 
variable **pointer** to track the target for replacement. 

1. Start traversing the array **arr**. If the page already exists, simply set its corresponding element in **second_chance** to true and return. 
2. If the page doesn’t exist, check whether the space pointed to by **pointer** is empty (indicating cache isn’t full yet) – if so, we will put the element there and return, else we’ll traverse the array **arr** one by one (cyclically using the value of **pointer**), marking all corresponding **second_chance** elements as false, till we find a one that’s already false. That is the most suitable page for replacement, so we do so and return. 
3. Finally, we report the page fault count. 

!https://media.geeksforgeeks.org/wp-content/uploads/GFG-3.jpg

`// CPP program to find largest in an array`

`// without conditional/bitwise/ternary/ operators`

`// and without library functions.`

`#include<iostream>`

`#include<cstring>`

`#include<sstream>`

`using` `namespace` `std;`

`// If page found, updates the second chance bit to true`

`static` `bool` `findAndUpdate(int` `x,int` `arr[],`

`bool` `second_chance[],int` `frames)`

`{`

`int` `i;`

`for(i = 0; i < frames; i++)`

`{`

`if(arr[i] == x)`

`{`

`// Mark that the page deserves a second chance`

`second_chance[i] = true;`

`// Return 'true', that is there was a hit`

`// and so there's no need to replace any page`

`return` `true;`

`}`

`}`

`// Return 'false' so that a page for replacement is selected`

`// as he reuested page doesn't exist in memory`

`return` `false;`

`}`

`// Updates the page in memory and returns the pointer`

`static` `int` `replaceAndUpdate(int` `x,int` `arr[],`

`bool` `second_chance[],int` `frames,int` `pointer)`

`{`

`while(true)`

`{`

`// We found the page to replace`

`if(!second_chance[pointer])`

`{`

`// Replace with new page`

`arr[pointer] = x;`

`// Return updated pointer`

`return` `(pointer + 1) % frames;`

`}`

`// Mark it 'false' as it got one chance`

`// and will be replaced next time unless accessed again`

`second_chance[pointer] = false;`

`//Pointer is updated in round robin manner`

`pointer = (pointer + 1) % frames;`

`}`

`}`

`static` `void` `printHitsAndFaults(string reference_string,`

`int` `frames)`

`{`

`int` `pointer, i, l=0, x, pf;`

`//initially we consider frame 0 is to be replaced`

`pointer = 0;`

`//number of page faults`

`pf = 0;`

`// Create a array to hold page numbers`

`int` `arr[frames];`

`// No pages initially in frame,`

`// which is indicated by -1`

`memset(arr, -1, sizeof(arr));`

`// Create second chance array.`

`// Can also be a byte array for optimizing memory`

`bool` `second_chance[frames];`

`// Split the string into tokens,`

`// that is page numbers, based on space`

`string str[100];`

`string word = "";`

`for` `(auto` `x : reference_string)`

`{`

`if` `(x == ' ')`

`{`

`str[l]=word;`

`word = "";`

`l++;`

`}`

`else`

`{`

`word = word + x;`

`}`

`}`

`str[l] = word;`

`l++;`

`// l=the length of array`

`for(i = 0; i < l; i++)`

`{`

`x = stoi(str[i]);`

`// Finds if there exists a need to replace`

`// any page at all`

`if(!findAndUpdate(x,arr,second_chance,frames))`

`{`

`// Selects and updates a victim page`

`pointer = replaceAndUpdate(x,arr,`

`second_chance,frames,pointer);`

`// Update page faults`

`pf++;`

`}`

`}`

`cout << "Total page faults were "` `<< pf << "\n";`

`}`

`// Driver code`

`int` `main()`

`{`

`string reference_string = "";`

`int` `frames = 0;`

`// Test 1:`

`reference_string = "0 4 1 4 2 4 3 4 2 4 0 4 1 4 2 4 3 4";`

`frames = 3;`

`// Output is 9`

`printHitsAndFaults(reference_string,frames);`

`// Test 2:`

`reference_string = "2 5 10 1 2 2 6 9 1 2 10 2 6 1 2 1 6 9 5 1";`

`frames = 4;`

`// Output is 11`

`printHitsAndFaults(reference_string,frames);`

`return` `0;`

`}`

`// This code is contributed by NikhilRathor`

**Output:** 

```
Total page faults were 9
Total page faults were 11
```

**Note:**

1. The arrays **arr** and **second_chance** can be replaced and combined together via a hashmap (with element as key, true/false as value) to speed up search.
2. Time complexity of this method is **O(Number_of_frames*reference_string_length)** or **O(mn)** but since number of frames will be a constant in an Operating System (as main memory size is fixed), it is simply **O(n)** [Same as hashmap approach, but that will have lower constants]
3. Second chance algorithm may suffer from [Belady’s Anomaly](https://www.geeksforgeeks.org/operating-system-beladys-anomaly/).
*** Techniques to handle Thrashing
Prerequisite – [Virtual Memory](https://www.geeksforgeeks.org/virtual-memory-operating-systems/)

**Thrashing**
 is a condition or a situation when the system is spending a major 
portion of its time servicing the page faults, but the actual processing
 done is very negligible.

**Thrashing’s Causes**

Thrashing
 has an impact on the operating system’s execution performance. 
Thrashing also causes serious performance issues with the operating 
system. When the CPU’s usage is low, the process scheduling mechanism 
tries to load multiple processes into memory at the same time, 
increasing the degree of Multi programming.

In this case, the 
number of processes in the memory exceeds the number of frames available
 in the memory. Each process is given a set number of frames to work 
with.

If a high-priority process arrives in memory and the frame 
is not vacant at the moment, the other process occupying the frame will 
be moved to secondary storage, and the free frame will be allotted to a 
higher-priority process.

We may also argue that as soon as the 
memory is full, the procedure begins to take a long time to swap in the 
required pages. Because most of the processes are waiting for pages, the
 CPU utilization drops again.

As a result, a high level of multi 
programming and a lack of frames are two of the most common reasons for 
thrashing in the operating system.

!https://media.geeksforgeeks.org/wp-content/uploads/2-103.png

The
 basic concept involved is that if a process is allocated too few 
frames, then there will be too many and too frequent page faults. As a 
result, no useful work would be done by the CPU and the CPU utilization 
would fall drastically. The long-term scheduler would then try to 
improve the CPU utilization by loading some more processes into the 
memory thereby increasing the degree of multi programming. This would 
result in a further decrease in the CPU utilization triggering a chained
 reaction of higher page faults followed by an increase in the degree of
 multi programming, called Thrashing.

**Locality Model –** A
 locality is a set of pages that are actively used together. The 
locality model states that as a process executes, it moves from one 
locality to another. A program is generally composed of several 
different localities which may overlap.

For example, when a 
function is called, it defines a new locality where memory references 
are made to the instructions of the function call, it’s local and global
 variables, etc. Similarly, when the function is exited, the process 
leaves this locality.

**Techniques to handle:**

**1. Working Set Model –**

This model is based on the above-stated concept of the Locality Model. The
 basic principle states that if we allocate enough frames to a process 
to accommodate its current locality, it will only fault whenever it 
moves to some new locality. But if the allocated frames are lesser than 
the size of the current locality, the process is bound to thrash.

According
 to this model, based on parameter A, the working set is defined as the 
set of pages in the most recent ‘A’ page references. Hence, all the 
actively used pages would always end up being a part of the working 
set.

The accuracy of the working set is dependent on the value of
 parameter A. If A is too large, then working sets may overlap. On the 
other hand, for smaller values of A, the locality might not be covered 
entirely.

If D is the total demand for frames and

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-ad1c3b279ef9f93a2ca409e089dfb2cd_l3.svg

is the working set size for process i,

!https://www.geeksforgeeks.org/wp-content/ql-cache/quicklatex.com-a655a552933d09993a5e17c01d09e808_l3.svg

Now, if ‘m’ is the number of frames available in the memory, there are 2 possibilities:

- (i) D>m i.e. total demand exceeds the number of frames, then thrashing
will occur as some processes would not get enough frames.
- (ii) D<=m, then there would be no thrashing.

**2. Page Fault Frequency –**

A more direct approach to handling thrashing is the one that uses the Page-Fault Frequency concept.

!https://media.geeksforgeeks.org/wp-content/uploads/1-161.png

The problem associated with Thrashing is the high page fault rate and thus, the concept here is to control the page fault rate. If
 the page fault rate is too high, it indicates that the process has too 
few frames allocated to it. On the contrary, a low page fault rate 
indicates that the process has too many frames. Upper and lower limits can be established on the desired page fault rate as shown in the diagram. If
 the page fault rate falls below the lower limit, frames can be removed 
from the process. Similarly, if the page fault rate exceeds the upper 
limit, more frames can be allocated to the process. In other words, the graphical state of the system should be kept limited to the rectangular region formed in the given diagram. Here
 too, if the page fault rate is high with no free frames, then some of 
the processes can be suspended and frames allocated to them can be 
reallocated to other processes. The suspended processes can then be 
restarted later.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Allocating kernel memory (buddy system and slab system)
Prerequisite – [Buddy System](https://www.geeksforgeeks.org/operating-system-buddy-system-memory-allocation-technique/)

Two strategies for managing free memory that is assigned to kernel processes:

### 1. Buddy system –

Buddy
 allocation system is an algorithm in which a larger memory block is 
divided into small parts to satisfy the request. This algorithm is used 
to give best fit. The two smaller parts of block are of equal size and 
called as buddies. In the same manner one of the two buddies will 
further divide into smaller parts until the request is fulfilled. 
Benefit of this technique is that the two buddies can combine to form 
the block of larger size according to the memory request.

*Example –* If the request of 25Kb is made then block of size 32Kb is allocated.

!https://media.geeksforgeeks.org/wp-content/uploads/bud-1.jpg

**Four Types of Buddy System –**

1. Binary buddy system
2. Fibonacci buddy system
3. Weighted buddy system
4. Tertiary buddy system

**Why buddy system?** If the partition size and process size are different then poor match occurs and may use space inefficiently. It is easy to implement and efficient then dynamic allocation.

**Binary buddy system –** The
 buddy system maintains a list of the free blocks of each size (called a
 free list), so that it is easy to find a block of the desired size, if 
one is available. If no block of the requested size is available, 
Allocate searches for the first non-empty list for blocks of atleast the
 size requested. In either case, a block is removed from the free list.

**Example –**
 Assume the size of memory segment is initially 256kb and the kernel 
requests 25kb of memory. The segment is initially divided into two 
buddies. Let we call A1 and A2 each 128kb in size. One of these buddies 
is further divided into two 64kb buddies let say B1 and B2. But the next
 highest power of 25kb is 32kb so, either B1 or B2 is further divided 
into two 32kb buddies(C1 and C2) and finally one of these buddies is 
used to satisfy the 25kb request. A split block can only be merged with 
its unique buddy block, which then reforms the larger block they were 
split from.

**Fibonacci buddy system –** This is the system in which blocks are divided into sizes which are Fibonacci numbers. It satisfies the following relation:

```
  Zi = Z(i-1)+Z(i-2)
```

0,
 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 144, 233, 377, 610. The address 
calculation for the binary and weighted buddy systems is straight 
forward, but the original procedure for the Fibonacci buddy system was 
either limited to a small, fixed number of block sizes or a time 
consuming computation.

**Advantages –**

- In comparison to other simpler techniques such as dynamic allocation, the buddy memory system has little external fragmentation.
- The buddy memory allocation system is implemented with the use of a binary tree to represent used or unused split memory blocks.
- The buddy system is very fast to allocate or deallocate memory.
- In buddy systems, the cost to allocate and free a block of memory is low compared to that of best-fit or first-fit algorithms.
- Other advantage is coalescing.
- Address calculation is easy.

**What is coalescing?** It is defined as how quickly adjacent buddies can be combined to form larger segments this is known as coalescing. For
 example, when the kernel releases the C1 unit it was allocated, the 
system can coalesce C1 and C2 into a 64kb segment. This segment B1 can 
in turn be coalesced with its buddy B2 to form a 128kb segment. 
Ultimately we can end up with the original 256kb segment.

**Drawback –** The
 main drawback in buddy system is internal fragmentation as larger block
 of memory is acquired then required. For example if a 36 kb request is 
made then it can only be satisfied by 64 kb segment and remaining memory
 is wasted.

### 2. Slab Allocation –

A second strategy for 
allocating kernel memory is known as slab allocation. It eliminates 
fragmentation caused by allocations and deallocations. This method is 
used to retain allocated memory that contains a data object of a certain
 type for reuse upon subsequent allocations of objects of the same type.
 In slab allocation memory chunks suitable to fit data objects of 
certain type or size are preallocated. Cache does not free the space 
immediately after use although it keeps track of data which are required
 frequently so that whenever request is made the data will reach very 
fast. Two terms required are:

- **Slab –** A
slab is made up of one or more physically contiguous pages. The slab is
the actual container of data associated with objects of the specific
kind of the containing cache.
- **Cache –** Cache
represents a small amount of very fast memory. A cache consists of one
or more slabs. There is a single cache for each unique kernel data
structure.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/12-3.jpg

**Example –**

- A separate cache for a data structure representing processes descriptors
- Separate cache for file objects
- Separate cache for semaphores etc.

Each
 cache is populated with objects that are instantiations of the kernel 
data structure the cache represents. For example the cache representing 
semaphores stores instances of semaphore objects, the cache representing
 process descriptors stores instances of process descriptor objects.

**Implementation –** The
 slab allocation algorithm uses caches to store kernel objects. When a 
cache is created a number of objects which are initially marked as free 
are allocated to the cache. The number of objects in the cache depends 
on size of the associated slab. *Example –* A 12 kb slab (made 
up of three contiguous 4 kb pages) could store six 2 kb objects. 
Initially all objects in the cache are marked as free. When a new object
 for a kernel data structure is needed, the allocator can assign any 
free object from the cache to satisfy the request. The object assigned 
from the cache is marked as used.

In linux, a slab may in one of three possible states:

1. **Full –** All objects in the slab are marked as used
2. **Empty –** All objects in the slab are marked as free
3. **Partial –** The slab consists of both

The
 slab allocator first attempts to satisfy the request with a free object
 in a partial slab. If none exists, a free object is assigned from an 
empty slab. If no empty slabs are available, a new slab is allocated 
from contiguous physical pages and assigned to a cache.

**Benefits of slab allocator –**

- No memory is wasted due to fragmentation because each unique kernel data structure has an associated cache.
- Memory request can be satisfied quickly.
- The slab allocating scheme is particularly effective for managing when
objects are frequently allocated or deallocated. The act of allocating
and releasing memory can be a time consuming process. However, objects
are created in advance and thus can be quickly allocated from the cache. When the kernel has finished with an object and releases it, it is
marked as free and return to its cache, thus making it immediately
available for subsequent request from the kernel.
*** Buddy Memory Allocation Program | Set 1 (Allocation)
Prerequisite – [Buddy System](https://www.geeksforgeeks.org/operating-system-buddy-system-memory-allocation-technique/) **Question:** Write a program to implement the buddy system of memory allocation in Operating Systems.**Explanation –** The
 buddy system is implemented as follows- A list of free nodes, of all 
the different possible powers of 2, is maintained at all times (So if 
total memory size is 1 MB, we’d have 20 free lists to track-one for 
blocks of size 1 byte, 1 for 2 bytes, next for 4 bytes and so on). When
 a request for allocation comes, we look for the smallest block bigger 
than it. If such a block is found on the free list, the allocation is 
done (say, the request is of 27 KB and the free list tracking 32 KB 
blocks has at least one element in it), else we traverse the free list *upwards*
 till we find a big enough block. Then we keep splitting it in two 
blocks-one for adding to the next free list (of smaller size), one to 
traverse *down* the tree till we reach the target and return the 
requested memory block to the user. If no such allocation is possible, 
we simply return null.

**Example:** Let
 us see how the algorithm proceeds by tracking a memory block of size 
128 KB. Initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (0, 
127) }

- **Request:** 32 bytes No such block
found, so we traverse up and split the 0-127 block into 0-63, 64-127; we add 64-127 to list tracking 64 byte blocks and pass 0-63 downwards;
again it is split into 0-31 and 32-63; since we have found the required
block size, we add 32-63 to list tracking 32 byte blocks and return 0-31 to user. List is: {}, {}, {}, {}, {}, { (32, 63) }, { (64, 127) }, {}
- **Request:** 7 bytes No such block found-split block 32-63 into two blocks, namely 32-47 and
48-63; then split 32-47 into 32-39 and 40-47; finally, return 32-39 to
user (internal fragmentation of 1 byte occurs) List is: {}, {}, {}, { (40, 47) }, { (48, 63) }, {}, { (64, 127) }, {}
- **Request:** 64 bytes Straight up memory segment 64-127 will be allocated as it already exists. List is: {}, {}, {}, { (40, 47) }, { (48, 63) }, {}, {}, {}
- Request: 56 bytes Result: Not allocated

The result will be as follows:

!https://media.geeksforgeeks.org/wp-content/uploads/GFG-20.png

**Figure –** Buddy Allocation-128 shows the starting address of next possible block (if main memory size ever increases)

**Implementation –**

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`// Size of vector of pairs`

`int` `size;`

`// Global vector of pairs to store`

`// address ranges available in free list`

`vector<pair<int, int>> free_list[100000];`

`// Map used as hash map to store the starting`

`// address as key and size of allocated segment`

`// key as value`

`map<int, int> mp;`

`void` `initialize(int` `sz)`

`{`

`// Maximum number of powers of 2 possible`

`int` `n = ceil(log(sz) / log(2));`

`size = n + 1;`

`for(int` `i = 0; i <= n; i++)`

`free_list[i].clear();`

`// Initially whole block of specified`

`// size is available`

`free_list[n].push_back(make_pair(0, sz - 1));`

`}`

`void` `allocate(int` `sz)`

`{`

`// Calculate index in free list`

`// to search for block if available`

`int` `n = ceil(log(sz) / log(2));`

`// Block available`

`if` `(free_list[n].size() > 0)`

`{`

`pair<int, int> temp = free_list[n][0];`

`// Remove block from free list`

`free_list[n].erase(free_list[n].begin());`

`cout << "Memory from "` `<< temp.first`

`<< " to "` `<< temp.second << " allocated"`

`<< "\n";`

`// map starting address with`

`// size to make deallocating easy`

`mp[temp.first] = temp.second -`

`temp.first + 1;`

`}`

`else`

`{`

`int` `i;`

`for(i = n + 1; i < size; i++)`

`{`

`// Find block size greater than request`

`if(free_list[i].size() != 0)`

`break;`

`}`

`// If no such block is found`

`// i.e., no memory block available`

`if` `(i == size)`

`{`

`cout << "Sorry, failed to allocate memory \n";`

`}`

`// If found`

`else`

`{`

`pair<int, int> temp;`

`temp = free_list[i][0];`

`// Remove first block to split it into halves`

`free_list[i].erase(free_list[i].begin());`

`i--;`

`for(; i >= n; i--)`

`{`

`// Divide block into two halves`

`pair<int, int> pair1, pair2;`

`pair1 = make_pair(temp.first,`

`temp.first +`

`(temp.second -`

`temp.first) / 2);`

`pair2 = make_pair(temp.first +`

`(temp.second -`

`temp.first + 1) / 2,`

`temp.second);`

`free_list[i].push_back(pair1);`

`// Push them in free list`

`free_list[i].push_back(pair2);`

`temp = free_list[i][0];`

`// Remove first free block to`

`// further split`

`free_list[i].erase(free_list[i].begin());`

`}`

`cout << "Memory from "` `<< temp.first`

`<< " to "` `<< temp.second`

`<< " allocated"` `<< "\n";`

`mp[temp.first] = temp.second -`

`temp.first + 1;`

`}`

`}`

`}`

`// Driver code`

`int` `main()`

`{`

`// Uncomment following code for interactive IO`

`/*`

`int total,c,req;`

`cin>>total;`

`initialize(total);`

`while(true)`

`{`

`cin>>req;`

`if(req < 0)`

`break;`

`allocate(req);`

`}*/`

`initialize(128);`

`allocate(32);`

`allocate(7);`

`allocate(64);`

`allocate(56);`

`return` `0;`

`}`

`// This code is contributed by sarthak_eddy`

**Output:**

```
Memory from 0 to 31 allocated
Memory from 32 to 39 allocated
Memory from 64 to 127 allocated
Sorry, failed to allocate memory
```

**Time Complexity –** If the main memory size is **n**, we have log(n) number of different powers of 2 and hence log(n) elements in the array (named **arr**
 in the code) tracking free lists. To allocate a block, we only need to 
traverse the array once upwards and once downwards, hence time 
complexity is **O(2log(n))** or simply **O(logn)**
*** Buddy Memory Allocation Program | Set 2 (Deallocation)
Prerequisite – [Buddy Allocation | Set 1](https://www.geeksforgeeks.org/program-for-buddy-memory-allocation-scheme-in-operating-systems-set-1-allocation/) **Question:** Write a program to implement the buddy system of memory allocation and deallocation in Operating Systems.**Explanation –** As
 we already know from Set 1, the allocation is done via the usage of 
free lists. Now, for deallocation, we will maintain an extra data 
structure-a Map (unordered_set in C++, HashMap in Java) with the 
starting address of segment as key and size of the segment as value and 
update it whenever an allocation request comes. Now, when a deallocation
 request comes, we will first check the map to see if it is a valid 
request. If so, we will then add the block to the free list tracking 
blocks of their sizes. Then, we will search the free list to see if its *buddy*
 is free-if so, we will merge the blocks and place them on the free list
 above them (which tracks blocks of double the size), else we will not 
coalesce and simply return after that.How to know which block is a given block’s buddy?Let us define two terms-**buddyNumber** and **buddyAddress**. The **buddyNumber** of a block is calculated by the formula:

```
(base_address-starting_address_of_main_memory)/block_size
```

We
 note that this is always an integer, as both numerator and denominator 
are powers of 2. Now, a block will be another block’s buddy if both of 
them were formed by the splitting of the same bigger block. For example,
 if 4 consecutive allocation requests of 16 bytes come, we will end up 
with blocks 0-15, 16-31, 32-47, 48-63 where blocks 0-15 and 16-31 are 
buddies (as they were formed by splitting block 0-32) but 0-15 and 32-47
 aren’t. The **buddyAddress** of a block is the starting index of its buddy block, given by the formula: 

```
block_starting_address+block_size (if buddyNumber is even)
block_starting_address-block_size (if buddyNumber is odd)
```

Thus, all we have to do is find this **buddyAddress**
 in the free list (by comparing with all the starting addresses in that 
particular list), and if present, coalescing can be done.**Examples:** Let
 us see how the algorithm proceeds by tracking a memory block of size 
128 KB. Initially, the free list is: {}, {}, {}, {}, {}, {}, {}, { (0, 
127) } 

- **Allocation Request:** 16 bytes No such block found, so we traverse up and split the 0-127 block into
0-63, 64-127; we add 64-127 to list tracking 64-byte blocks and pass
0-63 downwards; again it is split into 0-31 and 32-63; we add 32-63 to
list tracking 32-byte blocks, passing 0-31 downwards; one more splits
done and 0-15 is returned to the user while 16-31 is added to free list
tracking 16-byte blocks. List is: {}, {}, {}, {}, { (16, 31) }, { (32, 63) }, { (64, 127) }, {}
- **Allocation Request:** 16 bytes Straight-up memory segment 16-31 will be allocated as it already exists. List is: {}, {}, {}, {}, {}, { (32, 63) }, { (64, 127) }, {}
- **Allocation Request:** 16 bytes No such block was found, so we will traverse up to block 32-63 and split
it into blocks 32-47 and 48-63; we will add 48-63 to list tracking
16-byte blocks and return 32-47 to a user. List is: {}, {}, {}, {}, { (48, 63) }, {}, { (64, 127) }, {}
- **Allocation Request:** 16 bytes Straight-up memory segment 48-63 will be allocated as it already exists. List is: {}, {}, {}, {}, {}, {}, { (64, 127) }, {}
- **Deallocation Request:** StartIndex = 0 Deallocation will be done but no coalescing is possible as its **buddyNumber** is 0 and **buddyAddress** is 16 (via the formula), none of which is in the free list. List is: {}, {}, {}, {}, { (0, 15) }, {}, { (64, 127) }, {}
- **Deallocation Request:** StartIndex = 9 Result: Invalid request, as this segment was never allocated. List is: {}, {}, {}, {}, { (0, 15) }, {}, { (64, 127) }, {}
- **Deallocation Request:** StartIndex = 32 Deallocation will be done but no coalescing is possible as the **buddyNumber** of the blocks are 0 and 2 **buddyAddress** of the blocks are 16 and 48, respectively, none of which is in the free list. List is: {}, {}, {}, {}, { (0, 15), (32-47) }, {}, { (64, 127) }, {}
- **Deallocation Request:** StartIndex = 16 Deallocation will be done and coalescing of the blocks 0-15 and 16-31 will also be done as the **buddyAddress** of block 16-31 is 0, which is present in the free list tracking 16-byte blocks. List is: {}, {}, {}, {}, { (32-47) }, { (0, 31) }, { (64, 127) }, {}

!https://media.geeksforgeeks.org/wp-content/uploads/GFG-21.png

**Figure –** Buddy algorithm-allocation and deallocation

**Implementation –** Below is the complete program.

`#include<bits/stdc++.h>`

`using` `namespace` `std;`

`// Size of vector of pairs`

`int` `size;`

`// Global vector of pairs to track all`

`// the free nodes of various sizes`

`vector<pair<int, int>> arr[100000];`

`// Map used as hash map to store the`

`// starting address as key and size`

`// of allocated segment key as value`

`map<int, int> mp;`

`void` `Buddy(int` `s)`

`{`

`// Maximum number of powers of 2 possible`

`int` `n = ceil(log(s) / log(2));`

`size = n + 1;`

`for(int` `i = 0; i <= n; i++)`

`arr[i].clear();`

`// Initially whole block of specified`

`// size is available`

`arr[n].push_back(make_pair(0, s - 1));`

`}`

`void` `allocate(int` `s)`

`{`

`// Calculate index in free list`

`// to search for block if available`

`int` `x = ceil(log(s) / log(2));`

`// Block available`

`if` `(arr[x].size() > 0)`

`{`

`pair<int, int> temp = arr[x][0];`

`// Remove block from free list`

`arr[x].erase(arr[x].begin());`

`cout << "Memory from "` `<< temp.first`

`<< " to "` `<< temp.second`

`<< " allocated"` `<< "\n";`

`// Map starting address with`

`// size to make deallocating easy`

`mp[temp.first] = temp.second -`

`temp.first + 1;`

`}`

`else`

`{`

`int` `i;`

`// If not, search for a larger block`

`for(i = x + 1; i < size; i++)`

`{`

`// Find block size greater`

`// than request`

`if` `(arr[i].size() != 0)`

`break;`

`}`

`// If no such block is found`

`// i.e., no memory block available`

`if` `(i == size)`

`{`

`cout << "Sorry, failed to allocate memory\n";`

`}`

`// If found`

`else`

`{`

`pair<int, int> temp;`

`temp = arr[i][0];`

`// Remove first block to split`

`// it into halves`

`arr[i].erase(arr[i].begin());`

`i--;`

`for(;i >= x; i--)`

`{`

`// Divide block into two halves`

`pair<int, int> pair1, pair2;`

`pair1 = make_pair(temp.first,`

`temp.first +`

`(temp.second -`

`temp.first) / 2);`

`pair2 = make_pair(temp.first +`

`(temp.second -`

`temp.first + 1) / 2,`

`temp.second);`

`arr[i].push_back(pair1);`

`// Push them in free list`

`arr[i].push_back(pair2);`

`temp = arr[i][0];`

`// Remove first free block to`

`// further split`

`arr[i].erase(arr[i].begin());`

`}`

`cout << "Memory from "` `<< temp.first`

`<< " to "` `<< temp.second`

`<< " allocate"` `<< "\n";`

`mp[temp.first] = temp.second -`

`temp.first + 1;`

`}`

`}`

`}`

`void` `deallocate(int` `id)`

`{`

`// If no such starting address available`

`if(mp.find(id) == mp.end())`

`{`

`cout << "Sorry, invalid free request\n";`

`return;`

`}`

`// Size of block to be searched`

`int` `n = ceil(log(mp[id]) / log(2));`

`int` `i, buddyNumber, buddyAddress;`

`// Add the block in free list`

`arr[n].push_back(make_pair(id,`

`id + pow(2, n) - 1));`

`cout << "Memory block from "` `<< id`

`<< " to "<< id + pow(2, n) - 1`

`<< " freed\n";`

`// Calculate buddy number`

`buddyNumber = id / mp[id];`

`if` `(buddyNumber % 2 != 0)`

`buddyAddress = id - pow(2, n);`

`else`

`buddyAddress = id + pow(2, n);`

`// Search in free list to find it's buddy`

`for(i = 0; i < arr[n].size(); i++)`

`{`

`// If buddy found and is also free`

`if` `(arr[n][i].first == buddyAddress)`

`{`

`// Now merge the buddies to make`

`// them one large free memory block`

`if` `(buddyNumber % 2 == 0)`

`{`

`arr[n + 1].push_back(make_pair(id,`

`id + 2 * pow(2, n) - 1));`

`cout << "Coalescing of blocks starting at "`

`<< id << " and "` `<< buddyAddress`

`<< " was done"` `<< "\n";`

`}`

`else`

`{`

`arr[n + 1].push_back(make_pair(`

`buddyAddress, buddyAddress +`

`2 * pow(2, n) - 1));`

`cout << "Coalescing of blocks starting at "`

`<< buddyAddress << " and "`

`<< id << " was done"` `<< "\n";`

`}`

`arr[n].erase(arr[n].begin() + i);`

`arr[n].erase(arr[n].begin() +`

`arr[n].size() - 1);`

`break;`

`}`

`}`

`// Remove the key existence from map`

`mp.erase(id);`

`}`

`// Driver code`

`int` `main()`

`{`

`// Uncomment following code for interactive IO`

`/*`

`int total,c,req;`

`cout<<"Enter Total Memory Size (in Bytes) => ";`

`cin>>total;`

`initialize(total);`

`label:`

`while(1)`

`{`

`cout<<"\n1. Add Process into Memory\n`

`2. Remove Process \n3. Allocation Map\n4. Exit\n=> ";`

`cin>>c;`

`switch(c)`

`{`

`case 1:`

`cout<<"Enter Process Size (in Bytes) => ";`

`cin>>req;`

`cout<<"\n===>";`

`allocate(req);`

`break;`

`case 2:`

`cout<<"Enter Starting Address => ";`

`cin>>req;`

`cout<<"\n===>";`

`deallocate(req);`

`break;`

`case 3:`

`print();`

`break;`

`case 4:`

`exit(0);`

`break;`

`default:`

`goto label;`

`}`

`}*/`

`Buddy(128);`

`allocate(16);`

`allocate(16);`

`allocate(16);`

`allocate(16);`

`deallocate(0);`

`deallocate(9);`

`deallocate(32);`

`deallocate(16);`

`return` `0;`

`}`

`// This code is contributed by sarthak_eddy`

**Output**

`Memory from 0 to 15 allocate
Memory from 16 to 31 allocated
Memory from 32 to 47 allocate
Memory from 48 to 63 allocated
Memory block from 0 to 15 freed
Sorry, invalid free request
Memory block from 32 to 47 freed
Memory block from 16 to 31 freed
Coalescing of blocks starting at 0 and 16 was done`

**Time Complexity –** As already discussed in set 1, the time complexity of allocation is **O(log(n))**. For deallocation, in the worst case, all the allocated blocks can be of size 1 unit, which will then require **O(n)**
 time to scan the list for coalescing. However, in practice, it is 
highly unlikely that such an allocation will happen so it is generally 
much faster than linear time.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Static and Dynamic Libraries
When
 a C program is compiled, the compiler generates object code. After 
generating the object code, the compiler also invokes linker. One of the
 main tasks for linker is to make code of library functions (eg 
printf(), scanf(), sqrt(), ..etc) available to your program. A linker 
can accomplish this task in two ways, by copying the code of library 
function to your object code, or by making some arrangements so that the
 complete code of library functions is not copied, but made available at
 run-time.

**Static Linking and Static Libraries**
 is the result of the linker making copy of all used library functions 
to the executable file. Static Linking creates larger binary files, and 
need more space on disk and main memory. Examples of static libraries 
(libraries which are statically linked) are, ***.a*** files in Linux and ***.lib*** files in Windows.

**Steps to create a static library** Let us create and use a Static Library in UNIX or UNIX like OS.**1.** Create a C file that contains functions in your library.

`/* Filename: lib_mylib.c */`

`#include <stdio.h>`

`void` `fun(void)`

`{`

`printf("fun() called from a static library");`

`}`

We have created only one file for simplicity. We can also create multiple files in a library.

**2.** Create a header file for the library

`/* Filename: lib_mylib.h */`

`void` `fun(void);`

**3.** Compile library files.

```
 gcc -c lib_mylib.c -o lib_mylib.o
```

**4.** Create static library. This step is to bundle multiple object files in one static library (see [ar](http://en.wikipedia.org/wiki/Ar_(Unix)) for details). The output of this step is static library.

```
 ar rcs lib_mylib.a lib_mylib.o
```

**5.** Now
 our static library is ready to use. At this point we could just copy 
lib_mylib.a somewhere else to use it. For demo purposes, let us keep the
 library in the current directory.

**Let us create a driver program that uses above created static library**.**1.** Create a C file with main function

`/* filename: driver.c  */`

`#include "lib_mylib.h"`

`void` `main()`

`{`

`fun();`

`}`

**2.** Compile the driver program.

```
gcc -c driver.c -o driver.o
```

**3.** Link the compiled driver program to the static library. Note that -L**.** is used to tell that the static library is in current folder (See [this](http://gcc.gnu.org/onlinedocs/gcc/Link-Options.html) for details of -L and -l options).

```
gcc -o driver driver.o -L. -l_mylib
```

**4.** Run the driver program

```
./driver
fun() called from a static library
```

Following are some important points about static libraries.**1.**
 For a static library, the actual code is extracted from the library by 
the linker and used to build the final executable at the point you 
compile/build your application.

**2.** Each process 
gets its own copy of the code and data. Where as in case of dynamic 
libraries it is only code shared, data is specific to each process. For 
static libraries memory footprints are larger. For example, if all the 
window system tools were statically linked, several tens of megabytes of
 RAM would be wasted for a typical user, and the user would be slowed 
down by a lot of paging.

**3.** Since library code is
 connected at compile time, the final executable has no dependencies on 
the library at run time i.e. no additional run-time loading costs, it 
means that you don’t need to carry along a copy of the library that is 
being used and you have everything under your control and there is no 
dependency.

**4.** In static libraries, once 
everything is bundled into your application, you don’t have to worry 
that the client will have the right library (and version) available on 
their system.

**5.** One drawback of static libraries
 is, for any change(up-gradation) in the static libraries, you have to 
recompile the main program every time.

**6.** One 
major advantage of static libraries being preferred even now “is speed”.
 There will be no dynamic querying of symbols in static libraries. Many 
production line software use static libraries even today.

**Dynamic linking and Dynamic Libraries**
 Dynamic Linking doesn’t require the code to be copied, it is done by 
just placing name of the library in the binary file. The actual linking 
happens when the program is run, when both the binary file and the 
library are in memory. Examples of Dynamic libraries (libraries which 
are linked at run-time) are, ***.so*** in Linux and ***.dll*** in Windows.

We will soon be covering more points on Dynamic Libraries and steps to create them.

This article is compiled by **Abhijit Saha** and
 reviewed by GeeksforGeeks team. Please write comments if you find 
anything incorrect, or you want to share more information about the 
topic discussed above.
*** Working with Shared Libraries | Set 1
This article is not for those algo geeks. If you are interested in systems related stuff, just read on…

Shared
 libraries are useful in sharing code which is common across many 
applications. For example, it is more economic to pack all the code 
related to TCP/IP implementation in a shared library. However, data 
can’t be shared as every application needs its own set of data. 
Applications like, browser, ftp, telnet, etc… make use of the shared 
‘network’ library to elevate specific functionality.

Every 
operating system has its own representation and tool-set to 
create shared libraries. More or less the concepts are same. On Windows 
every object file (*.obj, *.dll, *.ocx, *.sys, *.exe etc…) follow a 
format called Portable Executable. Even shared libraries (called as 
Dynamic Linked Libraries or DLL in short) are also represented in PE 
format. The tool-set that is used to create these libraries need to 
understand the binary format. Linux variants follow a format called 
Executable and Linkable Format (ELF). The ELF files are position 
independent (PIC) format. Shared libraries in Linux are referred as 
shared objects (generally with extension *.so). These are similar to 
DLLs in Windows platform. Even shared object files follow the ELF binary
 format.

Remember, the file extensions (*.dll, *.so, *.a, *.lib, 
etc…) are just for programmer convenience. They don’t have any 
significance. All these are binary files. You can name them as you wish.
 Yet ensure you provide absolute paths in building applications.

In
 general, when we compile an application the steps are simple. Compile, 
Link and Load. However, it is not simple. These steps are more versatile
 on modern operating systems.

When you link your application 
against static library, the code is part of your application. There is 
no dependency. Even though it causes the application size to increase, 
it has its own advantages. The primary one is speed as there will be no 
symbol (a program entity) resolution at runtime. Since every piece of 
code part of the binary image, such applications are independent of 
version mismatch issues. However, the cost is on fixing an issue in 
library code. If there is any bug in library code, entire application 
need to be recompiled and shipped to the client. In case of dynamic 
libraries, fixing or upgrading the libraries is easy. You just need to 
ship the updated shared libraries. The application need not to 
recompile, it only need to re-run. You can design a mechanism where we 
don’t need to restart the application.

When we link an 
application against a shared library, the linker leaves some stubs 
(unresolved symbols) to be filled at application loading time. These 
stubs need to be filled by a tool called, *dynamic linker* at run 
time or at application loading time. Again loading of a library is of 
two types, static loading and dynamic loading. Don’t confuse between ***static loading*** vs ***static linking*** and ***dynamic loading*** vs ***dynamic linking***.

For example, you have built an application that depends on *libstdc++.so*
 which is a shared object (dynamic library). How does the application 
become aware of required shared libraries? (If you are interested, 
explore the tools *tdump* from Borland tool set, *objdump* or *nm* or *readelf* tools on Linux).

Static loading: 

- In static loading, all of those dependent shared libraries are loaded into memory even before the application starts execution. If loading of any
shared library fails, the application won’t run.
- A dynamic
loader examines application’s dependency on shared libraries. If these
libraries are already loaded into the memory, the library address space
is mapped to application virtual address space (VAS) and the dynamic
linker does relocation of unresolved symbols.
- If these libraries are not loaded into memory (perhaps your application might be first to
invoke the shared library), the loader searches in standard library
paths and loads them into memory, then maps and resolves symbols. Again
loading is big process, if you are interested write your own loader :).
- While resolving the symbols, if the dynamic linker not able to find any
symbol (maybe due to older version of shared library), the application
can’t be started.

Dynamic Loading: 

- As the name indicates, dynamic loading is about loading of library on demand.
- For example, if you want a small functionality from a shared library. Why
should it be loaded at the application load time and sit in the memory?
You can invoke loading of these shared libraries dynamically when you
need their functionality. This is called dynamic loading. In this case,
the programmer aware of situation ‘when should the library be loaded’.
The tool-set and relevant kernel provides API to support dynamic
loading, and querying of symbols in the shared library.

More details in later articles.

*Note:
 If you come across terms like loadable modules or equivalent terms, 
don’t mix them with shared libraries. They are different from 
shared libraries  The kernels provide framework to support loadable 
modules.*****

**[Working with Shared Libraries | Set 2](https://www.geeksforgeeks.org/working-with-shared-libraries-set-2/)**

**Exercise:**

1.
 Assuming you have understood the concepts, How do you design an 
application (e.g. Banking) which can upgrade to new shared libraries 
without re-running the application.

— **[Venki](http://www.linkedin.com/in/ramanawithu)**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
*** Working with Shared Libraries | Set 2
We have covered basic information about shared libraries in the [previous post](https://www.geeksforgeeks.org/working-with-shared-libraries-set-1/). In the current article, we will learn how to create shared libraries on Linux.

Prior
 to that, we need to understand how a program is loaded into memory, and
 the various (basic) steps involved in the process.

Let us see a typical “Hello World” program in C. Simple Hello World program screen image is given below.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/Hello-World-300x119.png

We were compiling our code using the command “**gcc -o sample shared.c**” When we compile our code, the compiler won’t resolve implementation of the function **printf()**.
 It only verifies the syntactical checking. The tool chain leaves a stub
 in our application which will be filled by a dynamic linker. Since 
printf is a standard function the compiler implicitly invokes its shared
 library. More details down.

We are using I*dd* to list 
dependencies of our program binary image. In the screen image, we can 
see our sample program depends on three binary files namely, *linux-vdso.so.1*, *libc.so.6* and */lib64/ld-linux-x86-64.so.2*.

The
 file VDSO is a fast implementation of the system call interface and 
some other stuff, it is not our focus (on some older systems you may see
 the different file name in a line of *.vsdo.*). Ignore this file. We 
have an interest in the other two files.

The file **libc.so.6** is the C implementation of various standard functions. It is the file where we see *printf* definition needed for our *Hello World*. It is the shared library needed to be loaded into memory to run our Hello World program.

The
 third file /lib64/ld-linux-x86-64.so.2 is in fact an executable that 
runs when an application is invoked. When we invoke the program on the 
bash terminal, typically the bash forks itself and replaces its address 
space with an image of the program to run (so-called fork-exec pair). 
The kernel verifies whether the libc.so.6 resides in the memory. If not,
 it will load the file into memory and does the relocation of libc.so.6 
symbols. It then invokes the dynamic linker 
(/lib64/ld-linux-x86-64.so.2) to resolve unresolved symbols of the 
application code (printf in the present case). Then the control 
transfers to our program *main*. (I have intentionally omitted many details in the process, our focus is to understand basic details).

**Creating our own shared library:**

Let us work with a simple shared library on Linux. Create a file **library.c** with the following content.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/library-300x164.png

The file library.c defines a function ***signum*** that will be used by our application code. Compile the file library.c file using the following command.

**gcc -shared -fPIC -o liblibrary.so library.c**

The flag-shared instructs the compiler that we are building a shared library. The flag *-fPIC* is to generate position-independent code (ignore for now). The command generates a shared library *liblibrary.so* in the current working directory. We have our shared object file (shared library name in Linux) ready to use.

Create another file **application.c** with the following content.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/application-300x125.png

In the file **application.c**
 we are invoking the function signum which was defined in a shared 
library. Compile the application.c file using the following command.

**gcc application.c -L /home/geetanjali/coding/ -library -o sample**

The flag *-library*
 instructs the compiler to look for symbol definitions that are not 
available in the current code (signum function in our case). The option *-L*
 is a hint to the compiler to look in the directory followed by the 
option for any shared libraries (during link-time only). The command 
generates an executable named “**sample**“.

If you 
invoke the executable, the dynamic linker will not be able to find the 
required shared library. By default, it won’t look into the current 
working directory. You have to explicitly instruct the tool chain to 
provide proper paths. The dynamic linker searches standard paths 
available in the LD_LIBRARY_PATH and also searches in the system cache 
(for details explore the command ***ldconfig***). We have to add our working directory to the LD_LIBRARY_PATH environment variable. The following command does the same.

**export LD_LIBRARY_PATH=/home/geetanjali/coding/:$LD_LIBRARY_PATH**

You can now invoke our executable as shown.

**./sample**

The sample output on my system is shown below.

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/output-300x28.png

**Note:** *The path **/home/geetanjali/coding/**
 is a working directory path on my machine. You need to use your working
 directory path wherever it is being used in the above commands.*

Stay tuned, we haven’t even explored 1/3rd of shared library concepts. More advanced concepts in the later articles.

**Exercise:**

It is a workbook like an article. You won’t gain much unless you practice and do some research.

1. Create a similar example and write your own function in the shared library. Invoke the function in another application.

2. Is (Are) there any other tool(s) that can list dependent libraries?

3. What is position independent code (PIC)?

4. What is system cache in the current context? How does the directory /etc/ld.so.conf.d/* related in the current context?

— **[Venki](http://www.linkedin.com/in/ramanawithu)**.
 Please write comments if you find anything incorrect, or if you want to
 share more information about the topic discussed above.
 
*** Named Pipe or FIFO with example C program
In computing, a named pipe (also known as a **FIFO**) is one of the methods for inter-process communication. 

- It is an extension to the traditional pipe concept on Unix. A traditional
pipe is “unnamed” and lasts only as long as the process.
- A named pipe, however, can last as long as the system is up, beyond the life of the process. It can be deleted if no longer used.
- Usually a
named pipe appears as a file and generally processes attach to it for
inter-process communication. A FIFO file is a special kind of file on
the local storage which allows two or more processes to communicate with each other by reading/writing to/from this file.
- A FIFO special file is entered into the filesystem by calling *mkfifo()* in C. Once we have created a FIFO special file in this way, any process can open it for reading or writing, in the same way as an ordinary
file. However, it has to be open at both ends simultaneously before you
can proceed to do any input or output operations on it.

**Creating a FIFO file:** In order to create a FIFO file, a function calls i.e. mkfifo is used.

`int` `mkfifo(const` `char` `*pathname, mode_t mode);`

mkfifo() makes a FIFO special file with name ***pathname***. Here ***mode***
 specifies the FIFO’s permissions. It is modified by the process’s umask
 in the usual way: the permissions of the created file are (mode & 
~umask).**Using FIFO:** As named pipe(FIFO) is a kind of file, we can use all the system calls associated with it i.e. *open*, *read*, *write*, *close*.**Example Programs to illustrate the named pipe:** There
 are two programs that use the same FIFO. Program 1 writes first, then 
reads. The program 2 reads first, then writes. They both keep doing it 
until terminated.

Program 1(Writes first)

```

// C program to implement one side of FIFO
// This side writes first, then reads
#include <stdio.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

int main()
{
    int fd;

    // FIFO file path
    char * myfifo = "/tmp/myfifo";

    // Creating the named file(FIFO)
    // mkfifo(<pathname>, <permission>)
    mkfifo(myfifo, 0666);

    char arr1[80], arr2[80];
    while (1)
    {
        // Open FIFO for write only
        fd = open(myfifo, O_WRONLY);

        // Take an input arr2ing from user.
        // 80 is maximum length
        fgets(arr2, 80, stdin);

        // Write the input arr2ing on FIFO
        // and close it
        write(fd, arr2, strlen(arr2)+1);
        close(fd);

        // Open FIFO for Read only
        fd = open(myfifo, O_RDONLY);

        // Read from FIFO
        read(fd, arr1, sizeof(arr1));

        // Print the read message
        printf("User2: %s\n", arr1);
        close(fd);
    }
    return 0;
}

```

Program 2(Reads First)

```

// C program to implement one side of FIFO
// This side reads first, then reads
#include <stdio.h>
#include <string.h>
#include <fcntl.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

int main()
{
    int fd1;

    // FIFO file path
    char * myfifo = "/tmp/myfifo";

    // Creating the named file(FIFO)
    // mkfifo(<pathname>,<permission>)
    mkfifo(myfifo, 0666);

    char str1[80], str2[80];
    while (1)
    {
        // First open in read only and read
        fd1 = open(myfifo,O_RDONLY);
        read(fd1, str1, 80);

        // Print the read string and close
        printf("User1: %s\n", str1);
        close(fd1);

        // Now open in write mode and write
        // string taken from user.
        fd1 = open(myfifo,O_WRONLY);
        fgets(str2, 80, stdin);
        write(fd1, str2, strlen(str2)+1);
        close(fd1);
    }
    return 0;
}

```

**Output:** Run the two programs simultaneously on two terminals.

!https://media.geeksforgeeks.org/wp-content/uploads/pic19.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic23.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic32.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic42.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic52.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic62.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic71.png

!https://media.geeksforgeeks.org/wp-content/uploads/pic81.png

This article is contributed by **[Kishlay Verma](https://www.linkedin.com/in/kishlayverma/)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [write.geeksforgeeks.org](http://www.write.geeksforgeeks.org/)
 or mail your article to review-team@geeksforgeeks.org. See your article
 appearing on the GeeksforGeeks main page and help other Geeks.Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
 
*** Tracing memory usage in Linux
Often
 it’s necessary to trace memory usage of the system in order to 
determine the program that consumes all CPU resources or the program 
that is responsible to slowing down the activities of the CPU. Tracing 
memory usage also becomes necessary to determine the load on the server.
 Parsing the usage data enables the servers to be able to balance the 
load and serve the user’s request without slowing down the system.

1. **free** Displays the amount of memory which is currently available and used by
the system(both physical and swapped). free command gathers this data by parsing /proc/meminfo. By default, the amount of memory is display in
kilobytes.
    
    **free command in UNIX**
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-16-35-e1496478387353.png
    
    ```
    watch -n 5 free -m watch command is used to execute a program periodically.
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-46-53-e1496478442792.png
    
    According
     to the image above, there is a total of 2000 MB of RAM and 1196 MB of 
    swap space allotted to Linux system. Out of this 2000 MB of RAM, 834 MB 
    is currently used where as 590 MB is free. Similarly for swap space, out
     of 1196 MB, 0 MB is use and 1196 MB is free currently in the system.
    
2. **vmstat** vmstat command is used to display virtual memory statistics of the
system. This command reports data about the memory, paging, disk and CPU activities, etc. The first use of this command returns the data
averages since the last reboot. Further uses returns the data based on
sampling periods of length delays.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-21-16-e1496478359885.png
    
    ```
    vmstat -d Reports disk statistics
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-21-50.png
    
    ```
    vmstat -sDisplays the amount of memory used and available
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-22-54-e1496478489876.png
    
3. **top** top command displays all the currently running process in the system.
This command displays the list of processes and thread currently being
handled by the kernel. top command can also be used to monitor the total amount of memory usage.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-31-08.png
    
    ```
     top -H Threads-mode operation
        Displays individual thread that are currently in the system. Without this command
    option, a summation of all thread in each process is displayed.
    ```
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-34-39.png
    
4. **/proc/meminfo** This file contains all the data about the memory usage. It provides the current memory usage details rather than old stored values.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-11-42-25-e1496478415564.png
    
5. **htop** htop is an interactive process viewer. This command is similar to top
command except that it allows to scroll vertically and horizontally to
allows users to view all processes running on the system, along with
their full command line as well as viewing them as a process tree,
selecting multiple processes and acting on them all at once.
    
    **working of htop command in UNIX:**
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Screenshot-from-2017-06-03-13-48-49.png
    

**Reference:**

- **[Ubuntu Manual](http://manpages.ubuntu.com/)**

This article is contributed by **[Mayank Kumar](https://www.linkedin.com/in/mayank-kumar-a9058b137/)**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.
** Disk Management
*** File Systems in Operating System
A 
file is a collection of related information that is recorded on 
secondary storage. Or file is a collection of logically related 
entities. From user’s perspective a file is the smallest allotment of 
logical secondary storage.

**The name  of the file is divided into two parts as shown below:**

- name
- extension, separated by a period.

### Files attributes and its operations:

[](https://www.notion.so/88bbf97ff98b4ac79ecf102748e1ecd1?pvs=21)

[](https://www.notion.so/544f229d8e7c4a199b3be9f970dbe846?pvs=21)

**FILE DIRECTORIES:** Collection
 of files is a file directory. The directory contains information about 
the files, including attributes, location and ownership. Much of this 
information, especially that is concerned with storage, is managed by 
the operating system. The directory is itself a file, accessible by 
various file management routines.

**Information contained in a device directory are:**

- Name
- Type
- Address
- Current length
- Maximum length
- Date last accessed
- Date last updated
- Owner id
- Protection information

**Operation performed on directory are:**

- Search for a file
- Create a file
- Delete a file
- List a directory
- Rename a file
- Traverse the file system

**Advantages of maintaining directories are:**

- **Efficiency:** A file can be located more quickly.
- **Naming:** It becomes convenient for users as two users can have same name for different files or may have different name for same file.
- **Grouping:** Logical grouping of files can be done by properties e.g. all java programs, all games etc.

**SINGLE-LEVEL DIRECTORY** In this a single directory is maintained for all the users.

- **Naming problem:** Users cannot have same name for two files.
- **Grouping problem:** Users cannot group files according to their need.

!https://media.geeksforgeeks.org/wp-content/uploads/directory.jpg

**TWO-LEVEL DIRECTORY** In this separate directories for each user is maintained. 

- Path name:Due to two levels there is a path name for every file to locate that file.
- Now,we can have same file name for different user.
- Searching is efficient in this method.

!https://media.geeksforgeeks.org/wp-content/uploads/directory-user-level.jpg

**TREE-STRUCTURED DIRECTORY :** Directory
 is maintained in the form of a tree. Searching is efficient and also 
there is grouping capability. We have absolute or relative path name for
 a file. 

!https://media.geeksforgeeks.org/wp-content/uploads/directory-tree-format.jpg

**FILE ALLOCATION METHODS** **:**

**1. Continuous Allocation –**A
 single continuous set of blocks is allocated to a file at the time of 
file creation. Thus, this is a pre-allocation strategy, using variable 
size portions. The file allocation table needs just a single entry for 
each file, showing the starting block and the length of the file. This 
method is best from the point of view of the individual sequential file.
 Multiple blocks can be read in at a time to improve I/O performance for
 sequential processing. It is also easy to retrieve a single block. For 
example, if a file starts at block b, and the ith block of the file is 
wanted, its location on secondary storage is simply b+i-1. 

!https://media.geeksforgeeks.org/wp-content/uploads/directory-file-allocation.jpg

**Disadvantage –**

- External fragmentation will occur, making it difficult to find contiguous blocks of space of sufficient length. Compaction algorithm will be necessary
to free up additional space on disk.
- Also, with pre-allocation, it is necessary to declare the size of the file at the time of creation.

**2. Linked Allocation(Non-contiguous allocation) –**Allocation
 is on an individual block basis. Each block contains a pointer to the 
next block in the chain. Again the file table needs just a single entry 
for each file, showing the starting block and the length of the file. 
Although pre-allocation is possible, it is more common simply to 
allocate blocks as needed. Any free block can be added to the chain. The
 blocks need not be continuous. Increase in file size is always possible
 if free disk block is available. There is no external fragmentation 
because only one block at a time is needed but there can be internal 
fragmentation but it exists only in the last disk block of file.

**Disadvantage –**

- Internal fragmentation exists in last disk block of file.
- There is an overhead of maintaining the pointer in every disk block.
- If the pointer of any disk block is lost, the file will be truncated.
- It supports only the sequential access of files.

**3. Indexed Allocation –**It
 addresses many of the problems of contiguous and chained allocation. In
 this case, the file allocation table contains a separate one-level 
index for each file: The index has one entry for each block allocated to
 the file. Allocation may be on the basis of fixed-size blocks or 
variable-sized blocks. Allocation by blocks eliminates external 
fragmentation, whereas allocation by variable-size blocks improves 
locality. This allocation technique supports both sequential and direct 
access to the file and thus is the most popular form of file 
allocation. 

!https://media.geeksforgeeks.org/wp-content/uploads/directory-indexing.jpg

**Disk Free Space Management :**

Just
 as the space that is allocated to files must be managed ,so the space 
that is not currently allocated to any file must be managed. To perform 
any of the file allocation techniques,it is necessary to know what 
blocks on the disk are available. Thus we need a disk allocation table 
in addition to a file allocation table.The following are the approaches 
used for free space management. 

1. **Bit Tables** : This method uses a vector containing one bit for each block on the
disk. Each entry for a 0 corresponds to a free block and each 1
corresponds to a block in use. For example: 00011010111100110001
    
    In
     this vector every bit correspond to a particular block and 0 implies 
    that, that particular block is free and 1 implies that the block is 
    already occupied. A bit table has the advantage that it is relatively 
    easy to find one or a contiguous group of free blocks. Thus, a bit table
     works well with any of the file allocation methods. Another advantage 
    is that it is as small as possible.
    
2. **Free Block List** : In this method, each block is assigned a number sequentially and the
list of the numbers of all free blocks is maintained in a reserved block of the disk.

!https://media.geeksforgeeks.org/wp-content/uploads/directory-table.jpg

This article is contributed by **Aakansha yadav**

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Unix File System
Unix file system is a logical method of **organizing and storing**
 large amounts of information in a way that makes it easy to manage. A 
file is a smallest unit in which the information is stored. Unix file 
system has several important features. All data in Unix is organized 
into files. All files are organized into directories. These directories 
are organized into a tree-like structure called the file system.

Files
 in Unix System are organized into multi-level hierarchy structure known
 as a directory tree. At the very top of the file system is a directory 
called “root” which is represented by a “/”. All other files are 
“descendants” of root.

Hello friends and welcome to

Remaining Time -5:45

1x

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/unix.png

**Directories or Files and their description –**

- **/ :** The slash / character alone denotes the root of the filesystem tree.
- **/bin :** Stands for “binaries” and contains certain fundamental utilities, such as ls or cp, which are generally needed by all users.
- **/boot :** Contains all the files that are required for successful booting process.
- **/dev :** Stands for “devices”. Contains file representations of peripheral devices and pseudo-devices.
- **/etc :** Contains system-wide configuration files and system databases. Originally also
contained “dangerous maintenance utilities” such as init,but these have
typically been moved to /sbin or elsewhere.
- **/home :** Contains the home directories for the users.
- **/lib :** Contains system libraries, and some critical files such as kernel modules or device drivers.
- **/media :** Default mount point for removable devices, such as USB sticks, media players, etc.
- **/mnt :** Stands for “mount”. Contains filesystem mount points. These are used, for
example, if the system uses multiple hard disks or hard disk partitions. It is also often used for remote (network) filesystems, CD-ROM/DVD
drives, and so on.
- **/proc :** procfs virtual filesystem showing information about processes as files.
- **/root :** The home directory for the superuser “root” – that is, the system
administrator. This account’s home directory is usually on the initial
filesystem, and hence not in /home (which may be a mount point for
another filesystem) in case specific maintenance needs to be performed,
during which other filesystems are not available. Such a case could
occur, for example, if a hard disk drive suffers physical failures and
cannot be properly mounted.
- **/tmp :** A place for
temporary files. Many systems clear this directory upon startup; it
might have tmpfs mounted atop it, in which case its contents do not
survive a reboot, or it might be explicitly cleared by a startup script
at boot time.
- **/usr :** Originally the directory
holding user home directories,its use has changed. It now holds
executables, libraries, and shared resources that are not system
critical, like the X Window System, KDE, Perl, etc. However, on some
Unix systems, some user accounts may still have a home directory that is a direct subdirectory of /usr, such as the default as in Minix. (on
modern systems, these user accounts are often related to server or
system use, and not directly used by a person).
- **/usr/bin :** This directory stores all binary programs distributed with the operating system not residing in /bin, /sbin or (rarely) /etc.
- **/usr/include :** Stores the development headers used throughout the system. Header files are mostly used by the **#include** directive in C/C++ programming language.
- **/usr/lib :** Stores the required libraries and data files for programs stored within /usr or elsewhere.
- **/var :** A short for “variable.” A place for files that may change often –
especially in size, for example e-mail sent to users on the system, or
process-ID lock files.
- **/var/log :** Contains system log files.
- **/var/mail :** The place where all the incoming mails are stored. Users (other than root)
can access their own mail only. Often, this directory is a symbolic link to /var/spool/mail.
- **/var/spool :** Spool directory. Contains print jobs, mail spools and other queued tasks.
- **/var/tmp :** A place for temporary files which should be preserved between system reboots.

**Types of Unix files –** The UNIX files system contains several different types of files :

!https://media.geeksforgeeks.org/wp-content/cdn-uploads/unix-file-system.png

**1. Ordinary files –** An ordinary file is a file on the system that contains data, text, or program instructions.

- Used to store your information, such as some text you have written or an
image you have drawn. This is the type of file that you usually work
with.
- Always located within/under a directory file.
- Do not contain other files.
- In long-format output of ls -l, this type of file is specified by the “-” symbol.

**2. Directories –**
 Directories store both special and ordinary files. For users familiar 
with Windows or Mac OS, UNIX directories are equivalent to folders. A 
directory file contains an entry for every file and subdirectory that it
 houses. If you have 10 files in a directory, there will be 10 entries 
in the directory. Each entry has two components.(1) The Filename(2) A unique identification number for the file or directory (called the inode number)

- Branching points in the hierarchical tree.
- Used to organize groups of files.
- May contain ordinary files, special files or other directories.
- Never contain “real” information which you would work with (such as text). Basically, just used for organizing files.
- All files are descendants of the root directory, ( named / ) located at the top of the tree.

In long-format output of ls –l , this type of file is specified by the “d” symbol.

**3. Special Files –** Used to represent a real physical device such as a printer, tape drive or terminal, used for Input/Output (I/O) operations. **Device or special files**
 are used for device Input/Output(I/O) on UNIX and Linux systems. They 
appear in a file system just like an ordinary file or a directory.On UNIX systems there are two flavors of special files for each device, character special files and block special files :

- When a character special file is used for device Input/Output(I/O), data is
transferred one character at a time. This type of access is called raw
device access.
- When a block special file is used for device
Input/Output(I/O), data is transferred in large fixed-size blocks. This
type of access is called block device access.

For terminal 
devices, it’s one character at a time. For disk devices though, raw 
access means reading or writing in whole chunks of data – blocks, which 
are native to your disk.

- In long-format output of ls -l, character special files are marked by the “c” symbol.
- In long-format output of ls -l, block special files are marked by the “b” symbol.

**4. Pipes –**
 UNIX allows you to link commands together using a pipe. The pipe acts a
 temporary file which only exists to hold data from one command until it
 is read by another.A Unix pipe provides a one-way flow of data.The 
output or result of the first command sequence is used as the input to 
the second command sequence. To make a pipe, put a vertical bar (|) on 
the command line between two commands.For example: **who | wc -l**

In long-format output of ls –l , named pipes are marked by the “p” symbol.

**5. Sockets –**
 A Unix socket (or Inter-process communication socket) is a special file
 which allows for advanced inter-process communication. A Unix Socket is
 used in a client-server application framework. In essence, it is a 
stream of data, very similar to network stream (and network sockets), 
but all the transactions are local to the filesystem.

In long-format output of ls -l, Unix sockets are marked by “s” symbol.

**6. Symbolic Link –**
 Symbolic link is used for referencing some other file of the file 
system.Symbolic link is also known as Soft link. It contains a text form
 of the path to the file it references. To an end user, symbolic link 
will appear to have its own name, but when you try reading or writing 
data to this file, it will instead reference these operations to the 
file it points to. If we delete the soft link itself , the data file 
would still be there.If we delete the source file or move it to a 
different location, symbolic file will not function properly.

In long-format output of ls –l , Symbolic link are marked by the “l” symbol (that’s a lower case L).**Reference –**

[UNIX – Concepts and Applications](http://www.mheducation.co.in/9780070635463-india-unix-concepts-and-applications) | Sumitabha Das |Tata McGraw Hill |4th Edition

This article is contributed by **Saloni Gupta** . If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Privileged and Non-Privileged Instructions in Operating System
In any Operating System, it is necessary to have a **[Dual Mode Operation](https://www.geeksforgeeks.org/dual-mode-operations-os/)**
 to ensure the protection and security of the System from unauthorized 
or errant users. This Dual Mode separates the User Mode from the System 
Mode or Kernel Mode.

!https://media.geeksforgeeks.org/wp-content/uploads/Untitled-drawing-9-1.jpg

**What are Privileged Instructions?**

> The Instructions that can run only in Kernel Mode are called Privileged Instructions .
> 

Privileged Instructions possess the following characteristics :

(i)
 If any attempt is made to execute a Privileged Instruction in User 
Mode, then it will not be executed and treated as an illegal 
instruction. The Hardware traps it in the Operating System.

(ii) Before transferring the control to any User Program, it is the responsibility of the Operating System to ensure that the **Timer** is set to interrupt. Thus, if the timer interrupts then the Operating System regains the control. Thus, any instruction which can modify the contents of the Timer is Privileged Instruction.

(iii) Privileged Instructions are used by the Operating System in order to achieve correct operation.

(iv) Various examples of Privileged Instructions include:

- I/O instructions and Halt instructions
- Turn off all Interrupts
- Set the Timer
- Context Switching
- Clear the Memory or Remove a process from the Memory
- Modify entries in the Device-status table

**What are Non-Privileged Instructions?** 

> The Instructions that can run only in User Mode are called Non-Privileged Instructions .
> 

Various examples of Non-Privileged Instructions include:

- Reading the status of Processor
- Reading the System Time
- Generate any Trap Instruction
- Sending the final printout of Printer

Also,
 it is important to note that in order to change the mode from 
Privileged to Non-Privileged, we require a Non-privileged Instruction 
that does not generate any interrupt.
*** Path Name in File Directory
Prerequisite – [File Systems](https://www.geeksforgeeks.org/file-system-operating-systems/)

**Hierarchical Directory Systems –**

Directory
 is maintained in the form of a tree. Each user can have as many 
directories as are needed so, that files can be grouped together in a 
natural way.

Advantages of this structure:

- Searching is efficient
- Groping capability of files increase

When the file system is organized as a directory tree, some way is needed for specifying file names.

Two different methods are commonly used:

1. **Absolute Path name –** In this method, each file is given an **absolute path** name consisting of the path from the root directory to the file. As an example, the path **/usr/ast/mailbox** means that the root directory contains a subdirectory usr, which in
turn contains a subdirectory ast, which contains the file mailbox.
    
    Absolute path names always start at the root directory and are unique.
    
    In UNIX the components of the path are separated by ‘/’. In Windows, the separator is ‘\’.**Windows** \usr\ast\mailbox**UNIX** /usr/ast/mailbox
    
2. **Relative Path name –** This is used in conjunction with the concept of the **working directory** (also called the **current directory**).A user can designate one directory as the current working directory, in
which case all path names not beginning at the root directory are taken
relative to the working directory.
    
    For **example**, if 
    the current working directory is /usr/ast, then the file whose absolute 
    path is /usr/ast/mailbox can be referenced simply as mailbox.In other words, the UNIXcommand : **cp /usr/ast/mailbox /usr/ast/mailbox.bak**and the command : **cp mailbox mailbox.bak**do exactly the same thing if the working directory is /usr/ast.
    

**When to use which approach?**Some
 programs need to access a specific file without regard to what the 
working directory is. In that case, they should always use absolute path
 names. For example, a spelling checker might need to read 
/usr/lib/dictionary to do its work. It should use the full, absolute 
path name in this case because it does not know what the working 
directory will be when it is called. The absolute path name will always 
work, no matter what the working directory is.

Of course, if the 
spelling checker needs a large number of files from /usr/lib, an 
alternative approach is for it to issue a system call to change its 
working directory to /usr/lib, and then use just dictionary as the first
 parameter to open. By explicitly changing the working directory, it 
knows for sure where it is in the directory tree, so it can then use 
relative paths.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** Structures of Directory in Operating System
A **directory** is a container that is used to contain folders and files. It organizes files and folders in a hierarchical manner.

!https://media.geeksforgeeks.org/wp-content/uploads/111-11.png

There are several logical structures of a directory, these are given below.

- **Single-level directory –** The single-level directory is the simplest directory structure. In it, all
files are contained in the same directory which makes it easy to support and understand.
    
    A single level directory has a significant 
    limitation, however, when the number of files increases or when the 
    system has more than one user. Since all the files are in the same 
    directory, they must have a unique name. if two users call their dataset
     test, then the unique name rule violated.
    

!https://media.geeksforgeeks.org/wp-content/uploads/222-13.png

**Advantages:**

- Since it is a single directory, so its implementation is very easy.
- If the files are smaller in size, searching will become faster.
- The operations like file creation, searching, deletion, updating are very easy in such a directory structure.

**Disadvantages:**

- There may chance of name collision because two files can have the same name.
- Searching will become time taking if the directory is large.
- This can not group the same type of files together.
- **Two-level directory –** As we have seen, a single level directory often leads to confusion of
files names among different users. the solution to this problem is to
create a separate directory for each user.
    
    In the two-level directory structure, each user has their own *user files directory (UFD)*. The UFDs have similar structures, but each lists only the files of a single user. system’s *master file directory (MFD)*
     is searches whenever a new user id=s logged in. The MFD is indexed by 
    username or account number, and each entry points to the UFD for that 
    user.
    

!https://media.geeksforgeeks.org/wp-content/uploads/222-2-1.png

**Advantages:**

- We can give full path like /User-name/directory-name/.
- Different users can have the same directory as well as the file name.
- Searching of files becomes easier due to pathname and user-grouping.

**Disadvantages:**

- A user is not allowed to share files with other users.
- Still, it not very scalable, two files of the same type cannot be grouped together in the same user.
- **Tree-structured directory –** Once we have seen a two-level directory as a tree of height 2, the natural
generalization is to extend the directory structure to a tree of
arbitrary height. This generalization allows the user to create their own subdirectories and to organize their files accordingly.

!https://media.geeksforgeeks.org/wp-content/uploads/222-3-1.png

A
 tree structure is the most common directory structure. The tree has a 
root directory, and every file in the system has a unique path.

**Advantages:**

- Very general, since full pathname can be given.
- Very scalable, the probability of name collision is less.
- Searching becomes very easy, we can use both absolute paths as well as relative.

**Disadvantages:**

- Every file does not fit into the hierarchical model, files may be saved into multiple directories.
- We can not share files.
- It is inefficient, because accessing a file may go under multiple directories.
- **Acyclic graph directory –** An acyclic graph is a graph with no cycle and allows us to share
subdirectories and files. The same file or subdirectories may be in two
different directories. It is a natural generalization of the
tree-structured directory.
    
    It is used in the situation like when two 
    programmers are working on a joint project and they need to access 
    files. The associated files are stored in a subdirectory, separating 
    them from other projects and files of other programmers since they are 
    working on a joint project so they want the subdirectories to be into 
    their own directories. The common subdirectories should be shared. So 
    here we use Acyclic directories.
    
    It is the point to note that the
     shared file is not the same as the copy file. If any programmer makes 
    some changes in the subdirectory it will reflect in both subdirectories.
    

!https://media.geeksforgeeks.org/wp-content/uploads/222-4-1.png

**Advantages:**

- We can share files.
- Searching is easy due to different-different paths.

**Disadvantages:**

- We share the files via linking, in case deleting it may create the problem,
- If the link is a soft link then after deleting the file we left with a dangling pointer.
- In the case of a hard link, to delete a file we have to delete all the references associated with it.
- **General graph directory structure –** In general graph directory structure, cycles are allowed within a
directory structure where multiple directories can be derived from more
than one parent directory. The main problem with this kind of
directory structure is to calculate the total size or space that has
been taken by the files and directories.

!https://media.geeksforgeeks.org/wp-content/uploads/333-5.png

**Advantages:**

- It allows cycles.
- It is more flexible than other directories structure.

**Disadvantages:**

- It is more costly than others.
- It needs garbage collection.
*** File Allocation Methods
The
 allocation methods define how the files are stored in the disk blocks. 
There are three main disk space or file allocation methods.

- Contiguous Allocation
- Linked Allocation
- Indexed Allocation

The main idea behind these methods is to provide:

- Efficient disk space utilization.
- Fast access to the file blocks.

All the three methods have their own advantages and disadvantages as discussed below:

**1. Contiguous Allocation**

In
 this scheme, each file occupies a contiguous set of blocks on the disk.
 For example, if a file requires n blocks and is given a block b as the 
starting location, then the blocks assigned to the file will be: *b, b+1, b+2,……b+n-1.*
 This means that given the starting block address and the length of the 
file (in terms of blocks required), we can determine the blocks occupied
 by the file.The directory entry for a file with contiguous allocation contains

- Address of starting block
- Length of the allocated portion.

The *file ‘mail’* in the following figure starts from the block 19 with length = 6 blocks. Therefore, it occupies *19, 20, 21, 22, 23, 24* blocks.

**Advantages:**

!https://media.geeksforgeeks.org/wp-content/uploads/Contiguous-Allocation.jpg

- Both the Sequential and Direct Accesses are supported by this. For direct
access, the address of the kth block of the file which starts at block b can easily be obtained as (b+k).
- This is extremely fast since the number of seeks are minimal because of contiguous allocation of file blocks.

**Disadvantages:**

- This method suffers from both internal and external fragmentation. This makes it inefficient in terms of memory utilization.
- Increasing file size is difficult because it depends on the availability of contiguous memory at a particular instance.

**2. Linked List Allocation**

In this scheme, each file is a linked list of disk blocks which **need not be** contiguous. The disk blocks can be scattered anywhere on the disk.The
 directory entry contains a pointer to the starting and the ending file 
block. Each block contains a pointer to the next block occupied by the 
file.

*The file ‘jeep’ in following image shows how the blocks 
are randomly distributed. The last block (25) contains -1 indicating a 
null pointer and does not point to any other block.*

!https://media.geeksforgeeks.org/wp-content/uploads/linkedListAllocation.jpg

**Advantages:**

- This is very flexible in terms of file size. File size can be increased
easily since the system does not have to look for a contiguous chunk of
memory.
- This method does not suffer from external fragmentation. This makes it relatively better in terms of memory utilization.

**Disadvantages:**

- Because the file blocks are distributed randomly on the disk, a large number of seeks are needed to access every block individually. This makes linked
allocation slower.
- It does not support random or direct access.
We can not directly access the blocks of a file. A block k of a file can be accessed by traversing k blocks sequentially (sequential access )
from the starting block of the file via block pointers.
- Pointers required in the linked allocation incur some extra overhead.

**3. Indexed Allocation**

In this scheme, a special block known as the **Index block**
 contains the pointers to all the blocks occupied by a file. Each file 
has its own index block. The ith entry in the index block contains the 
disk address of the ith file block. The directory entry contains the 
address of the index block as shown in the image:

**Advantages:**

!https://media.geeksforgeeks.org/wp-content/uploads/indexedAllocation.jpg

- This supports direct access to the blocks occupied by the file and therefore provides fast access to the file blocks.
- It overcomes the problem of external fragmentation.

**Disadvantages:**

- The pointer overhead for indexed allocation is greater than linked allocation.
- For very small files, say files that expand only 2-3 blocks, the indexed
allocation would keep one entire block (index block) for the pointers
which is inefficient in terms of memory utilization. However, in linked
allocation we lose the space of only 1 pointer per block.

For files that are very large, single index block may not be able to hold all the pointers.Following mechanisms can be used to resolve this:

1. **Linked scheme:** This scheme links two or more index blocks together for holding the
pointers. Every index block would then contain a pointer or the address
to the next index block.
2. **Multilevel index:** In
this policy, a first level index block is used to point to the second
level index blocks which inturn points to the disk blocks occupied by
the file. This can be extended to 3 or more levels depending on the
maximum file size.
3. **Combined Scheme:** In this scheme, a special block called the **Inode (information Node)** contains all the information about the file such as the name, size,
authority, etc and the remaining space of Inode is used to store the
Disk Block addresses which contain the actual file *as shown in the image below.* The first few of these pointers in Inode point to the **direct blocks** i.e the pointers contain the addresses of the disk blocks that contain
data of the file. The next few pointers point to indirect blocks.
Indirect blocks may be single indirect, double indirect or triple
indirect. **Single Indirect block** is the disk block that
does not contain the file data but the disk address of the blocks that
contain the file data. Similarly, **double indirect blocks** do not contain the file data but the disk address of the blocks that
contain the address of the blocks containing the file data.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/Combined-Scheme.jpg
    

This article is contributed by **Saloni Baweja**. If you like GeeksforGeeks and would like to contribute, you can also write an article using [contribute.geeksforgeeks.org](http://www.contribute.geeksforgeeks.org/)
 or mail your article to contribute@geeksforgeeks.org. See your article 
appearing on the GeeksforGeeks main page and help other Geeks.

Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp
*** File Access Methods in Operating System
Prerequisite – [File Systems](https://www.geeksforgeeks.org/file-system-operating-systems/) When
 a file is used, information is read and accessed into computer memory 
and there are several ways to access this information of the file. Some 
systems provide only one access method for files. Other systems, such as
 those of IBM, support many access methods, and choosing the right one 
for a particular application is a major design problem.

There are three ways to access a file into a computer system: Sequential-Access, Direct Access, Index sequential Method.

1. **Sequential Access –** It is the simplest access method. Information in the file is processed in
order, one record after the other. This mode of access is by far the
most common; for example, editor and compiler usually access the file in this fashion.
    
    Read and write make up the bulk of the operation on a file. A read operation *-read next-*
     read the next position of the file and automatically advance a file 
    pointer, which keeps track I/O location. Similarly, for the -write *next-* append to the end of the file and advance to the newly written material.
    
    **Key points:**
    
    - Data is accessed one record right after another record in an order.
    - When we use read command, it move ahead pointer by one
    - When we use write command, it will allocate memory and move the pointer to the end of the file
    - Such a method is reasonable for tape.
2. **Direct Access –** Another method is *direct access method* also known as *relative access method*. A filed-length logical record that allows the program to read and write record rapidly. in no particular order. The direct access is based on
the disk model of a file since disk allows random access to any file
block. For direct access, the file is viewed as a numbered sequence of
block or record. Thus, we may read block 14 then block 59, and then we
can write block 17. There is no restriction on the order of reading and
writing for a direct access file. A block number provided by the user to the operating system is normally a *relative block number*, the first relative block of the file is 0 and then 1 and so on. 
3. **Index sequential method –** It is the other method of accessing a file that is built on the top of the sequential access method. These methods construct an index for the
file. The index, like an index in the back of a book, contains the
pointer to the various blocks. To find a record in the file, we first
search the index, and then by the help of pointer we access the file
directly.
    
    **Key points:**
    
    - It is built on top of Sequential access.
    - It control the pointer by using index.
    
*** Secondary Memory
In a
 computer, memory refers to the physical devices that are used to store 
programs or data on a temporary or permanent basis. It is a group of 
registers. Memory are of two types (i) primary memory, (ii) secondary 
memory. Primary memory is made up of semiconductors, It is also divided 
into two types, Read-Only Memory (ROM) and Random Access Memory (RAM). 
Secondary memory is a physical device for the permanent storage of 
programs and data(Hard disk, Compact disc, Flash drive, etc.).

!https://media.geeksforgeeks.org/wp-content/uploads/20210525193336/memory-660x292.jpg

### Primary Memory

Primary
 memory is made up of semiconductors and it is the main memory of the 
computer system. It is generally used to store data or information on 
which the computer is currently working, so we can say that it is used 
to store data temporarily. Data or information is lost when the systems 
are off. It is also divided into two types:

(i). Read-Only Memory (ROM)

(ii). Random Access Memory (RAM).

**1. Random Access Memory:**
 Primary memory is also called internal memory. This is the main area in
 a computer where data, instructions, and information are stored. Any 
storage location in this memory can be directly accessed by the Central 
Processing Unit. As the CPU can randomly access any storage location in 
this memory, it is also called Random Access Memory or RAM. The CPU can 
access data from RAM as long as the computer is switched on. As soon as 
the power to the computer is switched off, the stored data and 
instructions disappear from RAM. Such type of memory is known as 
volatile memory. RAM is also called read/write memory.

**2. Read-Only Memory:**
  Read-Only Memory (ROM) is a type of primary memory from which 
information can only be read. So it is also known as Read-Only Memory. 
ROM can be directly accessed by the Central Processing Unit. But, the 
data and instructions stored in ROM are retained even when the computer 
is switched off OR we can say it holds the data after being switched 
off. Such type of memory is known as non-volatile memory.

### Secondary Memory

We
 have read so far, that primary memory is volatile and has limited 
capacity. So, it is important to have another form of memory that has a 
larger storage capacity and from which data and programs are not lost 
when the computer is turned off. Such a type of memory is called 
secondary memory. In secondary memory, programs and data are stored. It 
is also called auxiliary memory. It is different from primary memory as 
it is not directly accessible through the CPU and is non-volatile. 
Secondary or external storage devices have a much larger storage 
capacity and the cost of secondary memory is less as compared to primary
 memory.

### Use of Secondary memory

Secondary memory is used for different purposes but the main purposes of using secondary memory are:

- **Permanent storage:** As we know that primary memory stores data only when the power supply
is on, it loses data when the power is off. So we need a secondary
memory to stores data permanently even if the power supply is off.
- **Large Storage:** Secondary memory provides large storage space so that we can store large data
like videos, images, audios, files, etc permanently.
- **Portable:** Some secondary devices are removable. So, we can easily store or transfer data from one computer or device to another.

### Types of Secondary memory

Secondary memory is of two types:

**1. Fixed storage**

In
 secondary memory, a fixed storage is an internal media device that is 
used to store data in a computer system. Fixed storage is generally 
known as fixed disk drives or hard drives. Generally, the data of the 
computer system is stored in a built-in fixed storage device. Fixed 
storage does not mean that you can not remove them from the computer 
system, you can remove the fixed storage device for repairing, for the 
upgrade, or for maintenance, etc. with the help of an expert or 
engineer.

**Types of fixed storage:**

Following are the types of fixed storage:

- Internal flash memory (rare)
- SSD (solid-state disk)
- Hard disk drives (HDD)

**2. Removable storage**

In
 secondary memory, removable storage is an external media device that is
 used to store data in a computer system. Removable storage is generally
 known as disks drives or external drives. It is a storage device that 
can be inserted or removed from the computer according to our 
requirements. We can easily remove them from the computer system while 
the computer system is running. Removable storage devices are portable 
so we can easily transfer data from one computer to another. Also, 
removable storage devices provide the fast data transfer rates 
associated with storage area networks (SANs).

**Types of Removable Storage:**

- Optical discs (like CDs, DVDs, Blu-ray discs, etc.)
- Memory cards
- Floppy disks
- Magnetic tapes
- Disk packs
- Paper storage (like punched tapes, punched cards, etc.)

### Secondary memory devices

Following are the commonly used secondary memory devices are:

**1. Floppy Disk:** A
 floppy disk consists of a magnetic disc in a square plastic case. It is
 used to store data and to transfer data from one device to another 
device. Floppy disks are available in two sizes (a) Size: 3.5 inches, 
the Storage capacity of 1.44 MB (b) Size: 5.25 inches, the Storage 
capacity of 1.2 MB. To use a floppy disk, our computer needs to have a 
floppy disk drive. This storage device becomes obsolete now and has been
 replaced by CDs, DVDs, and flash drives.

**2. Compact Disc:** A
 Compact Disc (CD) is a commonly used secondary storage device. It 
contains tracks and sectors on its surface. Its shape is circular and is
 made up of polycarbonate plastic. The storage capacity of CD is up to 
700 MB of data. A CD may also be called a CD-ROM (Compact Disc Read-Only
 Memory), in this computers can read the data present in a CD-ROM, but 
cannot write new data onto it. For a CD-ROM, we require a CD-ROM. CD is 
of two types:

- **CD-R (compact disc recordable):** Once the data has been written onto it cannot be erased, it can only be read.
- **CD-RW (compact disc rewritable):** It is a special type of CD in which data can be erased and rewritten as many times as we want. It is also called an erasable CD.

**3. Digital Versatile Disc:** A
 Digital Versatile Disc also known as DVD it is looks just like a CD, 
but the storage capacity is greater compared to CD, it stores up to 4.7 
GB of data. DVD-ROM drive is needed to use DVD on a computer. The video 
files, like movies or video recordings, etc., are generally stored on 
DVD and you can run DVD using the DVD player. DVD is of three types:

- **DVD-ROM(Digital Versatile Disc Readonly):** In DVD-ROM the manufacturer writes the data in it and the user can only read that data, cannot write new data in it. For example movie DVD,
movie DVD is already written by the manufacturer we can only watch the
movie but we cannot write new data into it.
- **DVD-R(Digital Versatile Disc Recordable):** In DVD-R you can write the data but only one time. Once the data has been written onto it cannot be erased, it can only be read.
- **DVD-RW(Digital Versatile Disc Rewritable and Erasable):** It is a special type of DVD in which data can be erased and rewritten as
many times as we want. It is also called an erasable DVD.

**4. Blu-ray Disc:**
 A Blu-ray disc looks just like a CD or a DVD but it can store data or 
information up to 25 GB data. If you want to use a Blu-ray disc, you 
need a Blu-ray reader. The name Blu-ray is derived from the technology 
that is used to read the disc ‘Blu’ from the blue-violet laser and ‘ray’
 from an optical ray.

**5. Hard Disk:** A hard disk 
is a part of a unit called a hard disk drive. It is used to storing a 
large amount of data. Hard disks or hard disk drives come in different 
storage capacities.(like 256 GB, 500 GB, 1 TB, and 2 TB, etc.). It is 
created using the collection of discs known as platters. The platters 
are placed one below the other. They are coated with magnetic material. 
Each platter consists of a number of invisible circles and each circle 
having the same centre called tracks. Hard disk is of two types (i) 
Internal hard disk (ii) External hard disk.

**6. Flash Drive:**
 A flash drive or pen drive comes in various storage capacities, such as
 1 GB, 2 GB, 4 GB, 8 GB, 16 GB, 32 GB, 64 GB, up to 1 TB. A flash drive 
is used to transfer and store data. To use a flash drive, we need to 
plug it into a USB port on a computer. As a flash drive is easy to use 
and compact in size, Nowadays it is very popular.

**7. Solid-state disk:** It
 is also known as SDD. It is a non-volatile storage device that is used 
to store and access data. It is faster, does noiseless 
operations(because it does not contain any moving parts like the hard 
disk), consumes less power, etc. It is a great replacement for standard 
hard drives in computers and laptops if the price is low and it is also 
suitable for tablets, notebooks, etc because they do not require large 
storage.

**8. SD Card:** It is known as a Secure 
Digital Card. It is generally used in portable devices like mobile 
phones, cameras, etc., to store data. It is available in different sizes
 like 1 GB, 2 GB, 4 GB, 8 GB, 16 GB, 32 GB, 64 GB, etc. To view the data
 stored in the SD card you can remove them from the device and insert 
them into a computer with help of a card reader. The data stores in the 
SD card is stored in memory chips(present in the SD Card) and it does 
not contain any moving parts like the hard disk.

### Sample Problem

**Question 1: What are the differences between primary and secondary memories?**

**Solution:**

> Primary Memory                    Secondary Memory(i) Primary memory devices are semiconductor memories.(i) Secondary memory devices are magnetic, optical and electronic memories.(ii) Primary memory can be volatile as well as non-volatile. RAM is the volatile memory and ROM is the non-volatile memory,(ii) Secondary memory is always non-volatile(iii) Primary memory is more effective and interacts more quickly with the microprocessor(iii) Secondary memory is somewhat slower in interacting with the microprocessor than primary memory.
> 

**Question 2: What is memory?**

**Solution:**

> Memory refers to the physical devices that are used to store programs or data on a temporary or permanent basis.
> 

**Question 3: What is a volatile memory?**

**Solution:**

> Memory that loses its content when the power is turned off is known as volatile memory.
> 

**Question 4:  What is non-volatile memory?**

**Solution:**

> Memory that retains its content even after the power is turned off is known as non-volatile memory.
> 

**Question 5: Why do we need secondary memory?**

**Solution:**

> As
 we know, that primary memory is volatile and has limited capacity. So, 
it is important to have another form of memory that has a larger storage
 capacity and from which data and programs are not lost when the 
computer is turned off. Such a type of memory is known as secondary 
memory. Secondary memory has a much larger storage capacity than primary
 memory. Secondary memory is non-volatile memory.
> 

**Question 6: Name some secondary storage devices.**

**Solution:**

> Floppy disk, Hard disk, Compact disc, Digital versatile disc, Flash drive.
> 

**Question 7: Which secondary memory is faster, smaller**, **and more durable?**

**Solution:**

> Flash drive or Pen drive.
> 

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-24-10-54-13-BIwizardInArticleAd.webp
*** Hard Disk Drive (HDD) Secondary memory
A hard disk is a memory storage device that looks like this:

!https://media.geeksforgeeks.org/wp-content/uploads/harddisk1.jpg

The disk is divided into **tracks**. Each track is further divided into **sectors**.
 The point to be noted here is that outer tracks are bigger in size than
 the inner tracks but they contain the same number of sectors and have 
equal storage capacity. This is because the storage density is high in 
sectors of the inner tracks whereas the bits are sparsely arranged in 
sectors of the outer tracks. Some space of every sector is used for 
formatting. So, the actual capacity of a sector is less than the given 
capacity.

Read-Write(R-W) head moves over the rotating hard disk.
 It is this Read-Write head that performs all the read and writes 
operations on the disk and hence, the position of the R-W head is a 
major concern. To perform a read or write operation on a memory 
location, we need to place the R-W head over that position. Some 
important terms must be noted here:

1. **Seek time –** The time taken by the R-W head to reach the desired track from its current position.
2. **Rotational latency –** Time is taken by the sector to come under the R-W head.
3. **Data transfer time –** Time is taken to transfer the required amount of data. It depends upon the rotational speed.
4. **Controller time –** The processing time taken by the controller.
5. **Average Access time –** seek time + Average Rotational latency + data transfer time + controller time.

**Note:** Average Rotational latency is mostly 1/2*(Rotational latency).

In questions, if the seek time and controller time are not mentioned, take them to be zero.

If
 the amount of data to be transferred is not given, assume that no data 
is being transferred. Otherwise, calculate the time taken to transfer 
the given amount of data.

The average rotational latency is taken
 when the current position of the R-W head is not given. Because the R-W
 may be already present at the desired position or it might take a whole
 rotation to get the desired sector under the R-W head. But, if the 
current position of the R-W head is given then the rotational latency 
must be calculated.

**Example –** Consider a hard disk with: 4 surfaces 64 tracks/surface 128 sectors/track 256 bytes/sector

1. What is the capacity of the hard disk? Disk capacity = surfaces * tracks/surface * sectors/track * bytes/sector Disk capacity = 4 * 64 * 128 * 256 Disk capacity = 8 MB
2. The disk is rotating at 3600 RPM, what is the data transfer rate? 60 sec -> 3600 rotations 1 sec -> 60 rotations Data transfer rate = number of rotations per second * track capacity *
number of surfaces (since 1 R-W head is used for each surface) Data transfer rate = 60 * 128 * 256 * 4 Data transfer rate = 7.5 MB/sec
3. The disk is rotating at 3600 RPM, what is the average access time? Since seek time, controller time and the amount of data to be transferred is not given, we consider all three terms as 0. Therefore, Average Access time = Average rotational delay Rotational latency => 60 sec -> 3600 rotations 1 sec -> 60 rotations Rotational latency = (1/60) sec = 16.67 msec. Average Rotational latency = (16.67)/2 = 8.33 msec. Average Access time = 8.33 msec.
4. Another example: [GATE IT 2007 | Question 44](https://www.geeksforgeeks.org/gate-gate-it-2007-question-44/)
*** Disk Scheduling Algorithms
**Disk scheduling** is done by operating systems to schedule I/O requests arriving for the disk. Disk scheduling is also known as I/O scheduling.

Disk scheduling is important because: 

- Multiple I/O requests may arrive by different processes and only one I/O request can be served at a time by the disk controller. Thus other I/O requests need to wait in the waiting queue and need to be scheduled.
- Two or more request may be far from each other so can result in greater disk arm movement.
- Hard drives are one of the slowest parts of the computer system and thus need to be accessed in an efficient manner.

There are many Disk Scheduling Algorithms but before discussing them let’s have a quick look at some of the important terms: 

- **Seek Time:**Seek time is the time taken to locate the disk arm to a specified track
where the data is to be read or write. So the disk scheduling algorithm
that gives minimum average seek time is better.
- **Rotational Latency:** Rotational Latency is the time taken by the desired sector of disk to
rotate into a position so that it can access the read/write heads. So
the disk scheduling algorithm that gives minimum rotational latency is
better.
- **Transfer Time:** Transfer time is
the time to transfer the data. It depends on the rotating speed of the
disk and number of bytes to be transferred.
- **Disk Access Time:** Disk Access Time is:

```

      Disk Access Time = Seek Time +
                         Rotational Latency +
                         Transfer Time
```

!https://media.geeksforgeeks.org/wp-content/uploads/disc-scheduling-algorithms.png

- **Disk Response Time:** Response Time is the average of time spent by a request waiting to perform its I/O operation. *Average Response time* is the response time of the all requests. *Variance Response Time* is measure of how individual request are serviced with respect to average
response time. So the disk scheduling algorithm that gives minimum
variance response time is better.

**Disk Scheduling Algorithms** 

1. **FCFS:** FCFS is the simplest of all the Disk Scheduling Algorithms. In FCFS, the
requests are addressed in the order they arrive in the disk queue.Let us understand this with the help of an example.

### **Example:**

1. Suppose the order of request is- (82,170,43,140,24,16,190)And current position of Read/Write head is : 50 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608201201/fcfs3.jpg

1. So, total seek time: =(82-50)+(170-82)+(170-43)+(140-43)+(140-24)+(24-16)+(190-16) =642 

Advantages: 

- Every request gets a fair chance
- No indefinite postponement

Disadvantages: 

- Does not try to optimize seek time
- May not provide the best possible service
1. **SSTF:** In SSTF (Shortest Seek Time First), requests having shortest seek time
are executed first. So, the seek time of every request is calculated in
advance in the queue and then they are scheduled according to their
calculated seek time. As a result, the request near the disk arm will
get executed first. SSTF is certainly an improvement over FCFS as it
decreases the average response time and increases the throughput of
system.Let us understand this with the help of an example.

### **Example:**

1. Suppose the order of request is- (82,170,43,140,24,16,190)And current position of Read/Write head is : 50 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608201702/sstf1.jpg

1. 

So, total seek time:

1. =(50-43)+(43-24)+(24-16)+(82-16)+(140-82)+(170-140)+(190-170) =208

Advantages: 

- Average Response Time decreases
- Throughput increases

Disadvantages: 

- Overhead to calculate seek time in advance
- Can cause Starvation for a request if it has higher seek time as compared to incoming requests
- High variance of response time as SSTF favours only some requests
1. **SCAN:** In SCAN algorithm the disk arm moves into a particular direction and
services the requests coming in its path and after reaching the end of
disk, it reverses its direction and again services the request arriving
in its path. So, this algorithm works as an elevator and hence also
known as **elevator algorithm.** As a result, the requests at the midrange are serviced more and those arriving behind the disk arm will have to wait.

### **Example:**

1. Suppose the requests to be addressed are-82,170,43,140,24,16,190. And the
Read/Write arm is at 50, and it is also given that the disk arm should
move **“towards the larger value”.** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608202008/scan4.jpg

1. 

Therefore, the seek time is calculated as:

1. =(199-50)+(199-16) =332

Advantages: 

- High throughput
- Low variance of response time
- Average response time

Disadvantages: 

- Long waiting time for requests for locations just visited by disk arm
1. **CSCAN**: In SCAN algorithm, the disk arm again scans the path that has been
scanned, after reversing its direction. So, it may be possible that too
many requests are waiting at the other end or there may be zero or few
requests pending at the scanned area.

These situations are avoided in *CSCAN* algorithm
 in which the disk arm instead of reversing its direction goes to the 
other end of the disk and starts servicing the requests from there. So, 
the disk arm moves in a circular fashion and this algorithm is also 
similar to SCAN algorithm and hence it is known as C-SCAN (Circular 
SCAN). 

### **Example:**

Suppose the 
requests to be addressed are-82,170,43,140,24,16,190. And the Read/Write
 arm is at 50, and it is also given that the disk arm should move **“towards the larger value”.** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608202230/cscan1.jpg

Seek time is calculated as:

=(199-50)+(199-0)+(43-0) =391

Advantages: 

- Provides more uniform wait time compared to SCAN
1. **LOOK:** It is similar to the SCAN disk scheduling algorithm except for the
difference that the disk arm in spite of going to the end of the disk
goes only to the last request to be serviced in front of the head and
then reverses its direction from there only. Thus it prevents the extra
delay which occurred due to unnecessary traversal to the end of the
disk.

### **Example:**

1. Suppose the requests to be addressed are-82,170,43,140,24,16,190. And the
Read/Write arm is at 50, and it is also given that the disk arm should
move **“towards the larger value”.** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608202613/look1.jpg

1. 

So, the seek time is calculated as:

1. =(190-50)+(190-16) =314
2. **CLOOK:** As LOOK is similar to SCAN algorithm, in similar way, CLOOK is similar
to CSCAN disk scheduling algorithm. In CLOOK, the disk arm in spite of
going to the end goes only to the last request to be serviced in front
of the head and then from there goes to the other end’s last request.
Thus, it also prevents the extra delay which occurred due to unnecessary traversal to the end of the disk.

### **Example:**

1. Suppose the requests to be addressed are-82,170,43,140,24,16,190. And the
Read/Write arm is at 50, and it is also given that the disk arm should
move **“towards the larger value”** 

!https://media.geeksforgeeks.org/wp-content/uploads/20200608202846/clook1.jpg

1. 

So, the seek time is calculated as:

1. =(190-50)+(190-16)+(43-16) =341
2. **RSS**– It stands for random scheduling and just like its name it is nature. It is used in situations where scheduling involves random attributes such
as random processing time, random due dates, random weights, and
stochastic machine breakdowns this algorithm sits perfect. Which is why
it is usually used for and analysis and simulation.
3. **LIFO**– In LIFO (Last In, First Out) algorithm, newest jobs are serviced before the existing ones i.e. in order of requests that get serviced the job
that is newest or last entered is serviced first and then the rest in
the same order.
    
    **Advantages**
    
    - Maximizes locality and resource utilization
    - Can seem a little unfair to other requests and if new requests keep coming in, it cause starvation to the old and existing ones.
4. **N-STEP SCAN** – It is also known as N-STEP LOOK algorithm. In this a buffer is created
for N requests. All requests belonging to a buffer will be serviced in
one go. Also once the buffer is full no new requests are kept in this
buffer and are sent to another one. Now, when these N requests are
serviced, the time comes for another top N requests and this way all get requests get a guaranteed service
    
    **Advantages**
    
    - It eliminates starvation of requests completely
5. **FSCAN**– This algorithm uses two sub-queues. During the scan all requests in the first queue are serviced and the new incoming requests are added to the second queue. All new requests are kept on halt until the existing
requests in the first queue are serviced. **Advantages**
    - FSCAN along with N-Step-SCAN prevents “arm stickiness” (phenomena in I/O
    scheduling where the scheduling algorithm continues to service requests
    at or near the current sector and thus prevents any seeking)

Each algorithm is unique in its own way. Overall Performance depends on the number and type of requests. **Note:**Average Rotational latency is generally taken as 1/2(Rotational latency). Exercise

**1)**
 Suppose a disk has 201 cylinders, numbered from 0 to 200. At some time 
the disk arm is at cylinder 100, and there is a queue of disk access 
requests for cylinders 30, 85, 90, 100, 105, 110, 135 and 145. If 
Shortest-Seek Time First (SSTF) is being used for scheduling the disk 
access, the request for cylinder 90 is serviced after servicing 
____________ number of requests. (GATE CS 2014 (A) 1 (B) 2 (C) 3 (D) 4 See [this](https://www.geeksforgeeks.org/gate-gate-cs-2014-set-1-question-29/) for solution.

**2)**
 Consider an operating system capable of loading and executing a single 
sequential user process at a time. The disk head scheduling algorithm 
used is First Come First Served (FCFS). If FCFS is replaced by Shortest 
Seek Time First (SSTF), claimed by the vendor to give 50% better 
benchmark results, what is the expected improvement in the I/O 
performance of user programs? (GATE CS 2004) (A) 50% (B) 40% (C) 25% (D) 0% See [this](https://www.geeksforgeeks.org/gate-gate-cs-2004-question-12/) for solution.

**3)** Suppose
 the following disk request sequence (track numbers) for a disk with 100
 tracks is given: 45, 20, 90, 10, 50, 60, 80, 25, 70. Assume that the 
initial position of the R/W head is on track 50. The additional distance
 that will be traversed by the R/W head when the Shortest Seek Time 
First (SSTF) algorithm is used compared to the SCAN (Elevator) algorithm
 (assuming that SCAN algorithm moves towards 100 when it starts 
execution) is _________ tracks (A) 8 (B) 9 (C) 10 (D) 11 See [this](https://www.geeksforgeeks.org/gate-gate-cs-2015-set-1-question-40/) for solution.

**4)**
 Consider a typical disk that rotates at 15000 rotations per minute 
(RPM) and has a transfer rate of 50 × 10^6 bytes/sec. If the average 
seek time of the disk is twice the average rotational delay and the 
controller’s transfer time is 10 times the disk transfer time, the 
average time (in milliseconds) to read or write a 512 byte sector of the
 disk is _____________ See [this](https://www.geeksforgeeks.org/gate-gate-cs-2015-set-2-question-59/) for solution.

This article is contributed by **Ankit Mittal**. Please write comments if you find anything incorrect, or you want to share more information about the topic discussed above.

*** Program for SSTF disk scheduling algorithm
Prerequisite – [Disk scheduling algorithms](https://www.geeksforgeeks.org/disk-scheduling-algorithms/) Given
 an array of disk track numbers and initial head position, our task is 
to find the total number of seek operations done to access all the 
requested tracks if **Shortest Seek Time First (SSTF)** is a disk scheduling algorithm is used.

**Shortest Seek Time First (SSTF) –** Basic idea is the tracks which are closer to current disk head position should be serviced first in order to *minimise the seek operations*.

**Advantages of Shortest Seek Time First (SSTF) –**

1. Better performance than FCFS scheduling algorithm.
2. It provides better throughput.
3. This algorithm is used in Batch Processing system where throughput is more important.
4. It has less average response and waiting time.

**Disadvantages of Shortest Seek Time First (SSTF) –**

1. Starvation is possible for some requests as it favours easy to reach request and ignores the far away processes.
2. Their is lack of predictability because of high variance of response time.
3. Switching direction slows things down.

**Algorithm –**

1. Let Request array represents an array storing indexes of tracks that have been requested. ‘head’ is the position of disk head.
2. Find the positive distance of all tracks in the request array from head.
3. Find a track from requested array which has not been accessed/serviced yet and has minimum distance from head.
4. Increment the total seek count with this distance.
5. Currently serviced track position now becomes the new head position.
6. Go to step 2 until all tracks in request array have not been serviced. 

**Example –** Request sequence = {176, 79, 34, 60, 92, 11, 41, 114} Initial head position = 50

The following chart shows the sequence in which requested tracks are serviced using SSTF.

!https://media.geeksforgeeks.org/wp-content/uploads/3333-4.png

Therefore, total seek count is calculated as:

```
= (50-41)+(41-34)+(34-11)+(60-11)+(79-60)+(92-79)+(114-92)+(176-114)
= 204
```

**Which can also be directly calculated as: (50-11)+(176-11)Implementation –** Implementation
 of SSTF is given below. Note that we have made a node class having 2 
members. ‘distance’ is used to store the distance between head and the 
track position. ‘accessed’ is a boolean variable which tells whether the
 track has been accessed/serviced before by disk head or not.

`// C++ program for implementation of`

`// SSTF disk scheduling`

`#include <bits/stdc++.h>`

`using` `namespace` `std;`

`// Calculates difference of each`

`// track number with the head position`

`void` `calculatedifference(int` `request[], int` `head,`

`int` `diff[][2], int` `n)`

`{`

`for(int` `i = 0; i < n; i++)`

`{`

`diff[i][0] = abs(head - request[i]);`

`}`

`}`

`// Find unaccessed track which is`

`// at minimum distance from head`

`int` `findMIN(int` `diff[][2], int` `n)`

`{`

`int` `index = -1;`

`int` `minimum = 1e9;`

`for(int` `i = 0; i < n; i++)`

`{`

`if` `(!diff[i][1] && minimum > diff[i][0])`

`{`

`minimum = diff[i][0];`

`index = i;`

`}`

`}`

`return` `index;`

`}`

`void` `shortestSeekTimeFirst(int` `request[],`

`int` `head, int` `n)`

`{`

`if` `(n == 0)`

`{`

`return;`

`}`

`// Create array of objects of class node`

`int` `diff[n][2] = { { 0, 0 } };`

`// Count total number of seek operation`

`int` `seekcount = 0;`

`// Stores sequence in which disk access is done`

`int` `seeksequence[n + 1] = {0};`

`for(int` `i = 0; i < n; i++)`

`{`

`seeksequence[i] = head;`

`calculatedifference(request, head, diff, n);`

`int` `index = findMIN(diff, n);`

`diff[index][1] = 1;`

`// Increase the total count`

`seekcount += diff[index][0];`

`// Accessed track is now new head`

`head = request[index];`

`}`

`seeksequence[n] = head;`

`cout << "Total number of seek operations = "`

`<< seekcount << endl;`

`cout << "Seek sequence is : "` `<< "\n";`

`// Print the sequence`

`for(int` `i = 0; i <= n; i++)`

`{`

`cout << seeksequence[i] << "\n";`

`}`

`}`

`// Driver code`

`int` `main()`

`{`

`int` `n = 8;`

`int` `proc[n] = { 176, 79, 34, 60, 92, 11, 41, 114 };`

`shortestSeekTimeFirst(proc, 50, n);`

`return` `0;`

`}`

`// This code is contributed by manish19je0495`

**Output:**

```
Total number of seek operations = 204
Seek Sequence is
50
41
34
11
60
79
92
114
176
```

**Time Complexity:** O(N^2)**Auxiliary Space:** O(N)
*** What exactly Spooling is all about?
SPOOL is an acronym for **simultaneous peripheral operations on-line**. 
 It is a kind of buffering mechanism or a process in which data is 
temporarily held to be used and executed by a device, program or the 
system. Data is sent to and stored in memory or other volatile storage 
until the program or computer requests it for execution.

In
 a computer system peripheral equipments, such as printers and punch 
card readers, etc (batch processing), are very slow relative to the 
performance of the rest of the system. Getting input and output from the
 system was quickly seen to be a bottleneck. Here comes the need for 
spool.

Spooling works like a typical 
request queue where data, instructions and processes from multiple 
sources are accumulated for execution later on. Generally, it is 
maintained on computer’s physical memory, buffers or the I/O 
device-specific interrupts. The spool is processed in FIFO manner i.e. 
whatever first instruction is there in the queue will be popped and 
executed.

**Applications/Implementations of Spool:**

1)
 The most common can be found in I/O devices like keyboard printers and 
mouse. For example, In printer, the documents/files that are sent to the
 printer are first stored in the memory or the printer spooler. Once the
 printer is ready, it fetches the data from the spool and prints it.

Even
 experienced a situation when suddenly for some seconds your mouse or 
keyboard stops working? Meanwhile, we usually click again and again here
 and there on the screen to check if its working or not. When it 
actually starts working, what and wherever we pressed during its hang 
state gets executed very fast because all the instructions got stored in
 the respective device’s spool.

2) A 
batch processing system uses spooling to maintain a queue of 
ready-to-run jobs which can be started as soon as the system has the 
resources to process them.

3) Spooling 
is capable of overlapping I/O operation for one job with processor 
operations for another job. i.e. multiple processes can write documents 
to a print queue without waiting and resume with their work.

4)
 E-mail: an email is delivered by a MTA (Mail Transfer Agent) to a 
temporary storage area where it waits to be picked up by the MA (Mail 
User Agent)

5) Can also be used for 
generating Banner pages (these are the pages used in computerized 
printing in order to separate documents from each other and to identify 
e.g. the originator of the print request by username, an account number 
or a bin for pickup. Such pages are used in office environments where 
many people share the small number of available resources).

About the Author:

Ekta is a  very active contributor on Geeksforgeeks. Currently studying at Delhi Technological University. She has also made a Chrome extension for  [www.geeksquiz.com](http://www.geeksquiz.com/) to practice MCQs randomly. She can be reached at  [github.com/Ekta1994](https://github.com/Ekta1994)

**If you also wish to showcase your blog here, please see [GBlog](http://geeksquiz.com/gblog/) for guest blog writing on GeeksforGeeks.**
*** Difference between Spooling and Buffering
There
 are two ways by which Input/output subsystems can improve the 
performance and efficiency of the computer by using a memory space in 
the main memory or on the disk and these two are *spooling and buffering*.

**[Spooling](https://www.geeksforgeeks.org/what-exactly-spooling-is-all-about/) –**

Spooling
 stands for Simultaneous peripheral operation online. A spool is similar
 to buffer as it holds the jobs for a device until the device is ready 
to accept the job. It considers disk as a huge buffer that can store as 
many jobs for the device till the output devices are ready to accept 
them.

**Buffering –**

The main memory has an 
area called buffer that is used to store or hold the data temporarily 
that is being transmitted either between two devices or between a device
 or an application. Buffering is an act of storing data temporarily in 
the buffer. It helps in matching the speed of the data stream between 
the sender and the receiver. If the speed of the sender’s transmission 
is slower than the receiver, then a buffer is created in the main memory
 of the receiver, and it accumulates the bytes received from the sender 
and vice versa.

The basic difference between Spooling and 
Buffering is that Spooling overlaps the input/output of one job with the
 execution of another job while the buffering overlaps the input/output 
of one job with the execution of the same job.

**Differences between Spooling and Buffering –**

- The key difference between spooling and buffering is that Spooling can
handle the input/output of one job along with the computation of another job at the same time while buffering handles input/output of one job
along with its computation.
- Spooling stands for Simultaneous Peripheral Operation online. Whereas buffering is not an acronym.
- Spooling is more efficient than buffering, as spooling can overlap processing two jobs at a time.
- Buffering uses limited area in main memory while Spooling uses the disk as a huge buffer.

**Comparison chart –**

[](https://www.notion.so/4e697c80ec994973acf2fd37543e367a?pvs=21)

*** Free space management in Operating System
The 
system keeps tracks of the free disk blocks for allocating space to 
files when they are created. Also, to reuse the space released from 
deleting the files, free space management becomes crucial. The system 
maintains a free space list which keeps track of the disk blocks that 
are not allocated to some file or directory. The free space list can be 
implemented mainly as:

1. **Bitmap or Bit vector –**A Bitmap or Bit Vector is series or collection of bits where each bit
corresponds to a disk block. The bit can take two values: 0 and 1: *0 indicates that the block is allocated* and 1 indicates a free block.The given instance of disk blocks on the disk in *Figure 1* (where green blocks are allocated) can be represented by a bitmap of 16 bits as: **0000111000000110**.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/1-265.png
    
    **Advantages –**
    
    - Simple to understand.
    - Finding the first free block is efficient. It requires scanning the words (a
    group of 8 bits) in a bitmap for a non-zero word. (A 0-valued word has
    all bits 0). The first free block is then found by scanning for the
    first 1 bit in the non-zero word.
    
    The block number can be calculated as:*(number of bits per word) *(number of 0-values words) + offset of bit first bit 1 in the non-zero word* .
    
    For the *Figure-1*, we scan the bitmap sequentially for the first non-zero word.The
     first group of 8 bits (00001110) constitute a non-zero word since all 
    bits are not 0. After the non-0 word is found, we look for the first 1 
    bit. This is the 5th bit of the non-zero word. So, offset = 5.Therefore, the first free block number = 8*0+5 = 5.
    
2. **Linked List –**In this approach, the free disk blocks are linked together i.e. a free
block contains a pointer to the next free block. The block number of the very first disk block is stored at a separate location on disk and is
also cached in memory.
    
    !https://media.geeksforgeeks.org/wp-content/uploads/2-190.png
    
    In *Figure-2*,
     the free space list head points to Block 5 which points to Block 6, the
     next free block and so on. The last free block would contain a null 
    pointer indicating the end of free list.A drawback of this method is the I/O required for free space list traversal.
    
3. **Grouping –**This approach stores the address of the free blocks in the first free block. The first free block stores the address of some, say n free blocks. Out of these n blocks, the first n-1 blocks are actually free and the last
block contains the address of next free n blocks.An **advantage** of this approach is that the addresses of a group of free disk blocks can be found easily.
4. **Counting –**This approach stores the address of the first free disk block and a number n of free contiguous disk blocks that follow the first block.Every entry in the list would contain:
    1. Address of first free disk block
    2. A number n
    
    For example, *in Figure-1*, the first entry of the free space list would be: ([Address of Block 5], 2), because 2 contiguous free blocks follow block 5.
    

!https://media.geeksforgeeks.org/wp-content/post-ads-banner/2022-05-25-15-57-36-InterviewSeriesPrepareInArticleAd.webp

[Previous](https://www.geeksforgeeks.org/file-allocation-methods/)

[File Allocation Methods](https://www.geeksforgeeks.org/file-allocation-methods/)

[Next](https://www.geeksforgeeks.org/disk-scheduling-algorithms/)

[Disk Scheduling Algorithms](https://www.geeksforgeeks.org/disk-scheduling-algorithms/)

Recommended Articles
** foot notes 
